% Created 2016-12-09 Fr 23:09
\documentclass[a4paper]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\usepackage{siunitx}
\usepackage{fontspec}
\sisetup{load-configurations = abbrevations}
\newcommand{\estimates}{\overset{\scriptscriptstyle\wedge}{=}}
\usepackage{mathtools}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother
\DeclareMathOperator{\Exists}{\exists}
\DeclareMathOperator{\rot}{rot}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Forall}{\forall}
\def\cvec#1{\left(\vcenter{\halign{\hfil$##$\hfil\cr \cvecA#1;;}}\right)}
\def\cvecA#1;{\if;#1;\else #1\cr \expandafter \cvecA \fi}
\renewcommand{\d}{\mathrm{d}}
\newcommand{\f}[2]{\frac{#1}{#2}}
\newcommand{\dd}[2]{\frac{\d #1}{\ d#2}}
\renewcommand{\v}[1]{\vec{#1}}
\usepackage{pgfplots}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\theoremstyle{plain}
\newtheorem{thm}{Satz}
\theoremstyle{remark}
\newtheorem{remark}{Bemerkung}
\theoremstyle{remark}
\newtheorem{ex}{Beispiel}
\usepackage{etoolbox}
\patchcmd{\thmhead}{(#3)}{#3}{}{}
\usepackage{xparse}% http://ctan.org/pkg/xparse
\NewDocumentCommand{\overarrow}{O{=} O{\uparrow} m}{%
\overset{\makebox[0pt]{\begin{tabular}{@{}c@{}}#3\\[0pt]\ensuremath{#2}\end{tabular}}}{#1}
}
\NewDocumentCommand{\underarrow}{O{=} O{\downarrow} m}{%
\underset{\makebox[0pt]{\begin{tabular}{@{}c@{}}\ensuremath{#2}\\[0pt]#3\end{tabular}}}{#1}
}
\renewcommand*{\proofname}{Beweis}
\newcommand{\I}{\ensuremath{i}}%
\newcommand{\ubar}[1]{\underline{#1}}
\newcommand{\eps}{\ensuremath{\varepsilon}}%
\author{Robin Heinemann}
\date{\today}
\title{Theoretische Physik (Hebecker)}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs 25.1.1 (Org mode 8.2.10)}}
\begin{document}

\maketitle
\tableofcontents

Einleitung: \\
\begin{itemize}
\item Website: www.thphys.uni-heidelberg.de/hebecker/TP1/tp1.html
\item Bartelman Skripte
\end{itemize}

\section{Semesterüberblick}
\label{sec-1}
\begin{enumerate}
\item Newtonsche Mechanik
\item Lagrange / Hamilton Mechanik / Statistik / Kontinua
\item Elektrodynamik / Spezielle Relativitätstheorie
\item Quantenmechanik
\item Thermodynamik / Quantenstatistik
\item Allgemeine Relativitätstheorie / Kosmologie
\item Quantenfeldtheorie I (ggf. 5.)
\item Quantenfeldtheorie II (ggf. 6. $\impliedby$ Stringtheorie / Teilchenphysik / Supersymmetrie)
\item Masterarbeit
\item Masterarbeit
\end{enumerate}
\subsection{Mathe}
\label{sec-1-1}
\textbf{wichtig:}
\begin{itemize}
\item Gruppentheorie
\item Differientialgeometrie
\end{itemize}
\section{Kinematik des Massenpunktes}
\label{sec-2}
Massenpunkt / Punktmasse - (selbstevidente) Abstraktion
Kinematik: Beschreibung der Bewegung (Ursachen der Bewegung $\rightarrow$ Dynamik)
\subsection{Kinematik der Massenpunktes in einer Dimension}
\label{sec-2-1}
\subsubsection{Graphik}
\label{sec-2-1-1}
\begin{itemize}
\item Ort: $x$
\item zu Zeit $t:~x(t)$
\item Geschwindigkeit: $v(t) \equiv \frac{\mathrm{d}x(t)}{\mathrm{d}t} \equiv \dot{x}(t)$
\item Beschleunigung: $a(t) \equiv \dot{v}(t) = \ddot{x}(t)$
\item Beispiel: $x(t) \equiv x_0 + v_0 t + \frac{a_0}{2}t^2, ~v(t) = v_0 + a_0 t,~a(t) = a_0$
\item Umgekehrt: Integration, z.B. von Geschwindigkeit zu Trajektorie: Anfangsposition muss gegeben sein, z.B. $x(t_0) \equiv x_0$
     \[x(t)=x_0 + \int_{t_0}^{t}v(t')\mathrm{d}t'\]
     Man prüft leicht $\dot{x}(t) = v(t)$
\begin{itemize}
\item Es gibt keine andere Funktion $\tilde{x}(t)$ mit $\dot{\tilde{x}}(t) = v(t)$ und $\tilde{x}(t_0) = x_0$
\end{itemize}
Analog: Von Beschleunigung zur Geschwindigkeit, und dann weiter zur Trajektorie
\end{itemize}
\subsubsection{Üben dieser Logik an unserem Beispiel}
\label{sec-2-1-2}
Gegeben: $a(t) = a_0,~t_0=0,v_0,x_0$ \\
    \[\implies~v(t) = v_0 + \int_0^t a_0\mathrm{d}t' = v_0 + a_0 t\]
\[x(t) = x_0 + \int_0^t (v_0 + a_0 t')\mathrm{d}t' = x_0 + v_0 t + \frac{a_0}{2}t^2\]
\subsection{Grundbegriffe der Differenzial und Integralrechung}
\label{sec-2-2}
\subsubsection{Funktion}
\label{sec-2-2-1}
\[f: \mathbb{R} \rightarrow \mathbb{R},~x \mapsto f(x)\]
\subsubsection{Differentiation oder Ableitung}
\label{sec-2-2-2}
\[\frac{\mathrm{d}f(x)}{\mathrm{d}x} = f'(x) = \lim_{\Delta x \to 0} \frac{f(x + \Delta x) - f(x)}{\Delta x}\]
$\mathrm{d}f$ bezeichnet den in $\Delta x$ linearen Anteil des Zuwachs $\Delta f\equiv f(x + \Delta x) - f(x)$.
\begin{itemize}
\item Aus $\Delta f = f'(x)\Delta(x) + O(\Delta x^2)~\text{folgt}~\mathrm{d}f = f'(x)\Delta x$
\item Anwendung auf die Identitätsabbildung: $x \mapsto x \implies \mathrm{d}x = \Delta x$
      \[\implies \mathrm{d}f = f'(x)\mathrm{d}x~\text{oder}~\frac{\mathrm{d}f(x)}{\mathrm{d}x} = f'(x)\]
      Dies ist eigentlich nur eine Schreibweise für $f'(x)$, \uline{aber} nützlich, weil bei kleinen $\Delta x~\mathrm{d}f \simeq \Delta f$ (Schreibweise beinhaltet intuitiv die Grenzwert-Definition)
\item $f'(x)$ wieder Funktion $\implies$ analog: $f''(x),~f'''(x),\ldots,f^{(n)}(x)$
\item Praxis
\[(f\cdot g)' = f' g + g' f~\text{(Produkt/Leibnizregel)}\]
\[(f \circ g)'(x) = f'(g(x))g'(x)~\text{(Kettenregel)}\]
\[(f^{-1})'(x) = \frac{1}{f'(f^{-1}(x))}~\text{(Ableitung der Inversen Funktion)}\]
\begin{itemize}
\item Begründung (nur zum letzten Punkt)
\[(f^{-1})'(x) = \frac{\mathrm{d}y}{\mathrm{d}x} = \frac{\mathrm{d}y}{\mathrm{d}(f(y))} = \frac{\mathrm{d}y}{f'(y)\mathrm{d}y} = \frac{1}{f'(f^{-1}(x))}\]
\item Schöne Beispiele
\[(x^x)' = (e^{\ln{x^x}})' = (e^{x\ln{x}})' = e^{x\ln{x}}(\ln{x} + 1) = x^x(\ln{x} + 1)\]
\[\arctan'(x) \equiv (\tan^{-1}(x)) = \frac{1}{\tan^{-1}(y)}~\text{wobei $y = \tan^{-1}(x)$}\]
Besser: \[\tan^{-1}(y) = (\sin{y} \frac{1}{\cos{y}})' = \cos{y} \frac{1}{\cos{y}} + \sin{y}(\frac{1}{\cos{y}})' = 1 + \sin{y}(-\frac{1}{\cos^2{y}})(-\sin{y}) = \] \\
        \[ 1 + \tan^2{y} = 1 + x^2 \\ \implies \arctan'(x) = \frac{1}{1 + x^2}\]
\end{itemize}
\item Verknüpfung \[f\circ g: x\mapsto f(g(x))\]
\item Inverse \[f^{-1} : x=f(y)\mapsto y\]
\item Grenzwerte:
\begin{itemize}
\item nützliche Regel: l'Hospital ("$\frac{0}{0}$") \\
        Falls $\lim_{x\to x_0} f,g = 0$ und $\lim_{x\to x_0} \frac{f'}{g'}$ existiert, so gilt $\lim_{x\to x_0}\frac{f}{g} = \lim_{x\to x_0} \frac{f'}{g'}$
\item weitere nützliche Regel \[\lim \frac{\text{Beschränkt}}{\text{Unbeschränkt und monoton wachsend}} = 0\]
\begin{itemize}
\item Beispiel: \[\lim_{y\to 0} \frac{\sin{\frac{1}{y}}}{\frac{1}{y}}\]
\end{itemize}
\item Kürzen unter $\lim$
\begin{itemize}
\item Beispiel: \[\lim_{x\to\infty} \frac{x}{2x + \sqrt{x}} = \lim_{x\to\infty}\frac{1}{2+\frac{1}{\sqrt{x}}} = \frac{1}{2}\]
\end{itemize}
\end{itemize}
\end{itemize}
\subsubsection{Integrieren}
\label{sec-2-2-3}
\paragraph{Fundamentalsatz der Analysis}
\label{sec-2-2-3-1}
\[\int^y f(x)\mathrm{d}x = F(y) \&  F'(y) = f(y)\]
\[\int f(x)\mathrm{d}x = F(x) + C\]
\[\int_a^b f(x)\mathrm{d}x = F(b) - F(a)\]
($\to$ saubere Definition über Riemannsches Integral)
\paragraph{Praxis}
\label{sec-2-2-3-2}
\subparagraph{Partielle Integration}
\label{sec-2-2-3-2-1}
\[\int^y f(x)g'(x)\mathrm{d}x = f(y)g(y) - \int^y f'(x)g(x)\mathrm{d}x\]
\subparagraph{Substitution}
\label{sec-2-2-3-2-2}
Unter Annahme einer invertierbaren Funktion $x: y\mapsto x(y)$
\[\int f(x)\mathrm{d}x = \int f(x)\frac{\mathrm{d}x}{\mathrm{d}y}\mathrm{d}y = \int f(x(y)) x'(y)\mathrm{d}y\]
Andere Formulierung: \[\int_a^b f(g(x))g'(x)\mathrm{d}x = \int_{g(a)}^{g(b)}f(y)\mathrm{d}y\]
Substitution $y=g(x)$
\subparagraph{Klassiker}
\label{sec-2-2-3-2-3}
\[\int \ln{x}\mathrm{d}x = \int \ln{x}1\mathrm{d}x = \ln{x}x - \int \frac{1}{x}x\mathrm{d}x = x(\ln{x} - 1)\]
\[\int x e^{x^2}\mathrm{d}x = \int e^{x^2}\frac{1}{2}\mathrm{d}(x^2) = \frac{1}{2}\int e^y \mathrm{d}y = \frac{1}{2}e^y = \frac{1}{2}e^{x^2}\]
\subsection{Kinematik in mehreren Dimensionen}
\label{sec-2-3}
\subsubsection{Zweidimensionale Bewegung}
\label{sec-2-3-1}
Zweidimensional $\rightarrow$ Bewegung in der Ebene. Trajektorie: $x(t),y(t)$
\paragraph{Beispiel}
\label{sec-2-3-1-1}
\[x(t) = v_0 t \sin{\omega t}\]
\[y(t) = v_0 t \cos{\omega t}\]
\subparagraph{{\bfseries\sffamily TODO} Skizze der Trajektorie (Bahnkurve)}
\label{sec-2-3-1-1-1}
\subparagraph{Raumkurve}
\label{sec-2-3-1-1-2}
Menge aller Punkte $\{x,y\}$, die das Teilchen durchläuft
\subparagraph{{\bfseries\sffamily TODO} Skizze Nichttriviale Darstellung \underline{nur} im Raum (Raumkurve)}
\label{sec-2-3-1-1-3}
\subsubsection{Dreidimensionale Bewegung}
\label{sec-2-3-2}
Die Darstellung der Trajektorie ist erschwert, denn man bräuchte $4$ Dimensionen: $3$ für Raum und $1$ für Zeit
Formal kein Problem: Trajektorie ist
\begin{itemize}
\item \[x(t),y(t),z(t)\]
\item \[x^1(t),x^2(t),x^3(t)\]
\item \[\{x^i(t)\},i=1,2,3\]
\end{itemize}

Dementsprechend:
\[v^i(t) = \dot{x}^i(t); a^i(t) = \dot{v}^i(t); i=1,2,3\]
\subsection{Vektorräume}
\label{sec-2-4}
Eine Menge $V$ heißt Vektorraum, wenn auf ihr zwei Abbildungen
\begin{itemize}
\item die Addition ($+$)
\item die Multiplikation mit reellen Zahlen ($*$)
\end{itemize}
definiert sind.

\[x : V\times V \rightarrow V\]
\[\text{Multiplikation}: \mathbb{R}\times V \rightarrow V\]
$V\times V$ - Produktmenge $\equiv$ Menge aller Paare
so dass gilt:
\[v + (w + u) = (v + w) + u\quad u,v,w\in V\tag*{Assoziativität}\]
\[v+w = w+v\tag*{Kommutativität}\]
\[\exists 0 \in V: v + 0 = v \Forall v\in V\tag*{Null}\]
\[\alpha(v+w) = \alpha v + \alpha w \tag*{Distributivität}\]
\[(\alpha + \beta)v = \alpha v + \beta v \quad \alpha,\beta \in \mathbb{R}\tag*{Distributivität}\]
\[\alpha(\beta v) = (\alpha\beta) v\tag*{Assoziativität der Multiplikation}\]
\[1 v = v \tag*{Multiplikation mit Eins}\]
\subsubsection{Einfachstes Beispiel}
\label{sec-2-4-1}
$V\equiv \mathbb{R}$ (mit der gewöhnlichen Addition und Multiplikation und mit $0\in\mathbb{R}$ als Vektorraum Null)
\subsubsection{Unser Haupt-Beispiel}
\label{sec-2-4-2}
Zahlentupel aus n-Zahlen:
\[V\equiv \mathbb{R}^n = \{(x^1,x^2,\ldots,x^n), x^i \in\mathbb{R}\}\]
Notation:
\[\vec{x} = \begin{pmatrix} x^1& x^2 & \ldots & x^n)\end{pmatrix}, \vec{y} = \begin{pmatrix} y^1 & \ldots y^n \end{pmatrix}\]
Man definiert:
\[\vec{x} + \vec{y} \equiv (x^1 + y^1, x^2 + y^2, \ldots, x^n + y^n)\]
\[\vec{0} \equiv (0,\ldots,0)\]
\[\alpha \vec{x} \equiv (\alpha x^1, \ldots, \alpha x^n)\]
\paragraph{{\bfseries\sffamily TODO} (Maybe) Skizze 3D Vektor}
\label{sec-2-4-2-1}
$\rightarrow$ übliche Darstellung durch "Pfeile"
\subsection{Kinematik in $d>1$}
\label{sec-2-5}
Trajektorie ist Abbildung: $\mathbb{R} \to \mathbb{R}^3, t\to \vec{x}(t) ) (x^1(t),x^1(t),x^3(t))$
\[\vec{v} = \dot{\vec{x}}(t), \vec{a(t)} = \dot{\vec{v}}(t) = \ddot{\vec{x}}(t)\]
Setzt allgemeine Definition der Ableitung voraus:
\[\frac{\mathrm{d}\vec{y}(x)}{\mathrm{d}x} = \lim_{\Delta x \to 0} \frac{\vec{y}(x + \Delta x) - \vec{y}(x)}{\Delta x}  \implies \vec{y}'(x) = (y^{1'}(x), \ldots,y^{n'}(x))\]
\subsubsection{Beispiel für 3-dimensionale Trajektorie}
\label{sec-2-5-1}
Schraubenbahn:
\[\vec{x}t = (R\cos{\omega t},R\sin{\omega t}, v_0 t)\]
\[\vec{v} = (-R\omega\sin{\omega t}, R\omega\cos{\omega t}, v_0)\]
\[\vec{a} = (-R\omega^2\cos{\omega t}, -R\omega^2\sin{\omega t}, 0)\]
\paragraph{{\bfseries\sffamily TODO} Skizze (Raumkurve)}
\label{sec-2-5-1-1}
\textbf{Kommentar:} \\
     $\vec{x},\vec{v},\vec{a}$ leben in verschiedenen Vektorräumen!
allein schon wegen $[x] = \si{\meter}$, $[v] = \si{\meter\per\second}$ \\
     Wir können wie in $d=1$ von $\vec{a}$ zu $\vec{v}$ zu $\vec{x}$ gelangen!
\[\vec{v}(t) = \vec{v_0} + \int_{t_0}^{t} \mathrm{d}t' \vec{a}(t') = (v_0^1 + \int_{t_0}^t \mathrm{d}t' a^1(t'), v_0^2 + \int_{t_0}^t \mathrm{d}t' a^2(t'), v_0^3 + \int_{t_0}^t \mathrm{d}t' a^2(t'))\]
\paragraph{Üben:}
\label{sec-2-5-1-2}
Schraubenbahn; $t_0 = 0$, $\vec{x_0} = \left(R, 0, 0), v_0 = (0, R\omega, v_0\right)$
Es folgt:
\begin{align*}
&\vec{v}(t) ) (0, R\omega, v_0) + \int_0^t \mathrm{d}t' ( -R\omega^2)(\cos{\omega t', \sin{\omega t'}, 0})\\
=& (0, R\omega, v_0) + (-R\omega^2)(\frac{1}{\omega}\sin{\omega t'}, -\frac{1}{\omega}\cos{\omega t'}, 0)\mid_0^t\\
=& (0, R\omega, v_0) - R\omega (\sin{\omega t}, -\cos{\omega t}, 0) - (0, -1, 0)\\
=& (-R\omega\sin{\omega t}, R\omega + R\omega\cos{\omega t} - R\omega, v_0)\\
=& (-R\omega\sin{\omega t}, R\omega\cos{\omega t}, v_0)
\end{align*}
\paragraph{Bemerkung}
\label{sec-2-5-1-3}
Man kann Integrale über Vektoren auch durch Riemannsche Summen definieren:
\[\int_{t_0}^t \vec{v}(t')\mathrm{d}t' = \lim_{n\to\infty} (v(t_0)\Delta t + \vec{v}(t_0 + \Delta t)\Delta t + \ldots + \vec{v}(t - \Delta t)\Delta t)\]
mit $\Delta t = \frac{t - t_0}{N}$
\subsection{Skalarprodukt}
\label{sec-2-6}
Führt von Vektoren wieder zu nicht-vektoriellen (Skalaren) Größen.
\subsubsection{Symmetrische Bilinearform}
\label{sec-2-6-1}
$f(\alpha x + \beta y) = \alpha f(x) + \beta f(y)$ "linear"
Abbildung von $V\times V \to \mathbb{R},~(v,w) \mapsto v\cdot w$ mit den Eigenschaften
\begin{itemize}
\item $v\cdot w = w\cdot v$
\item $(\alpha u + \beta v) \cdot w = \alpha u\cdot w + \beta v\cdot w$
\end{itemize}
Sie heißt positiv-semidefinit, falls  $v\cdot v\geq 0$, \\
    Sie heißt positiv-definit, falls  $v\cdot v = 0 \implies v = 0$
Hier : Skalarprodukt $\equiv$ positiv definite symmetrische Bilinearform
\subsubsection{Norm (Länge) eines Vektors}
\label{sec-2-6-2}
\[\abs{v} = \sqrt{v\cdot v} = \sqrt{v^2}\]
$\mathbb{R}^n$: Wir definieren \[\vec{x}\cdot\vec{y} = x^1y^1 + \ldots + x^n y^n \equiv \sum_{i=1}^n x^iy^i \equiv \underbrace{x^i y^i}_{\text{Einsteinsche Summenkonvention}}\]
\[\abs{\vec{x}} = \sqrt{(x^1)^2 + \ldots + (x^n)^2}\]
Wichtig: oben euklidisches Skalarprodukt! Anderes Skalarprodukt auf $\mathbb{R}^2: \vec{x}\cdot\vec{y} = 7x^1 y^2 + x^2y^2$
anderes Beispiel:
\[\vec{x}\cdot\vec{y} \equiv x^1y^1 - x^2y^2\]
symmetrische Bilinearform, \uline{nicht} positiv, semidefinit!
Frage: \\
    Beispiel für Bilinearform die positiv-semidefinit ist, aber \uline{nicht positiv definit}
\[\v x \v y = x^1 y^1\]
\subsection{Abstand zwischen Raumpunkten}
\label{sec-2-7}
Der anschauliche Abstand zwischen Raumpunkten $\v x,\v y$:
\[\abs{\v x - \v y} = \sqrt{(\v x - \v y)(\v x - \v y)} = \sqrt{(\v x - \v y)^2} = \sqrt{\sum_{i=1}^3 (x^i - y^i)^2} = \sqrt{(x^i - y^i)(x^i - y^i)}\]
\[=\sqrt{{\v x}^2 + {\v y}^2 - 2\v x \v y} = \sqrt{\abs{\v x}^2 +  \abs{\v y}^2 - 2\abs{\v x}\abs{\v y}}\cos{\theta}\]
Haben benutzt: $\v x\cdot \v y = \abs{\v x}\abs{\v y}\cos{\theta}$
\subsubsection{Spezialfall}
\label{sec-2-7-1}
\[\v x = (x^1, 0, 0), \v y = (y^1, y^2, 0)\]
\[\v x \cdot \v y = x^1 \cdot y^1; \cos{\theta} = \frac{y^1}{\abs{\v y}}; \abs{\v x} = x^1\]
\paragraph{{\bfseries\sffamily TODO} Skizze}
\label{sec-2-7-1-1}
\[\implies \v x\cdot \v y = \abs{\v x}\abs{\v y} \cos{\theta}\]
Dass dies für beliebige Vektoren gilt, wird später klar werden.
\subsubsection{Infinitesimaler Abstand}
\label{sec-2-7-2}
Speziell wird der infinitesimale Abstand wichtig sein:
\[\d\v x = (\d x^1, \d x^2,\d x^3)\]
\[\d\v x = (\f{\d x^1}{\d t}\d t, \f{\d x^2}{\d t}\d t, \f{\d x^3}{\d t}\d t) = (v^1\d t, v^2\d t, v^3\d t) = (v^1, v^2, v^3)\d t = \v v \d t,~\text{oder:}~\v v = \f{\d\v x}{\d t}\]
($\d \v x~\text{analog zu}~\d f$ vorher); \\
    $\d {\v x}^2 = \abs{\d \v x}^2 = \abs{\v v}^2 \d t^2$ \\ $\abs{\d x} = \abs{\v v}\d t$.
\subsection{Bogenlänge und begleitendes Dreibein}
\label{sec-2-8}
$\abs{d\v x}$ entlang $\v x(t)$ aufaddieren $\rightarrow$ Bogenlänge.
\[s(t) = \int_{t_0}^t \abs{\d \v x} = \int_{t_0}^t \d t' \abs{\f{\d \v x}{\d t'}} = \int_{t_0}^t\d t'\sqrt{\dot{\v x}(t')^2} = \int_{t_0}^t \sqrt{\v v(t')^2}\]
Infinitesimale Version: \[\f{\d s(t)}{\d t} = \abs{\f{\d\v x}{\d t}} = \abs{\v v}\]
Man kann (im Prinzip) $s(t) = s$ nach $t$ auflösen.
\[\implies t = t(s) \implies \underbrace{\v x(s)}_{\text{Parametrisierung der Trajektorie durch die Weglänge $s$}} \equiv \v x(t(s))\]
Nützlich, zum Beispiel für die Definition des Tangentenvektors:
\[\v T(s) = \f{\d\v x(s)}{\d s}\]
Es gilt \[\v T\parallel \v v; \abs{\v T} = \abs{\f{\v v \d t}{\abs{\v v}\d t}} = 1 \implies \v T \cdot \v T = 1\]
Ableiten nach $s$:
\[0 = \f{\d}{\d s}(1) = \f{\d \v T}{\d s}(\v T \cdot\v T) = \f{\d \v T}{\d s}\cdot \v T + \v T\cdot \f{\d\v T}{\d s} = 2\v T \cdot \f{\d \v T}{\d s}\]
Nutze \[\v T\cdot \v T = T^i T^i\]
$\implies$ Ableitung des Tangentenvektors ist orthogonal zum Tangentenvektor.
Krümmungsradius der Bahn: \[\rho \equiv \f{1}{\abs{\f{\d \v T}{\d s}}}\]
Normalenvektor: \[\v N = \f{\f{\d \v T}{\d s}}{\abs{\f{\d \v T}{\d s}}} = \rho \f{\d \v T}{\d s}\]
\subsubsection{Beispiel in d=2}
\label{sec-2-8-1}
\[\v x(t) = R(\cos{\omega t}, \sin{\omega t})\]
\[\v v(t) = R\omega (-\sin(\omega t), \cos{\omega t})\]
\[\abs{\v v} = \sqrt{(R\omega)^2 (\sin^2{\omega t}+\cos^2{\omega t})} = R\omega\]
\[s(t) = \int_{t_0 = 0}^t \d t' \abs{\v v} = R\omega t;~t(x) = = \f{s}{R\omega}\]
\[\implies \v x(s) = R(\cos{\f{s}{R}}, \sin{\f{s}{R}}), \v T = \f{\d\v x}{\d s} = (-\sin{\f{s}{R}},\cos{\f{s}{R}})\]
\[\f{\d\v T}{\d s} = -\f{1}{R}(\cos{\f{s}{R}}, \sin{\f{s}{R}}) \implies \rho = R;~\v N = -(\cos{\f{s}{R}}, \sin{\f{s}{R}})\]
\paragraph{{\bfseries\sffamily TODO} Skizze}
\label{sec-2-8-1-1}
\subsection{Vektorprodukt}
\label{sec-2-9}
\[V\times V \mapsto V;~(\v a, \v b) \mapsto \v c = \v a\times \v b\]
mit  \[c^i = (\v a \times \v b)^i \equiv \sum_{i,k=1}^3 \varepsilon^{ijk}a^jb^k = \varepsilon^{ijk}a^jb^k\]
dabei:
\begin{itemize}
\item $\varepsilon^{123} = \varepsilon^{231} = \varepsilon^{321} = 1$
\item $\varepsilon^{213} = \varepsilon^{132} = \varepsilon^{321} = -1$
\item sonst 0 ($\varepsilon^{ijk} = 0$, falls zwei Indizes gleich)
\end{itemize}
Alternativ:
\begin{itemize}
\item \[\abs{\v c} = \abs{\v a}\abs{\v b}\abs{\sin{\theta}}\]
\item Richtung von $\v c$ definiert durch $\v c \perp \v a \wedge \v c \perp \v c$
\item Vorzeichen von $\v c$ ist so, dass $\v a, \v b, \v c$ ein "Rechtssystem" bilden
\end{itemize}
\paragraph{{\bfseries\sffamily TODO} Skizze}
\label{sec-2-9-0-1}
\subsection{Binormalenvektor}
\label{sec-2-10}
\[\b B = \v T\times\v N\]
$\v T, \v N, \v B$ heißen "begleitendes Dreibein" und bilden ein Rechtssystem. alle haben Länge 1
\(\v T, \v N\) spannen die "Schmiegebene" auf
\subsubsection{Zur Information}
\label{sec-2-10-1}
\[\f{\d\v T}{\d s} = \frac{1}{\rho}\v N;~\f{\d \v B}{\d s} = -\f{1}{\sigma}\v B;~\f{\d\v N}{\d s}=\f{1}{\sigma}\v B - \f{1}{\rho}\v T\]
$\sigma$ definiert die Torsion.
\section{Grundbegriffe der Newtonsche Mechanik}
\label{sec-3}
\subsection{Newtonsche Axiome}
\label{sec-3-1}
Dynamik: Ursachen der Bewegungsänderung $\rightarrow$ Kräfte: $\v F = (F^1,F^2,F^3)$
\begin{enumerate}
\item Es existierten Inertialsysteme (Koordinatensysteme in denen eine Punktmasse an der keine Kraft wirkt) nicht oder sich geradlinig gleichförmig bewegt: $\ddot{\v x} = 0$
\item In solchen Systemen gilt: $\v F = m\ddot{\v x}$
\item Für Kräfte zwischen zwei Massenpunkten gilt:
\[\v{F}_12 = -\v{F}_21\]

\item definiert die \textbf{träge} Masse
\end{enumerate}
Die entscheidende physikalische Aussage von 2. ist das Auftreten von $\ddot{\v x}$ (nicht etwa $\dot{\v x}$ oder $\dddot{\v x}$)
Alternative Diskussionen der obigen Axiomatik:
\begin{itemize}
\item zum Beispiel Kapitel 1.2 von Jose/Saletan (mit $2$ Definition der Kraft)
\end{itemize}
\subsection{Trajektorie}
\label{sec-3-2}
Vorhersagen erfordern: $\v F \rightarrow \text{Trajektorie}$. Genauer: Sei $\v F(\v x,t)$ gegeben. Berechne $\v x(t)$ !
\subsection{Differentialgleichungen}
\label{sec-3-3}
hier nur "gewöhnliche DGL" (nur Ableitungen nach einer Variable) (im Gegensatz zu "partiellen" (Ableitung nach verschiedenen Variablen))
\subsubsection{1. Ordnung}
\label{sec-3-3-1}
Die allgemeine Form einer gewöhnlichen Dgl. 1. Ordnung ($\implies$ nur 1. Ableitung):
\[y'(x) = f(x,y)\]
\paragraph{Lösung}
\label{sec-3-3-1-1}
Funktion: $y:x\mapsto y(x)$ mit $y'(x) = f(x,y(x))$ (im Allgemeinen wird $x$ aus einem gewissen Intervall kommen: $x\in I\equiv (a,b)\subseteq \mathbb{R})$
\subsubsection{Anfangswertproblem}
\label{sec-3-3-2}
Gegeben durch:
\begin{enumerate}
\item Dgl.: $y' = f(x,y)$
\item Anfangsbedingung $(x_0;y_0) \in \mathbb{R}^2$
\end{enumerate}
Gesucht: Funktion $y(x)$ mit (für $x\in I, x_0 \in I$:
\begin{enumerate}
\item $y'(x) = f(x,y(x))$
\item $y(x_0) = y_0$
\end{enumerate}
\subsubsection{partielle Ableitung}
\label{sec-3-3-3}
Wir betrachten ab sofort auch Funktionen mehrerer Variablen: $f:\mathbb{R}\times\mathbb{R}\to\mathbb{R},(x,y)\mapsto f(x,y)$
Partielle Ableitung: \[\f{\partial f(x,y)}{\partial y} \equiv \lim_{\Delta y \to 0} \f{f(x,y + \Delta y) - f(x,y)}{\Delta y}\]
Rechenregeln: Wie bei normalen Ableitung, nur mit $x$ fest.
\paragraph{Beispiel}
\label{sec-3-3-3-1}
\[f(x,y,z) \equiv x^2 + y z\]
\[\f{\partial f}{\partial x} = 2x\]
\[\f{\partial f}{\partial y} = z\]
\[\f{\partial f}{\partial z} = y\]
\subsubsection{Existenz und Eindeutigkeit}
\label{sec-3-3-4}
\ldots{} viele Theoreme über Existenz und Eindeutigkeit (Peano und Picand / Lindelöf)
Insbesondere sind Existenz und Eindeutigkeit gesichert falls:
\[f(x,y) \wedge \f{\partial f(x,y)}{\partial y}\]
stetig sind.
\paragraph{"Begründung"}
\label{sec-3-3-4-1}
Zeichne an jedem Punkt $(x,y)$ einen Vektor $(1,f(x,y))$ ein.
\[\f{\d y(x)}{\d x} = y'(x) = f(x,y(x)) = \f{(x,y(x))}{1}\]
\paragraph{Weiteres Argument für die Existenz und Eindeutigkeit TODO(Skizze)}
\label{sec-3-3-4-2}
Steigung der gesuchten Funktion bei $x_0$ ist bekannt als $f(x_0, y_0)$
$\implies$ kann Wert der Funktion bei $x + \Delta x$ abschätzen: $y_0 + \Delta x f(x_0,y_0)$ (für kleine $\Delta x$)
Kenne Steigung bei $x_0 \Delta x: f(x_0 + \Delta x, y_0 + \Delta x f(x_0,y_0))$
$\implies$ Schätze Wert der Funktion bei $x_0 + 2\Delta x$ ab. ($\implies$ perfekt für Numerik)
\subsubsection{Beispiele}
\label{sec-3-3-5}
\begin{enumerate}
\item \[y'(x) = f(x,y), f(x,y) = 3\]
\[y'(x) = 3 \implies y(x) = \int 3\d x = 3 x + c\]
Das ist schon die allgemeine Lösung der Dgl.
Ein Anfangswertproblem, zum Beispiel mit $(x_0, y_0) = (-1,1)$ lässt sich durch Bestimmen der Konstanten lösen:
\[y(x) = 3 x + c \implies 1 = 3(-1) + c \implies c = 4 \implies y(x) = 3x + 4\]
\end{enumerate}
\subsubsection{Separation der Variablen}
\label{sec-3-3-6}
Separation der Variablen funktioniert wenn $f(x,y) = g(x)h(y)$
\paragraph{Beispiel}
\label{sec-3-3-6-1}
\[f(x,y) = \f{x}{y} \implies y'(x) = \f{x}{y(x)}\]
\[\f{\d x}{\d x} = \f{x}{y} \implies y\d y = x\d x\]
Variablen sind getrennt, kann einfach Integrieren
\[\int y\d y = \int x\d x \implies \f{y^2}{2} = \f{x^2}{2} + c \implies y = \pm \sqrt{x^2 + 2c}\]
\subparagraph{Lösen allgemeines Anfangswertproblem}
\label{sec-3-3-6-1-1}
allgemeines Anfangswertproblem mit Anfangsbedingung $(x_0,y_0)$
\[y_0^2 = x_0^2 + 2c \implies 2c = y_0^2 - x_0^2 \implies y = \begin{cases} \sqrt{y_0^2 + x^2 - x_0^2} & y_0 \geq 0 \\ -\sqrt{y_0^2 + x^2 - x_0^2} & y_0 \leq 0 \end{cases}\]
\begin{enumerate}
\item {\bfseries\sffamily TODO} Skizze
\label{sec-3-3-6-1-1-1}
\end{enumerate}
\subsubsection{System von Dgl.}
\label{sec-3-3-7}
(Fast) alles oben gesagte funktioniert auch für Systeme gewöhnlicher Dgl. 1. Ordnung:
\[\f{\d y^1(x)}{\d x} = f^1(x,y^1,\ldots,y^n)\]
\[\f{\d y^n(x)}{\d x} = f^n(x,y^n,\ldots,y^n)\]
Vektorschreibweise:
\[\f{\d \v y}{\d x} = \v f(x,\v y)\]
Wir haben hier eine vektorwertige Funktion von $n+1$ Variablen benutzt:
\[\v f:\mathbb{R}\times\mathbb{R}^n\to \mathbb{R}^n\]
Anfangsbedingungen: $(x_0,\v{y}_0)$ $\rightarrow$ $n+1$ Parameter. Einer davon entspricht der Verschiebung entlang ein und derselben Lösung $\implies$ allgemeine Lösung hat $(n + 1) - 1 = n$ Parameter oder Integrationskonstanten.
\subsubsection{Systeme von $n$ gewöhnlicher Dgl. p-ter Ordnung}
\label{sec-3-3-8}
\[\v{y}^{(p)}(x) = \v f(x,\v y,\v{y}',\v{y}'',\ldots,\v{y}^{(p-1)})\]
Anfangsbedingungen: $(x_0,\v{y}_0,\v{y}_0',\ldots,\v{y}_0^{(p - 1)}),\v{y}_0' \estimates \v{y}'(x)$ bei $x = x_0$ \\
\paragraph{Tatsache}
\label{sec-3-3-8-1}
Systeme von Dgl. können auf größere Systeme niedrigerer Ordnung zurückgeführt werden.
Wir illustrieren dies am Beispiel mit $p = 2$
\paragraph{Beispiel}
\label{sec-3-3-8-2}
\[\v{y}''(x) = \v{f}(x,\v{y},\v{y}')\]
Dies ist äquivalent zu einem System von $2n$ Dgl 1. Ordnung
\begin{equation}
\begin{cases}
\v{z}'(x) &= \v{f}(x,\v{y},\v{z}) \\
\v{y}'(x) &= \v z \tag{$\equiv g(x,\v y, \v z)$}
\end{cases}
\end{equation}
Ursprüngliche Form folgt durch Einsetzen der 2. Gleichung in die Erste.
Das verallgemeinert sich sofort auf die Ordnung $p$: Man gibt einfach der $(p - 1)$ niederen Ableitungen neue Namen und betrachtet sie als neue Variablen. Die zusätzlichen Dgl sind schlicht die Aussagen, dass es sich dabei immer noch um die ehemaligen Ableitungen handelt. \\
     $\implies$ System von $p$ Dgl 1. Ordnung; allgemeine Lösung hat $p$ Parameter
\subsubsection{Erste physikalische Beispiele}
\label{sec-3-3-9}
\paragraph{Punktmasse}
\label{sec-3-3-9-1}
3 Dgl 2. Ordnung: \[\ddot{\v x} = \f{1}{m}\v F(t,\v x,\dot{\v x})\]
$\implies$ 6 Dgl 1. Ordnung:
\begin{equation}
\begin{cases}
\dot{\v v} &= \f{1}{m}\v F(t,\v x,\v v) \\
\dot{\v x} &= \v v
\end{cases}
\end{equation}

In vielen Fällen: (zeitunabhängiges) Kraftfeld $\v F(\v x)$ ("Vektorfeld").
\subparagraph{Darstellung in $d = 2$ (Skizze Vektorfeld).}
\label{sec-3-3-9-1-1}
wichtig: doppelte Markierung der Achsen
\subparagraph{Einfachster Fall ($d = 1$)}
\label{sec-3-3-9-1-2}
betrachte den Fall, dass $F$ von $v$, aber nicht von $t$ abhängt:
\begin{equation}
\begin{cases}
\dot v &= \frac{F(x,v)}{m} \\
\dot x &= v
\end{cases}
\end{equation}
\[\cvec{v ; x} = \cvec{\frac{F(x,v)}{m} ; v}\]
\begin{enumerate}
\item {\bfseries\sffamily TODO} Darstellung im Phasenraum
\label{sec-3-3-9-1-2-1}
Analyse im Phasenraum passt perfekt zur früheren allgemeinen Analyse von Dgl 1. Ordnung
Analog in $d = 3$: Vektorfeld: $(\f{\v F}{m}, \v v)$, Phasenraum $(\v x, \v v)$ oder $(\v x, \v p)$ ist 6-dimensional
\end{enumerate}
\subparagraph{Harmonischer Oszillator ($d = 1$)}
\label{sec-3-3-9-1-3}
$F(x) = -k x$
\begin{equation}
\begin{cases}
\dot v &= -x \\
\dot x &= v
\end{cases}
\end{equation}
\begin{tikzpicture}
\begin{axis}[title={Phasenraum des Harmonischen Oszillator},domain=-2:2,view={0}{90},axis background/.style={fill=white}]
\addplot3[blue,quiver={u={y},v={-x},scale arrows=0.3},-stealth,samples=15] {y-x};
\end{axis}
\end{tikzpicture}
\subparagraph{Freier Fall mit Luftwiderstand}
\label{sec-3-3-9-1-4}
Aufgabe: Bestimme die zeitliche Entwicklung von $v$ wenn Körper im Schwerefeld losgelassen wird. $F_R = -cv^2$ \\
      Problem $1-dim$: x wachse nach unten, Start bei $t = 0, x = 0, \dot{x} = 0$
\[F=m\ddot x \implies m g - c \dot{x}^2 = m\ddot x \implies \begin{cases} m g - cv^2 &= m \dot{v} \\ v &= \dot{x} \end{cases}\]
Erste Gleichung enthält kein $x$ und kann unabhängig gelöst werden:
\begin{align*}
\frac{\d v}{\d t} &= g - \frac{c}{m}v^2 \\
\d t &= \frac{\d v}{g - \frac{c}{m}v^2}
\end{align*}
Konstanten und Dimensionen
\[[g] = \si{\meter\per\second\squared};[\frac{c}{m}] = \si{\newton\per\kilo\gram\per\meter\squared\second\squared}\]
Kann leicht Konstanten der Dimension Zeit und Geschwindigkeit bilden:
\[\hat{t} = \sqrt{\frac{m}{g c}},\hat{v} = \sqrt{\frac{g m}{c}}\]
Benutze jetzt die dimensionslosen Variablen $t' = \frac{t}{\hat{t}},v'=\frac{v}{\hat{v}}$
\[\implies \d t' = \frac{d v'}{1 - v^{2\prime}} = \frac{\d v'}{2}(\frac{1}{1 + v'} + \frac{1}{1 - v'})\]
\[2t' = \ln{1 + v'} - \ln{1 - v'} + c\]
$v' = 0$ bei $t' = 0 \implies c = 0$
Auflösen nach $v'$: \[e^{2t'} = \frac{1 + v'}{1 - v'} \implies \ldots\]
\[\implies v' = 1 - \frac{2}{e^{2t'} + 1} \implies v = \hat{v}(1 - \frac{2}{e^{\frac{2t}{\hat{t}}}} + 1)\]
$\implies \hat{v}$ ist Grenzgeschwindigkeit, wird exponentiell angenommen, wenn $t \gg \hat{t}$ \\

Zugabe: einfache physikalische Argumente für die Größe von $c$:
\begin{enumerate}
\item $[c] = \si{\kilo\gram\per\meter}$, Input: $A$ (Querschnitt), $\rho_L$
         $\implies c \sim \rho_L A$
\item Energiebilanz an verdrängter Luft: \[F_R\cdot l \sim E_{\text{kin,Luft}}\sim\rho_L l A \frac{v^2}{2}\]
\end{enumerate}
\subsection{Taylorentwicklung}
\label{sec-3-4}
Ohne Beschränkung der Allgemeinheit $x_0 = 0$. Untersuche Verhalten beliebiger glatter Funktionen $f(x)$ nahe $x = 0$
\begin{align*}
f(x) &= f(0) + \int_0^x\d x' f'(x') \\
&= f(0) + f'(x')(x_ - x)\Big|_0^x - \int_0^x\d x' f''(x')(x'-x) \\
&= f(0) + f'(0)x - f''(x')\frac{(x' - x)}{2}\Big|_0^x + \int_0^x\d x' f'''(x')\frac{(x' - x)^2}{2} \\
&= f(0) + f'(x)x + f''(0)\frac{x^2}{2} + \ldots
\end{align*}
Allgemein:
\[f(x) = f(0) + \sum_{n=1}^m f^{(n)}(0)\frac{x^n}{n!}+\overbrace{\int_0^x \d x' f^{(m+1)}(x')\frac{(x' - x)^m}{m!}}^{\text{Restglied}}\]
Falls das Restglied für $n\to\infty$ verschwindet:
\[f(x) = f(0) + \sum_{n=1}^\infty f^{(n)}(0)\frac{x^n}{n!}\]
Analog:
Taylor-Reihe: \[f(x) = f(x_0) + \sum_{n=1}^\infty f^{(n)}(x_0)\frac{(x - x_0)^n}{n!}\]
\begin{enumerate}
\item Oft erste Terme = gute Näherung
\item Verallgemeinerung auf viele Variablen
\end{enumerate}
\subsubsection{Interessantes "Gegenbeispiel"}
\label{sec-3-4-1}
\[f(x) \equiv \begin{cases} e^{-\frac{1}{x^2}} & x\neq 0 \\ 0 & x = 0 \end{cases}\]
Überzeugen sie sich, dass alle Ableitungen existieren, auch bei Null! \\
    Sie Brauchen:
\[\lim_{x\to 0}\frac{1}{x^n}e^{-\frac{1}{x^2}} = 0\]
Die Ableitungen verschwinden sogar bei Null $\implies$ Taylor-Reihe ist Null, keine gute Näherung
\subsection{Harmonischer Oszillator}
\label{sec-3-5}
\begin{itemize}
\item eines der wichtigsten physikalischen Systeme
\item beschreibt viele kompliziertere Systeme angenähert
\end{itemize}
\subsubsection{Eindimensionales System}
\label{sec-3-5-1}
$d = 1, F = F(x)$
\[F(x) = -\dd{}{x}v(x) = -v'(x)\]

Damit haben wir das \textbf{Potential} ($\rightarrow$ beschreibt die potentielle Energie des Massenpunktes) $v$ als Stammfunktion von $-F$ definiert
\begin{itemize}
\item Skizze
\end{itemize}
Massenpunkt kann nur ruhen, wo $F=0$ beziehungsweise $V'=0$. Genauer: Nur Minima (Maxima instabil). \\
\paragraph{Ziel}
\label{sec-3-5-1-1}
Untersuchung der Bewegung in der Nähe von Minimal (also bei $x\approx x_0$ wobei $v'(x_0) = 0$ gelte) \\
     $V(x)$ bei $x_0$, $V'(x_0) = 0, \abs{x - x_0}$ klein \\
     \[\implies V(x) \simeq V(x_0) + \frac{1}{2}v''(x_0)(x-x_0)^2\]
\[\implies F(x) \simeq - V''(x_0)(x-x_0)\]
\[x-x_0\equiv y \implies \underbrace{F(y) = -k y}_{\text{harmonischer Oszillator}}, k\equiv v''(0)\]
Wir sehen: Harmonischer Oszillator ist eine Idealisierung von potentiell sehr großem Nutzen (viele Systeme)
\paragraph{Lösung}
\label{sec-3-5-1-2}
Newton $\implies m\ddot{y} = -ky$ beziehungsweise $\ddot{y} = -\omega^2 y,\omega\equiv\sqrt{\frac{k}{m}}$ \\
     $\implies \sin{\omega t}$ und $\cos{\omega t}$ sind Lösungen \\
     $\implies y(t) = A\sin{\omega t} + B\cos{\omega t}$ ist auch Lösung (wegen Linearität) \\
     (wegen der beiden frei wählbaren Konstanten ist dies schon die allgemeine Lösung)
\paragraph{Verallgemeinerungen}
\label{sec-3-5-1-3}
\begin{itemize}
\item Reibungsterm $\sim \dot{y}$
\item treibende Kraft $\sim f(t)$
\end{itemize}
\subsection{Lineare Differentialgleichungen}
\label{sec-3-6}
allgemeine Form einer linearen Dgl. n-ter Ordnung:
\[y^{(n)} + f_{n -1}(x)y^{(n - 1)}(x) + \ldots + f_0(x)y(x) = f(x)\]
Das Wort linear bezieht sich nur auf $y$, nicht $x$ \\
   Die Dgl. heißt homogen falls $f(x)\equiv 0$
Homogen von Grad $p$: Ersetzung $y\to\alpha y$ führt zu Vorfaktor $\alpha p$, hier $p = 1$
\begin{itemize}
\item wir hatten oben dem Fall $n = 2$ "mit konstanten Koeffizienten"
\item noch einfacheres Beispiel: $n = 1, f\equiv 0$ (aber beliebige Koeffizienten)
\[y' + a(x)y = 0\]
Das ist separabel:
\[\dd{y}{x} + a(x) y = 0\]
\[\dd{y}{x} = -a(x) y\]
\[\frac{\d y}{x} = -a(x) \d x\]
\[\int\frac{\d y}{y} = - \int a(x)\d x\]
\[\ln{y} - A(x) + c_1\]
\[y = c e^{-A(x)}\]
$A(x)$ sei eine beliebige aber fest gewählte Stammfunktion von $a$
Wir können den inhomogenen Fall lösen, durch "Variation der Konstanten"
\begin{itemize}
\item Ansatz: $y = C(x)e^{-A(x)}$, Dgl. $y' + ay = f$
       \[(c e^{-A})' + a C e^{-A} = f\]
       \[c' e^{-A} - C A' e^{-A} + C a e^{-A} = f\]
       Beachte $A' = a$
       \[\implies c'e^{-A} = f e^{A},c(x) = \int\d x f(x) e^{A(x)}\]
       \[y(x) = \left[\int^x\d x' f(x') e^{A(x')}\right] e^{-A(x)}\]
       $f(x')$ ist eine frei wählbare additive Konstante im $x'$-Int. ($C(x)\to C(x) +\alpha$) entspricht der Addition der Lösung der homogenen Dgl.
\end{itemize}
\end{itemize}
\subsubsection{Zusammenfassung / Verallgemeinerung auf $n > 1$}
\label{sec-3-6-1}
\begin{defn}[Linear Unabhängig]
Ein Satz von Funktionen $f_1(x),\ldots,f_n(x)$ heißt linear unabhängig, falls jede Linearkombination bei der nicht alle Koeffizienten Null sind auch nicht Null ist:
\[\alpha_1 f_1(x) +\ldots \alpha_n f_n(x)\equiv 0 \implies \alpha_1 = \ldots = \alpha_n = 0\]
(identisch zur linearen Unabhängigkeit von Vektoren)
\end{defn}
\paragraph{Fakt}
\label{sec-3-6-1-1}
Kennt man $n$ linear unabhängige Lösungen einer homogenen linearen Dgl. $n$-ter Ordnung, so kennt man die allgemeine Lösung:
\[y_{hom}(x) = C_1 y_1(x) + \ldots + C_n y_n(x)\]
Die allgemeine Lösung ist stets von dieser Form.

Wenn wir außerdem eine \textbf{partikuläre} Lösung der inhomogenen Gleichung haben, so haben wir auch schon deren allgemeinen Lösung
\begin{align*}
y(x) = y_{hom}(x) + y_{part}(x)
\intertext{"Beweis" durch Einsetzen in}
y^{(n)} + f_{n - 1}y^{(n - 1)} + \ldots + f_0 y = f
\end{align*}
\subsubsection{Finden der partikulären Lösung}
\label{sec-3-6-2}
Auch bei $n > 1$: Variation der Konstanten (Funktioniert gut bei konstanten Koeffizienten)
Mächtigere Methoden: Überführen von System von linearen Dgl. 1. Ordnung (braucht Matrixrechnung)
\section{Erhaltungssätze in Newtonscher Mechanik}
\label{sec-4}
\subsection{Impulserhaltung}
\label{sec-4-1}
Systeme mit mehreren Massenpunkten $a,b\in\{1,\ldots, n\}$ \\
   Trajektorien: $\v x_a(t), a=1,\ldots,n$ \\
\begin{thm}[Impulserhaltung]
Bei verschwindenden externen Kräften ($\v F_{ext} = 0$) gilt:
\[\v p \equiv \sum_a \v{P_a} \equiv \sum_a m_a\dot{\v{x_a}} =~\text{const}\]
\end{thm}
\begin{proof}
\begin{align*}
\dot{\v p} &= \sum_a m_a \ddot{\v{x_a}} \\
&= \sum_a \v{F_a} \\
&= \sum_a(\sum_{\substack{b \\ a\neq b}} \v{F_{ab}}) \\
&= \sum_{\substack{a,b \\ a\neq b}} \v{F_{ab}} \tag{Summe über alle Paare von $a,b$} \\
&= \sum_{a > b}\v{F_{ab}} + \sum{a < b} \v{F_{ab}} \\
&= \sum_{a > b}(\v{F_{ab}} + \v{F_{ba}})
&\underarrow[=]{3. Newtonsches Axiom} 0 \tag*{\qedhere}
\end{align*}
\end{proof}

mit äußeren Kräften:
\[\dot{\v p} = \sum_a \v F_{a,ext.} \equiv \v F_{ext}\]
Falls zum Beispiel die äußere Kraft nicht in $x^1$ -Richtung wirkt (F$^{\text{1}}_{\text{ext}}$ = 0), so gilt immer noch $p^1 = \text{const}$
(eigentlich drei Erhaltungssätze für $p^1, p^2,p^3$, manchmal gelten nur einige davon)
\subsection{Drehimpulserhaltung}
\label{sec-4-2}
Oft: Kräfte wirken parallel zur Verbindungslinie zweier Massenpunkte:
\begin{itemize}
\item Gravitationskraft
\item Elektrostatische Kraft
\item Modell der masselosen Stange ($\rightarrow$ Modell für starre Körper!)
\end{itemize}
\begin{defn}[Drehimpuls]
\begin{align*}
\v L_a &\equiv \v x_a \times \v p_a \\
(\v L_a)^i &= \varepsilon^{ijk}x_a^j p_a^k
\end{align*}
\end{defn}
Falls $\v F_{a,ext} = 0$ und alle interne Kräfte wirken parallel zur Verbindungslinie der jeweiligen Punkte, dann gilt \textbf{Drehimpulserhaltung}
\begin{thm}[Drehimpulserhaltung]
\[\v L \equiv \sum_a \v L_a = \sum_a m_a \v x_a \times \dot{\v x}_a = \sum_a \v x_a \times \v p_a = ~\text{const}\]
\end{thm}
\begin{proof}
Nachrechnen:
\begin{align*}
\dot{\v L} &= \sum_a m_a (\dot{\v x}_a \times \dot{\v x}_a + \v x_a + \ddot{\v x}_a) \\
&= \sum_a \v x_a \times \v F_a \\
&= \sum_{a\neq b} \v x_a \times \v F_{ab} \tag{Summe über alle Paare von $a,b, a\neq b$} \\
&= \sum_{a > b}(\v x_a \times \v F_{ab} + \v x_b \times \v F_{ba}) \\
&= \sum_{a > b} (\v x_a - \v x_b)\times \v F_{ab} \\
\intertext{da $\v F_{ab} \parallel (\v x_a - \v x_b)$ per Annahme} \\
&= 0 \tag*{\qedhere}
\end{align*}
\end{proof}
Bei externen Kräften:
\[\dot{\v L} = \sum_a \v x_a \times \v F_{a,ext} \equiv \v M_{ext}\]
$M_{ext}$ ist das durch äußere Kräfte auf Punkt $a$ ausgeübte \textbf{Drehmoment}, allgemein (für einzelnen Punkt):
\[\v M = \v x \times \v F = \dot{\v L}\]

Wichtig: Drehimpulserhaltung gilt auch dann wenn alle äußeren Kräfte \textbf{Zentralkräfte} sind, Zentralkraft:
\[\v F_a \parallel \v x_a\]

Drehimpuls hängt vom Koordinatensystem ab.
\begin{remark}
$\v L \equiv \v x\times \v p$ (allgemeiner jedes Kreuzprodukt von Vektoren) \\
ist ein \textbf{Axial-} oder \textbf{Pseudovektor}, das heißt: Bei Drehungen wie Vektor, Bei Reflexion am Ursprung kein Vorzeichenänderung
\begin{proof}
\[\v a \to -\v a, \v b \to -\v b \implies \v a \times \v b \to + \v a \times \v b \qedhere\]
\end{proof}
\end{remark}
\subsection{Konservative Kräfte und Energieerhaltung}
\label{sec-4-3}
\begin{defn}[Gradient]
Gradient von $V$:
\[\v\nabla \equiv \left(\frac{\partial V}{\partial x^1}, \frac{\partial V}{\partial x^2}, \frac{\partial V}{\partial x^3}\right)\]
$\frac{\partial}{\partial x}$ ist ein "Differentialoperator", also:
\[\frac{\partial}{\partial x}:f(x,y)\mapsto \frac{\partial f(x,y)}{\partial x}\]
Dementsprechend $\frac{\partial^2}{\partial x^2}$ ist ein "Differentialoperator" zweiter Ordnung, also:
\[\frac{\partial^2}{\partial x^2}:f(x,y)\mapsto \frac{\partial^2 f(x,y)}{\partial x^2}\]
$\v\nabla V$ ist gute Schreibweise, weil $\v\nabla$ ein vektorwertiger Differentialoperator ist:
\[\v\nabla = \left(\frac{\partial}{\partial x^1}, \frac{\partial}{\partial x^2}, \frac{\partial}{\partial x^3}\right)\]
\end{defn}
\begin{defn}[konservatives Kraftfeld]
Ein zeitunabhängiges Kraftfeld $\v F(\v x)$ heißt \textbf{konservativ} falls es eine Funktion $V(\v x)$ ("Potential") gibt. sodass
\[\v F = -\v \nabla V\]
\end{defn}
\subsubsection{Energieerhaltung}
\label{sec-4-3-1}
Für einen Massenpunkt in einem konservativen Kraftfeld gilt:
\[E = \underset{\text{kinetisch}}{T} + \underset{\text{potentielle Energie}}{V} = \frac{m}{2}\dot{\v x}(t)^2 + V(\v x(t)) = \text{const}\]
\paragraph{Begründung}
\label{sec-4-3-1-1}
\begin{align*}
\dd{T}{t} &= \frac{m}{2}\dd{}{t}(\dot{x}^i\dot{x}^i) = \frac{m}{2}2 \dot{x}^i \ddot{x}^i = m\dot{\v x}\ddot{\v x} \\
\dd{V}{t} &= \lim_{\Delta t \to 0} \frac{V(x^1 + \Delta x^1, x^2 + \Delta x^2, x^3 + \Delta x^3) - V(x^1, x^2, x^3)}{\Delta t} \\
\intertext{mit $\Delta x = \dd{\v x}{t}\Delta t$} \\
\intertext{Umschreiben des Zählers} \\
&V(x^1 + \Delta x^1, x^2 + \Delta x^2, x^3 + \Delta x^3) - V(x^1, x^2 + \Delta x^2, x^3 + \Delta x^3) \\
+ &V(x^1, x^2 + \Delta x^2, x^3 + \Delta x^3) - V(x^1, x^2, x^3 + \Delta x^3) \\
+ &V(x^1, x^2, x^3 + \Delta x^3) - V(x^1, x^2, x^3) \\
&\cong \frac{\partial V}{\partial x^1}(x^1,x^2 + \Delta x^2, x^3 + \Delta x^3)\Delta x^1 + \frac{\partial V}{\partial x^1}(x^1,x^2,x^3 + \Delta x^3) \Delta x^2 + \frac{\partial V}{\partial x^1} (\v x)\Delta x^3
\intertext{Teilen durch $\Delta t$, Grenzwertbildung}
\dd{V}{t} &= \frac{\partial V}{\partial x^i}(\v x(t))\dd{x^i}{t} \\
\shortintertext{oder (allgemeine Rechenregel)}
\d V &= \frac{\partial V}{\partial x^i} \d x^i
\end{align*}
Allgemeine Formulierung der Rechenregel: Sei $f:\mathbb{R}^n \to\mathbb{R} \wedge \v x: \mathbb{R}\to\mathbb{R}^n$
Die Verknüpfung $f\circ \v x: \mathbb{R}\to \mathbb{R}$ ist eine Funktion. Für diese gilt:
\begin{align}
\underbrace{\d f}_{\mathclap{\text{totales Differential}}} &= \frac{\partial f}{\partial x^i}\d x^i = (\v\nabla f) \d \v x \\
\shortintertext{oder totale Ableitung:} \\
\dd{f}{t} &= \frac{\partial f}{\partial x^i} \frac{\d x^i}{\d t} \\
\shortintertext{Unsere Anwendung} \\
\dot{E} = m\dot{\v x} \ddot{\v x} + \frac{\partial V}{\partial x^i} \dot{x}^i = \v F \dot{\v x} + (\v\nabla V)\dot{\v x} = 0~\checkmark
\end{align}
\[V(x^1 + \Delta x^1, x^2 + \Delta x^2, x^3 + \Delta x^3) - V(x^1, x^2 + \Delta x^2, x^3 + \Delta x^3)\]
Vergleiche:
\[f(x + \Delta) - f(x) \cong f'(x)\Delta\]
\subsubsection{Kriterium für Konservativität}
\label{sec-4-3-2}
Für *einfach zusammenhängende Gebiete*\footnote{Jede geschlossene Kurve kann auf Länge Null zusammengezogen werden} gilt:
\[\v F ~\text{ist konservativ}~ \iff \v\nabla\v F = 0\]
\paragraph{Begründung}
\label{sec-4-3-2-1}
$\implies$ \\ \[\v F = -\v\nabla V \implies \underbrace{\v\nabla\times\v F}_{\mathclap{\equiv~\text{Rotation von $F$ ($\rot{F}$)}}} = 0\]
\begin{align*}
(\v\nabla\times\v F)^i &= \varepsilon^{ijk} \frac{\partial}{\partial x^j} F^k = \varepsilon^{ijk}\partial^i F^k \\
&= -\varepsilon^{ijk}\partial^j\partial^k V = -\frac{1}{2}(\varepsilon^{ijk} -\varepsilon^{ikj})\partial^j\partial^k V \\
&= - \frac{1}{2}\varepsilon^{ijk}\partial^j \partial^k V + \frac{1}{2}\varepsilon^{ikj}\underbrace{\partial^k\partial^j}_{\mathclap{\text{habe benutzt}~\frac{\partial}{\partial x}\frac{\partial}{\partial y} = \frac{\partial}{\partial y}\frac{\partial}{\partial x}}} V \\
&\underarrow[=]{$k \leftrightarrow j$} -\frac{1}{2}\varepsilon^{ijk}\partial^j\partial^k V + \frac{1}{2}\varepsilon^{ijk}\partial^j\partial^k V = 0
\end{align*}
$\impliedby$ \\
     Wähle beliebiges festes $\v x_0$ im Gebiet. Definiere Potential als minus Arbeit am Massenpunkt $\rightarrow Abbildung$
\begin{align*}
V(\v x) &\equiv -\int_{\v x_0}^{\v x} \v F(x)\d\v s \tag{Linienintegral} \\
\intertext{Linienintegral kann immer definiert werden, wenn Kurve durch Gebiet mit Vektorfeld verläuft}
\d\v s &\equiv \d\v x(s) =(\dd{x^1}{\d x}, \dd{x^2}{\d x}, \dd{x^3}{\d x}) \d s \\
\intertext{Also gilt:}
\v F\d\v s &= \underbrace{F^i (\dd{x^i}{s})\d s}_{\mathclap{\text{Integrand im normalen Riemann Integral}}} \\
\intertext{Wähle beliebigen kleinen Vektor $\v l$ und berechne:}
\v l \v F(\v x) &\cong -(-\int_{\v x}^{\v x + \v l} \d\v s \v F) \\
&= -((-\int_{\v x_0}^{\v x + \v l} \d \v s \v F) - ( -\int_{\v x_0}^{\v x} \d\v s \v F)) \\
&= -(V(\v x + \v l) -V(\v x)) \\
&\cong - \frac{\partial V}{\partial x^i}l^i = -\v l(\v\nabla V) \\
&\implies \v l(\v F + \v\nabla V) = 0 \\
&\implies \v F + \v\nabla V = 0 \checkmark
\end{align*}
Lücke: Wegunabhängigkeit der Definition von $V$: \\
     Wähle zwei unterschiedliche Wege ($L_1, L_2$):
\begin{align*}
\int_{L_1}\d\v s \v F - \int_{L_2}\d \v s \v F = \underarrow[\oint]{\text{Rand von } $\Sigma$} \d\v s\v F \\
\intertext{Satz von Stokes}
&= \int_{\Sigma}\v{d f}(\v\nabla \times\v F)
\end{align*}
\[(\rot \v F)^i = (\v\nabla \times \v F)^i = \varepsilon^{ijk}\frac{\partial}{\partial x^j} F^k\]
zum Beispiel:
\[(\v\nabla\times\v F)^1 = \frac{\partial F^3}{\partial x^2} - \frac{\partial F^2}{\partial x^3}\]
\[\int_{L_2}\d \v s \v F - \int_{L_1}\d \v s\v F = \oint_{\partial \Sigma} \d\v s \v F \underarrow[=]{\text{"Stokes"}} \int_{\Sigma}\d\v f*(\v\nabla\times\v F) \overset{!}{=} 0\]
\subsection{Kurvenintegrale}
\label{sec-4-4}
Jedes Kurvenintegral kann durch Parametrisierung der Kurve berechnet werden.
Kurve $C \to \v x(t)$
\begin{align*}
\d \v x &\equiv \d \v s \\
\int_C \d \v s \v F(\v x) &\equiv \int_C \d \v x \v F(\v x) = \int_{t_1}^{t_2} \d t \frac{\d \v x(t)}{\d t} \v F(\v x(t))
\end{align*}
\subsection{Satz von Stokes}
\label{sec-4-5}
\begin{defn}[Satz von Stokes]
\[\oint \d \v s\v F = \int_{\Sigma} \d \v f (\v \nabla \times \v F)\]
\end{defn}
\begin{proof}
\begin{align*}
\oint \d\v s\v F &= \int_0^{\Delta x^1}\d s F^1(x,0) + \int_0^{\Delta x^2}\d s F^2(\Delta x^1, s) - \int_0^{\Delta x^1}\d s F^1(s, \Delta x^2) - \int_0^{\Delta x^2}\d s F^2(0,s) \\
&= \int_0^{\Delta x^1}\d s(F^1(s,0) - F^1(s,\Delta x^2)) + \int_0^{\Delta x^2}\d s(F^2(\Delta x^1, s) - F^2(0,s)) \\
&= \int_0^{\Delta x^1}\d s(\frac{\partial F^1}{\partial x^2})\Delta x^2 + \int_0^{\Delta x^2}\d s \frac{\partial F^2}{\partial x^1}\Delta x^1 + O(\Delta^3) \\
&= \Delta x^1 \Delta x^2(\frac{\partial F^2}{\partial x^1} - \frac{\partial F^1}{\partial x^2}) + O(\Delta^3) \\
&= \Delta x^1 \Delta x^2(\v\nabla \times \v F)^3 + O(\Delta^3) \\
&= \underbrace{\Delta x^1 \Delta x^2 \hat{e_3}}_{\mathclap{\Delta \v f = ~\text{Der dem kleinen Flächenelement zugeordnete Vektor}}} (\v\nabla \times \v F) \\
&\approx \Delta \v f(\v \nabla \times \v F)
\end{align*}
Allgemein steht $\Delta \v f$ oder $\d \v f$ für ein kleines oder infinitesimales Flächenelement, Länge $\estimates$ Größe der Fläche
Die Richtung des Vektors definiert \textbf{Orientierung} der Fläche (Zum Beispiel Oben = da, wo der Pfeil hin zeigt) \\
   Randkurve: so definiert, dass man von oben gesehen linksherum (mathematisch positiver Drehsinn) läuft
\begin{enumerate}
\item Spezielle Lage in unserer Rechnung unwichtig
\item Übergang zu größeren Flächen durch Aufaddieren
\end{enumerate}
Fläche $= N\Delta ^2 \implies N\sim \frac{1}{Delta^2}$
\begin{align*}
\sum_{\text{Rechtecke}} \oint\d \v s\v F = \sum_{\text{Rechtecke}} \int\d\v f(\v\nabla \times \v F) + \underarrow[N]{Zahl der Rechtecke = $O(\Delta)$} O(\Delta^3) \\
\shortintertext{weil sich nicht "innere Ränder wegheben"}
\oint\d \v s\v F = \sum_{\text{Rechtecke}} \int\d\v f(\v\nabla \times \v F) \\
\shortintertext{klar}
\oint\d \v s\v F = \int\d\v f(\v\nabla \times \v F) \\
\end{align*}
Glätten des Randes:
Zerlegung des Randes $\Delta \v s$ in kleine Rechtecke $\Delta \v s_1, \Delta \v s_2$
\begin{align*}
\Delta \v s &= \Delta \v s_1 + \Delta \v s_2 \\
\v F \Delta\v s &= \v F \Delta \v s_1 + \v F \Delta \v s_2 = \v F_1 \Delta \v s_1 + \v F_2 \Delta \v s_2 + O(\Delta x^2)
\end{align*}
$\v F, \v F_1, \v F_2$ jeweils am Mittelpunkt der Linienelemente
Zahl derartiger Randelemente $\sim \frac{1}{\Delta} \implies$ Fehler $O(\Delta)$ \\
   $\implies$ Auch nach Summation bleibt Fehler von $O(\Delta)$

Besser wäre Zerlegung in Simplices ("Haben sie mal versucht eine Schildkröte zu fliesen")
\end{proof}

Für unsere Anwendung: wichtig, dass jede geschlossene Kurve in einem einfach zusammenhängenden Gebiet, \textbf{Rand} ist.
\subsection{Energieerhaltung für Systeme von Massenpunkten}
\label{sec-4-6}
Massenpunkte: $\v x_a, a = 1,\ldots, n$ \\
   Kräfte: seien $\parallel$ zu $\v x_a - \v x_b$ ("Zentralkräfte") \\
   Solche Kräfte kann man stets schreiben als:
\begin{align*}
\v F_{ab} = -\v\nabla_a V_{ab}(\abs{\v x_a - \v x_b}) \\
\shortintertext{mit:}
V_{ab} = V{ba}, \v\nabla_a = (\frac{\partial}{\partial x^1_a}, \frac{\partial}{\partial x^2_a}, \frac{\partial}{\partial x^3_a}) \\
\shortintertext{dazu:}
-\v\nabla_a V_{ab} (\abs{\v x_a - \v x_b}) &= (-\v\nabla_a \abs{\v x_a - \v x_b}) V_{ab}'(\abs{\v x_a - \v x_b}) \\
\shortintertext{Dies zeigt:}
&= -\v\nabla_a \sqrt{(\v x_a - \v x_b)^2} \\
&= \frac{\v x_a - \v x_b}{\abs{\v x_a - \v x_b}}
\end{align*}
Wir können passendes $V$ für jede Zentralkraft finden. Man berechnet einfach $V'$ und sucht die Stammfunktion.

Prüfe Konsistenz mit 3. Axiom:
\[\underbrace{-\v\nabla_a V_{ab}(\abs{\v x_a - \v x_b})}_{\v F_{ab}} = + \v\nabla_b V_{ab}(\abs{\v x_a - \v x_b}) = \underbrace{+ \v\nabla_b V_{ba}(\abs{\v x_b - \v x_a})}_{-\v F_{ba}}\]

In diesem System gilt Energieerhaltung:
\[E = \sum_a T_a + \frac{1}{2}\sum_{a\neq b} V_{ab} = \sum_a T_a + \sum_{a < b} V_{ab} = ~\text{const}\]

Begründung:
\begin{align*}
\dot{E} &= \sum_a \dot{\v x}_a \v F_a + \frac{1}{2}\sum_{a\neq b}((\v \nabla_a V_{ab})\dot{\v x}_a + (\v\nabla_b V_{ab})\dot{\v x}_b) \\
&= \sum_{a\neq b} \dot{\v x}_a \v F_{ab} + \frac{1}{2}\sum_{a\neq b} ( -\v F_{ab}\dot{\v x}_a - \underbrace{\v F_{ab} \dot{\v x}_b}_{\mathclap{\text{Umbenennung $a\leftrightarrow b$}}}) = 0 \\
(& = W - \frac{1}{2} W - \frac{1}{2} W)
\end{align*}

Bemerkung: Passend gewähltes $V_{ab}$ gibt das Modell der starren Stangen
\subsection{Eindimensionale Bewegung}
\label{sec-4-7}
\[F(x) = m\ddot{x}\]
\begin{itemize}
\item mit Einsatz allgemein lösbar!
\item Startpunkt: Jedes 1-dim. zeitunabhängiges Kraftfeld ist konservativ
\begin{align*}
E = \frac{m}{2}\dot{x}^2 + V(x) = ~\text{const} \\
\intertext{(bis auf Vorzeichen)}
\dot{x} = \sqrt{\frac{2}{m}(E-V(x))} \implies \d t = \frac{\d x}{\sqrt{\frac{2}{m}(E-V(x))}} \\
t = \in \frac{\d x}{\sqrt{\frac{2}{m}}(E - V(x))}
\intertext{Integral lösen, Integrationskonstante und Energie so bestimmen, das Anfangswertproblem gelöst}
t = t(x) ~\text{auflösen}~ \implies x = x(t) \checkmark
\intertext{viel einfacher als allgemeine Differentialgleichung 2. Ordnung}
\end{align*}
\end{itemize}
\section{Harmonischer Oszillator in komplexen Zahlen}
\label{sec-5}
\subsection*{Motivation}
   Harmonischer Oszillator mit Reibung:
   \[\ddot{x} = -\omega^2 x - c \dot{x}\]
   Exponentieller Ansatz:
   \[x\sim e^{\alpha t} \implies \alpha^2 + \omega^2 + c\alpha = 0\]
   gesucht: $\alpha$, Betrachte Grenzfälle:
\begin{enumerate}
\item $\omega$ klein
\[\implies \alpha^2 + c\alpha = 0 \implies \alpha = -c \implies x\sim e^{-c t}\]
\item $\omega$ groß (beziehungsweise $c$ klein)
\[\alpha^2 + \omega^2 \simeq 0\]
nicht lösbar!
\end{enumerate}
Aber: wir wissen schon $\sin{\omega t}, \cos{\omega t}$ sind Lösungen.

Falls jede gesuchte Gleichung lösbar $\implies$ Hoffnung auf elegante allgemeine Lösung

\textbf{Speziell:} $\alpha^1 = -1$ (für $\omega = 1, c = 0$)
\subsection{Komplexe Zahlen}
\label{sec-5-1}
\subsubsection{Ziel}
\label{sec-5-1-1}
reelle Zahlen so zu erweitern, dass $x^2 = - 1$ lösbar
\subsubsection{Naive Definition}
\label{sec-5-1-2}
Definiere "Imaginäre Einheit" $x^2 = -1$ lösbar "$\I$", so dass $\I^2 = -1$
Wollen addieren und Multiplizieren, deshalb erkläre komplexe Zahl $\mathbb{C}$ als:
\[\mathbb{C} \ni z ) x + \I y, x,y\in\mathbb{R}\]
Wir definieren außerdem:
\begin{itemize}
\item $z_1 + z_2 = (x_1 + \I y_1) + (x_2 + \I y_2) \equiv (x_1 + x_2) + \I(y_1 + y_2)$
\item $z_1 \cdot z_2 = (x_1 + \I y_1) (x_2 + \I y_2) \equiv x_1 x_2 + x_1 \I y_2 + \I y_1 x_2 + \I y_1 \I y_2 \equiv (x_1 x_2 - y_1 y_2) + i(x_1 y_2 + x_2 y_1)$
\end{itemize}
\subsubsection{präzisere Definition}
\label{sec-5-1-3}
\begin{defn}[Körper]
Körper ("Field") ist eine Menge $K$ mit zwei binären Operationen $("+","\cdot")$, so dass:
\begin{itemize}
\item $\alpha + (\beta + \gamma) = (\alpha + \beta) + \gamma$ \hfill (Assoziativität)
\item $\alpha + \beta = \beta + \alpha$ \hfill (Kommutativität)
\item $\Exists 0 \in K: \alpha + 0 = \alpha \Forall \alpha$ \hfill (Null)
\item $\Forall \alpha \Exists (-\alpha) \in K: \alpha + (-\alpha) = 0$ \hfill (Additives Inverses)
\item $(\alpha \cdot \beta) \cdot \gamma = \alpha \cdot (\beta \cdot \gamma)$ \hfill (Assoziativität der Mult.)
\item $\alpha \cdot \beta = \beta \cdot \alpha$ \hfill (Kommutativität der Mult.)
\item $\Exists 1 \in K: 1\cdot \alpha = \alpha \Forall \alpha$ \hfill (Eins)
\item $\Forall \alpha \neq 0 \Exists \alpha^{-1} \in K: \alpha \cdot \alpha^{-1} = 1$ \hfill (Inverses der Mult.)
\item $\alpha\cdot (\beta + \gamma) = \alpha \cdot \beta + \alpha \cdot \gamma$ \hfill (Distributivität)
\end{itemize}
\end{defn}
Wir kennen bereits:
\begin{itemize}
\item $K = \mathbb{Q}$ \hfill (rationale Zahlen)
\item $K = \mathbb{R}$ \hfill (reelle Zahlen)
\end{itemize}
\begin{defn}[Komplexer Zahlenkörper]
Komplexe Zahlen sind die Menge $\mathbb{C} = \mathbb{R} \times \mathbb{R} = \mathbb{R}^2$ mit den Operationen
\begin{itemize}
\item $(x_1, y_1) + (x_2, y_2) \equiv (x_1 + x_2, y_1 + y_2)$
\item $(x_1, y_1) \cdot (x_2, y_2) \equiv (x_1 \cdot x_2 - y_1 \cdot y_2, x_1 \cdot y_2 + x_2 \cdot y_1)$
\end{itemize}
Das ist äquivalent zu unserer "naiven Definition" $z = x + \I y$
\end{defn}
\textbf{Aufgabe:} Prüfen sie, dass die Axiome erfüllt sind! \\
    Schwierigster Teil: Multiplikations-Inverses, Idee / Vorschlag:
\[z^{-1} = (x + \I y)^{-1} = \frac{x}{x^2 + y^2} - \I \frac{y}{x^2 + y^2}\]

$\mathbb{C} = \mathbb{R}^2 \implies$ Darstellung durch Vektoren in Ebene liegt nahe.
\begin{itemize}
\item Addition: $\equiv$ Vektoraddition
\item Multiplikation: Beträge der Vektoren werden multipliziert, Winkel "$\arg$" werden addiert.
\item $\arg{z} = \phi$
\item $\Re{z} = x$
\item $\Im{z} = y$
\item $\abs{z} = \sqrt{x^2 + y^2}$
\end{itemize}

Übliche Funktionen ($\exp, \ln, \sin, \cos$) können mittels ihrer in $\mathbb{R}$ bekannten Taylorreihe auf $\mathbb{C}$ übertragen werden \\
    \textbf{Besonders wichtig:}
\[e^z \equiv \sum_{n = 0}^\infty \frac{z^n}{n!}\]
Brauchen $e^{z + w} = e^z \cdot e^w$ \\
    Nachrechnen:
\begin{align*}
e^{z + w} &= \sum_{n = 0}^\infty \frac{(z + w)^n}{n!} = \sum_{n = 0}^\infty \frac{1}{n!}\sum_{k = 0}^n \underarrow[\binom{n}{k}]{$\mathclap{\underset{\binom{n}{k} = \frac{n!}{k!(n -k)!}}{\text{Binominialkoeffizient}}}$} z^kw^{n - k}
\intertext{englisch: "n choose k"}
\intertext{durch Umschreiben der Summen erhält man:}
&= \sum_{n = 0}^\infty \sum_{k = 0}^n \frac{n^k}{k!} \frac{w^{n -k}}{(n -k)!} \\
&= \sum_{k = 0}^\infty \sum_{l = 0}^\infty \frac{z^k}{k!} \frac{w^l}{l!} = e^z e^w
\end{align*}
Insbesondere:
\[e^z = e^{x + \I y} = \underbrace{e^x}_{\mathclap{\text{reelle Zahl}}} \overbrace{e^{\I y}}^{\mathclap{\text{komplexe Zahl e vom Betrag $1$}}}\]
In der Tat:
\begin{align*}
e^{\I y} &= \sum_{n = 0}^\infty \frac{(\I y)^n}{n!} = \sum_{k = 0}^\infty \frac{\I y^{2 k}}{(2 k)!} + \sum_{k = 0}^\infty \frac{(\I y)^{2k + 1}}{(2 k + 1)!} \\
&= \sum_{k = 0}^\infty \frac{(-1)^k y^{2k}}{(2 k)!} + \I \sum_{k = 0}^\infty \frac{(-1)^k y^{2k + 1}}{(2k + 1)!} \\
&= \cos{y} + \I \sin{y}
\end{align*}
$\implies$ Eulersche Formel
\[e^{\I \phi} = \cos{\phi} + \I \sin{\phi}\]
\subsubsection{Zusammenfassung:}
\label{sec-5-1-4}
\[e^{\I\phi} = \cos{\phi} + \I \sin{\phi}\]
\[w = e^z = e^x e^{\I y} = \abs{w}e^{i\arg{w}}\]
\[\ln{w} = z = x + \I y = \ln{\abs{w}} + \I \arg{w}\]
Problem: $\arg{w}$ und deshalb $\ln$ nicht eindeutig definiert \\
   Lösung: Definiere $\arg{w} \in (-\pi, \pi)$

\subsubsection{Fundamentalsatz der Algebra}
\label{sec-5-1-5}
In $\mathbb{C}$ hat jedes Polynom
\[P_n(z) = \sum_{i = 0}^n a_i z^i\]
eine Nullstelle $z_0$

In der Tat hat es sogar $n$ Nullstellen:
\[P_n(z) = (z - z_0)\cdot \underbrace{P_{n - 1}(z)}_{\mathclap{\text{Hat wieder eine Nullstelle, usw.}}}\]

(Man sagt: Körper $\mathbb{C}$ ist algebraisch abgeschlossen)
\begin{itemize}
\item Es gibt auf $\mathbb{C}$ wichtige Abbildung: "komplexe Konjugation"
\[z \to z^\ast \estimates z\to \bar z\]
Definiert durch:
\[(x + \I y)^\ast = x - \I y, (\rho e^{\I \phi})^\ast = \rho e^{-\I \phi}\]
also auch
\[(z^\ast)^\ast = z\]
\end{itemize}
\subsubsection{Quaternionen}
\label{sec-5-1-6}
\[1,i \to 1,i,j,k, i^2 = j^2 = k^2 = -1\]
\[ij = k, j i = -k, jk = i,\ldots\]
\subsection{Anwendung auf harmonischen Oszillator}
\label{sec-5-2}
\begin{align*}
\intertext{Erinnerung: physikalisches Problem:}
\ddot{x} + c \dot{x} + \omega^2 x = 0 \\
\intertext{Fall $\frac{c}{2} > \omega$ (Kriechfall)}
x=e^{\alpha t}, \alpha^2 + c\alpha + \omega^2 = 0 \\
\alpha_{1,2} =-\frac{c}{2} \pm \sqrt{\frac{c^2}{4} - \omega^2} \\
\intertext{$\implies$ 2 linear unabhängige Lösungen, also allgemeine Lösung durch lineare Superposition}
\intertext{$\implies$ exponentielles Abfallverhalten, ohne Oszillationen}
\intertext{Fall $\frac{c}{2} < \omega$ (Schwingfall), $\sqrt{-x} = \I \sqrt{x}$}
\alpha_{1,2} = -\frac{c}{2}\pm \I\sqrt{\omega^2 - \frac{c^2}{4}} \equiv -\frac{c}{2} \pm \I \tilde\omega \\
x_{1,2} = e^{-\frac{c}{2}t} e^{\pm \I \omega t} = e^{-\frac{c}{2}t} (\cos{\pm \tilde \omega t} + \I \sin{\pm\tilde\omega t}) \\
x_{1,2} =  e^{-\frac{c}{2}t} (\cos{\tilde \omega t} \pm \I \sin{\tilde\omega t}) \\
\intertext{Durch Linearkombination $\to$ 2 reelle Lösungen:}
x_1 = e^{-\frac{c}{2}t}\cos{\tilde \omega t};\qquad x_2 = e^{-\frac{c}{2}t} \sin{\tilde\omega t}
\intertext{$\implies$ allgemeine Lösung durch Linearkombination}
\intertext{$\implies$ gedämpfte Schwingung}
\intertext{Fall $\frac{c}{2} = \omega$ (aperiodischer Grenzfall)}
\alpha_1 = \alpha_2
\intertext{$\implies$ Nur eine linear unabhängige Lösung, brauche weitere Lösung um allgemeine Anfangsbedingungen zu erfüllen}
\intertext{Idee: Betrachte Schwingfall Lösungen für $\tilde \omega \to 0$}
\intertext{Taylor:}
\cos{x} = 1 + \O(x^2);\qquad \sin{x} = x + O(x^3) \\
\implies x_1 = e^{-\frac{c}{2}t}; \qquad x_2 = e^{-\frac{c}{2}t} \tilde\omega t
\intertext{$\implies$ Wieder asymptotische Annäherung an $0$ ohne Oszillation}
\end{align*}
\subsection{harmonischer Oszillator mit periodisch treibender Kraft}
\label{sec-5-3}
\begin{align*}
\intertext{Inhomogene Dgl:}
\ddot{x} + c \dot{x} + \omega^2 x = \frac{1}{m} F(t), F(t) = f e^{\I \ubar\omega t}  \\
\intertext{Ansatz:}
x(t) = A e^{\I \ubar \omega t} \\
\implies (A (-\ubar\omega^2 + \I c \ubar\omega + \omega^2) - \frac{f}{m}) e^{\I \omega t = 0} \\
A \equiv \abs{A}e^{\I \phi} = \frac{f}{m}\cdot\frac{1}{\omega^2 - \ubar\omega^2 + \I c \ubar \omega} \\
\intertext{mit}
\frac{1}{a + \I b} = \frac{a - \I b}{a - \I b} \frac{1}{a + \I b} = \frac{a - \I b}{a^2 - b^2} \\
\intertext{und elementarer Algebra findet man den Realteil der Lösung:}
\Re x(t) = \abs{A} \cos{\ubar \omega t + \phi} \\
\abs{A} = \frac{\frac{f}{m}}{\sqrt{\omega^2 - \ubar \omega^2} + c^2 \ubar\omega^2}, \tan{\phi} = \frac{c\ubar\omega}{\ubar\omega^2 - \omega^2}
\end{align*}
Allgemeine Lösung ergibt sich, indem man zu dieser partikulären Lösung der inhomogenen Gleichung die allgemeine Lösung der homogenen Gleichung addiert.

\textbf{Wichtig:} Langzeitverhalten ist durch die partikuläre Lösung bestimmt $\implies$ Resonanzkatastrophe bei $c \to 0 \& \ubar \omega \to \omega$
\section{Symmetrie der Raum zeit}
\label{sec-6}
\subsection{Matrix, Determinante, Inverse Matrix}
\label{sec-6-1}
\begin{defn}[Permutation]
Eine Permutation (Bez: $\sigma$) von $n$ Elementen ist eine umkehrbare Abbildung einer Menge von $n$ Elementen auf sich selbst: \\
   Menge: $\{1, \ldots, n\}$, Abb: $\sigma: \{1, \ldots, n\} \to \{1, \ldots, n\}, i \mapsto \sigma{i}$ \\
   oft nützlich: Man denke an die elementweise Anwendung von $\sigma$ auf $\{1, \ldots, n\}: \to \{\sigma(1),\ldots, \sigma(n)\}$ \\

Eine Permutation heißt \textbf{gerade} $(\sgn(\sigma) = 1)$, falls sie sich aus geradzahlig vielen Vertauschungen von Nachbarn ergibt.
Zum Beispiel ist $123\to 312$ das Produkt von $123\to 132$ und $123\to 213$:
\[123\to 132 \to 312\]
\end{defn}
\begin{defn}[Levi-Civita-Tensor]
\[\eps^{\sigma(1) \ldots \sigma(n)} \equiv \sgn(\sigma)\]
Insbesondere: $\eps^{12\ldots n} = 1$
\end{defn}

\begin{itemize}
\item Eine $(n\times m)$- Matrix ist ein Schema $A^{ij}$ von Zahlen, die jeweils Eintrag $i$ in Zeile $i$ und Spalte $j$ bezeichnen.
\item Man kann eine $(n\times m)$-Matrix mit einer $(m\times p)$-Matrix multiplizieren:
\[(AB)^{ij} = \sum_{k = 1}^{m}A^{ik} B^{kj}\]
das Ergebnis ist eine $(n\times p)$-Matrix
\end{itemize}

\begin{defn}[Determinante]
Für quadratische ($(n\times n)$-Matrizen) definieren wir die \textbf{Determinante}:
\[\det A = \frac{1}{n!} \underarrow[\eps]{n-dim. Levi-Civita-Symbol}^{i_1 \ldots i_n} A^{i_1 j_1} A^{i_2 j_2} \ldots A^{i_n j_n} \eps^{j_1 \ldots j_n} \]

Damit erhält man: \\
   Determinante einer $(1\times 1)$-Matrix: die Zahl selbst.

Erstes nicht triviales Beispiel: $(2\times 2)$-Matrix
\begin{align*}
\det A &= \det \begin{pmatrix} A^{11} & A^{12} \\ A^{21} & A^{22}\end{pmatrix} = \frac{1}{2!} \eps^{ij} A^{ik} A^{jl} \eps^{kl} \\
&= \frac{1}{2!}(\eps^{12}A^{11}A^{22}\eps^{12} + \eps^{12}A^{12}A^{21}\eps^{21} + \eps^{21}A^{21}A^{12}\eps^{12} + \eps^{21}A^{22}A^{11}\eps^{21}) \\
&= \frac{1}{2} (A^{11} A^{22} + A^{12} A^{21} - A^{21}A^{12} + A^{22}A^{11}) = A^{11}A^{22} - A^{12}A^{21} \\
\intertext{Man überlegt sich leicht:}
\det A = \sum_{\sigma} \sgn{\sigma}A^{1\sigma(1)} A^{2\sigma(2)} \ldots A^{n\sigma(n)} \\
\end{align*}
Also: $n!$ Summanden, Jeder ist Produkt von je einem Element aus jeder Zeile und Spalte der Matrix. Vorzeichen ist Vorzeichen der Permutation (siehe unten)
\end{defn}
\begin{ex}[$(3\times 3)$-Matrix]
Rechenschema:
\begin{align*}
A&=\begin{pmatrix} A^{11} & A^{12} & A^{13} & A^{11} & A^{12} \\ A^{21} & A^{22} & A^{23} & A^{21} & A^{22} \\ A^{31} & A^{32} & A^{33} & A^{31} & A^{32} \\ \end{pmatrix} \\
\det A &= A^{11}A^{22}A^{33} + A^{12}A^{23}A^{31} + \ldots
\end{align*}
\end{ex}
Betrachte nun den Ausdruck:
\begin{align*}
\eps^{i_1 i_2 \ldots i_n} A^{i_1 j_1} A^{i_2 j_2} \ldots A^{i_n j_n} = \eps^{i_1 i_2 \ldots} A^{i_2 j_2}A^{i_1 j_2} \ldots \\
= \eps^{i_2 i_1\ldots } A^{i_2 j_2} A^{i_2 j_1}  \ldots = - \eps^{i_1 i_2 \ldots i_n} A^{i_1 j_2} A^{i_2 j_1} \ldots A^{i_n j_n}
\intertext{Vorzeichenwechsel durch Vertauschen zweier Indizes, obiger Ausdruck ist "total antisymmetrisch"}
\end{align*}

Totale Antisymmetrie ist die definierende Eigenschaft von $\eps$. Sie bestimmt jeden Ausdruck mit $u$ Indizes bis auf Vorfaktor. Deshalb:
\begin{align*}
\eps^{i_1 \ldots i_n}A^{i_1 j_1} \ldots A^{i_n j_n} = c \eps^{j_1 \ldots j_n} \\
\intertext{Multipliziere mit $\eps^{j_1 \ldots j_n}$:}
n! \det A = c \eps^{j_1 \ldots j_n} \eps^{j_1 \ldots j_n} = c n! \\
\intertext{$\implies$ alternative Formel für $\det A$:}
e^{i_1 \ldots i_n} A^{i_1 j_1} \ldots A^{i_n j_n} = (\det A) \eps^{j_1 \ldots j_n}
\end{align*}
Zentraler Fakt: $A$ invertierbar $\iff \det A \neq 0$

Inverse Matrix:
\begin{align*}
(A^{-1})^{ij} &= \frac{1}{(n - 1)! \det A} \eps^{j i_2 \ldots i_n} \eps^{i j_2 \ldots j_n} A^{i_2 j_2} \ldots A^{i_n j_n} \\
\intertext{Prüfen:}
(A^{-1})^{ij} A^{jk} &= \frac{1}{(n - 1)! \det A} \eps^{j i_2 \ldots i_n} \eps^{i j_2 \ldots j_n} A^{j k} A^{i_2 j_2} \ldots A^{i_n j_n} \\
&= \frac{1}{(n - 1)! \det A} (\det A) \underbrace{\eps^{k j_2 \ldots j_n} \eps^{i j_2 \ldots j_n}}_{(n - 1)! \delta^{ik}} \\
&= \delta^{ik} \checkmark
\end{align*}

Kommentar:
\begin{align*}
\frac{1}{(n - 1)!} \eps^{i i_2 \ldots i_n} \eps^{j j_2 \ldots j_n} A^{i_2 j_2} \ldots A^{i_n j_n} \\
= (-1)^{i + j} \det(\underarrow[M]{Matrix der Cofaktoren}(i,j))
\intertext{Matrix der Cofaktoren ergibt sich aus $A$ Streichen von Zeile $i$ und Spalte $j$}
\end{align*}
\subsection{Der Euklidische Raum}
\label{sec-6-2}
physikalischer Raum: $V = \mathbb{R}^3$ mit Skalarprodukt $\v x, \v y \to \v x \cdot \v y = x^i y^i$ \\
   Unser Ziel:
Symmetrien, also Abbildungen $R: V \to V, \v x \mapsto \v x'$, welche die Struktur des Raumes respektieren.
Das heißt:
\begin{align*}
R(\alpha \v x + \beta \v x) &= \alpha R(\v x) + \beta R(\v y) \\
R(\v x, \v y) \underarrow[\equiv]{Sagt nur: Zahlen transformieren nicht} \v x  \v y = R(x)R(y)
\end{align*}
Zunächst nur Linearitätsbedingung (wird respektiert von allgemeinen linearen Transformationen)
\[x^i \mapsto x^{\prime i} = R^{i j} x^j\]
oder
\[\cvec{x^1 ; \vdots ; x^n} \mapsto \cvec{x^{\prime 1} ; \vdots ; x^{\prime n}} = \begin{pmatrix} R^{11} & \ldots & R^{1n} \\ \vdots & & \vdots \\ R^{n 1} & \ldots & R^{nn}\end{pmatrix}\]

Symmetrie: Lineare Transformation: $\v x \mapsto R(\v x)$ \\
   konkret: $x^i \mapsto x^{\prime i} = R^{ij}x^j$
\[\cvec{x^1 ; \vdots ; x^n} \mapsto \cvec{x^{\prime 1} ; \vdots ; x^{\prime n}} = \begin{pmatrix} R^{11} & \ldots & R^{1n} \\ \vdots & & \vdots \\ R^{n 1} & \ldots & R^{nn}\end{pmatrix} \cvec{x^1 ; \vdots ; x^n}\]
Kurzschreibweise:
\[x\mapsto x' = R x\]
\begin{itemize}
\item hier: Großbuchstaben = Matrizen
\item Kleinbuchstaben = Vektoren
\end{itemize}
\begin{ex}[n = 2]
\begin{align*}
x^{\prime 1} &= R^{11} x^1 + R^{12}x^2 \\
x^{\prime 2} &= R^{21} x^1 + R^{22}x^2 \\
\end{align*}

Ganz explizit
\[R = \begin{pmatrix} 2 & 0 \\ 1 & 1 \end{pmatrix} \implies \cvec{1 ; 0} \mapsto \cvec{2; 1}; \cvec{0 ; 1} \mapsto \cvec{0; 1}\]
Transformation der beiden Basisvektoren

Jeder andere Vektor ($\cvec{\alpha; \beta}$) ist schreibbar als $\alpha \cvec{1; 0} + \beta \cvec{0 ; 1}$. Er transformiert demnach gemäß
\[\cvec{\alpha; \beta} \mapsto \alpha \cvec{2 ; 1} + \beta \cvec{0 ; 1} = \cvec{2 \alpha ; \alpha + \beta}\]
\end{ex}
Wichtig: Symmetrietransformationen müssen verknüpfbar sein: \\
   Dazu: Betrachte zwei Transformationen:
\begin{align*}
R_1: x\mapsto R_1 x; R_2: x\mapsto R_2 x \\
\intertext{zusammen:}
R_1 \circ R_2:x\mapsto R_2 R_1 x \\
\end{align*}
Die Komponente $i$ des entstehenden Vektors ist:
\[(R_2 R_1 x)^i = R_2^{ij}(R_1 x)^j = R_2^{ij}R_1^{jk}x^k\]
Man kann die Abbildung $x\mapsto R_2 R_2 x$ auch "in einem Schritt" als Transformation durch die Produktmatrix $R_2 R_1$ realisieren:
\[(R_2 R_1 x)^i = \underbrace{(R_2^{ij}R_1^{jk})}_{\mathclap{\text{Produktmatrix}}}x^k\]
Die Produktmatrix ist $(R_2 R_1)^{ij} = R_2^{ij} R_1^{jk} \equiv R_3^{ik}$
Hinweis zum expliziten Rechnen:
\begin{align*}
&\begin{pmatrix} R_1^{11} & R_1^{12} & R_1^{13} \\ R_1^{21} & R_1^{22} & R_1^{23} \\ R_1^{31} & R_1^{32} & R_1^{33}\end{pmatrix} \\
\begin{pmatrix} R_2^{11} & R_2^{12} & R_2^{13} \\ R_2^{21} & R_2^{22} & R_2^{23} \\ R_2^{31} & R_2^{32} & R_2^{33}\end{pmatrix} &\begin{pmatrix} R_3^{11} & R_3^{12} & R_3^{13} \\ R_3^{21} & R_3^{22} & R_3^{23} \\ R_3^{31} & R_3^{32} & R_3^{33}\end{pmatrix} \\
\end{align*}
Für den Begriff der Symmetrie brauchen wir Invertierbarkeit. Wir nennen eine Transformation beziehungsweise die entsprechende Matrix $R$ invertierbar, falls es eine zweite Matrix $R^{-1}$ gibt, so dass
\begin{align*}
R^{-1} \circ R &= id \tag{Identitätsabbildung} \\
(R^{-1})^{ij} R^{jk} &= \mathbb{1}^{ik} \equiv \delta^{ik}
\end{align*}

Wäre Linearität die einzige wichtige Eigenschaft: dann wären die Symmetriefunktionen alle
\[R \in \underarrow[GL]{Menge aller invertierbaren $n\times m$ Matrizen}(n)\]
Wir brauchen zusätzlich:
\[\v x \v y = R(\v x) R(\v y), R(x)^i = R^{ij} \times j\]
Dazu wichtige Schreibweise
\[(M^{T})^{ij} = M^{ji} \tag{T für transponiert}\]
auch: $x = \cvec{x^i ; \ldots ; x^n}, x^{\v i} = (x^1 \ldots x^n)$
Es gilt: $\v x \v y = x^{T} y = x^i y^i$ \\
   es gilt weiterhin:
\[R(\v x) R(\v y) = (Rx)^{T}(Ry) = (x^{T} T^{T})(Ry) = x^{T}R^t Ry\]

\begin{align*}
\intertext{Nebenrechnung}
((AB)^{T})^{ij} = (AB)^{ji} = A^{jk} B^{jk} = B^{ki} B^{j k} = (B^{T})^{ik} (A^{tilde})^{kj} \\
= (B^{T} A^{T})^{ij} \\
\implies (AB)^{T} = B^T A^T \\
\end{align*}

Ziel: $x^T R^T Ry = x T_y$ soll gelten für beliebige x,y. Dies gilt genau dann wenn $R^{T} R = \mathbb{1}$
\[(R^{T})^{ik} R^{kj} = \delta^{ii}\]
\[R^{ki} R^{kj} = \delta^{ii}\]
\[R^{ik} R^{jk} = \delta^{ii}\]
wenn $AB = \mathbb{1}$, so auch $B A = \mathbb{1}$


Symmetrien des euklidischen Raums:
$x \t R x$ mit $R^{T} T = \mathbb{1}$
$R \in O(3) \subset ULU3$
\subsection{Symmetriegruppe (M)}
\label{sec-6-3}
Symmetrien in Physik und Mathe $\rightarrow$ Gruppen \\
   Bisher:
\begin{itemize}
\item Matrixgruppen
\begin{itemize}
\item $GL(n)$ - Symmetriegruppe des Vektorraums $\mathbb{R}^n$
\item $O(n)$ - Symmetriegruppe des euklidischen Raumes
\end{itemize}
\end{itemize}

Allgemeiner: Eine Gruppe ist eine Menge $G$ mit einer Binären Operation $G \times G \to G$ für die gilt:
\begin{itemize}
\item $(a \cdot b) \cdot c = a \cdot (b\cdot c)$
\item $\Exists e \in G: a\cdot e = e\cdot a = a \Forall a$ ("Eins")
\item $\Forall a\in G \Exists a^{-1} \in G: a\cdot a^{-1} = a^{-1} \cdot a = e$
\end{itemize}

Eine Gruppe heißt "abelsch" falls $a\cdot b = b\cdot a \Forall a,b$
Beispiele dafür:
\begin{itemize}
\item $\mathbb{Q}\setminus \{0\}, \mathbb{R} \setminus \{0\}, \mathbb{C} \setminus \{0\}$
\end{itemize}
Falls sie statt "$\cdot$" die Operation "$+$" zur Gruppenoperation erklären, dann sind
\begin{itemize}
\item $\mathbb{Q}, \mathbb{R}, \mathbb{C}, \mathbb{Z}$
\end{itemize}
Gruppen mit $"+"$

\begin{defn}[Körper]
$K$ mit Operationen $+,\cdot$ ist ein Körper falls:
\begin{itemize}
\item $(K,+)$ ist abelsche Gruppe (Eins = 0)
\item $(K\setminus\{0\}, \cdot)$ ist auch abelsche Gruppe
\item Distributivität
\end{itemize}
\end{defn}

$GL(n)$ ist eine (nicht abelsche) Gruppe. Müssen prüfen: $A,B$ invertierbar $\implies A\cdot B$ invertierbar. Wir geben das Inverse zu $A\cdot B$ einfach an:
\[(B^{-1}A^{-1})(AB) = B^{-1}(A A^{-1}) B = B^{-1} B = \mathbb{1}\]

$GL^+ (n)$ - orientierungserhaltende Untergruppe $\equiv$ alle $A$ in  $GL(n)$ mit $\det A > 0$ \\
   $O(n)$ ist Untergruppe von $GL(n)$. Müssen prüfen dass $A,B$ orthogonal $\implies A\cdot B$ orthogonal. Dazu:
\[(A\cdot B)^{T} (A\cdot B) = B^{T} A^{T} A B = B^{T} B = \mathbb{1}\]

Wichtige Untergruppe: Spezielle Orthogonale Transformation $SO(n)$ \\
   Diese Transformationen erfüllen: $\det(R) = 1$

Dazu zwei Fakten: $\det A^{T} = \det A, \det(AB) = (\det(A))(\det B)$
Damit folgt aus $R^{T} R =\mathbb{1}$
\[\det(R^{T} R) = \det(R^{T})(\det R) = (\det R)^2 = \det {\mathbb{R}} = 1, \det R = \pm 1\]
$\equiv$ Matrizen in $O(n)$ mit $\det = 1$

Speziell in $n = 3$ (3d-Raum) wird die Reflexion bezüglich y,z Ebene beschrieben durch:
\begin{align*}
R_x = \begin{pmatrix} -1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}, \det R_x = -1
\end{align*}
Fakt: Jedes Element von $O(3)$ ist schreibbar als $R$ oder $R \cdot R_x$ mit $R \in SO(3)$, $SO(3)$ sind "echte" Drehungen.

Überlegen Sie sich, dass $R\in SO(2)$ allgemein schreibbar ist als
\[R = \begin{pmatrix} \cos{\phi} & -\sin{\phi} \\ \sin{\phi} & \cos{\phi}\end{pmatrix}\]
Identifizieren sie $SO(2)$ mit folgender Menge
\[\{z \in \mathbb{C} \mid \abs{z} = 1\}\]
Die Gruppenoperation soll der komplexen Multiplikation entsprechen
\subsection{Tensoren}
\label{sec-6-4}
Ein Tensor von Rang (oder Stufe) $m$ im n-dimensionalen Vektorraum $V = \mathbb{R}^n$ ist eine multilineare Abbildung:
\[t:\underbrace{V\times V\times \ldots \times V}_{m\text{-mal}} \to \mathbb{R}\]
Praktisch:
\[t: (\v x_{(1)}, \v x_{(2)}, \ldots, \v x_{(n)}) \mapsto t_{i_1 \ldots i_m} x_{(1)}^{i_1} \ldots x_{(m)}^{i_m} \]
\begin{ex}
\begin{itemize}
\item Euklidisches Skalarprodukt: $V\times V \to \mathbb{R}$
     \[\delta: (\v x, \v y) \mapsto \delta_{ij} x^i y^j \equiv \v x \cdot \v y \in \mathbb{R}\]
\item Noch einfacher:
\[t: V \to \mathbb{R}: t_i \v x \to t_i x^i \in \mathbb{R}\]
Die Menge solcher linearen Abbildungen bildet auch einen n-dimensionalen Vektorraum, den sogenannten Dualraum $V^\ast$ (zu $V$)
Notation: $\ubar t = \{t_1, \ldots, t_n\} \in V^\ast$ \\
     Erinnerung: $\v x = \{x^1, \ldots, x^n\} \in V$ \\
     Oben(Unten schreiben der Indizes macht die "natürliche Wirkung") von $\ubar t$ auf $\v x$ besonders deutlich: $t_i x^i \in \mathbb{R}$
\begin{itemize}
\item oben: kontravariant
\item unten: kovariant ($\rightarrow$ Co-Vektor $\in V^\ast$)
\end{itemize}
Für und: enorme Vereinfachung: \\
     Wir haben immer euklidischen Raum und damit die besondere Rolle von $\delta_{ij}$ und die inversen Matrix $\delta^{ij}$
\[\delta_{ij} \delta^{jk} = \delta_i^k = (\mathbb{1}_i^k)\]
Dies erlaubt uns Indizes beliebig zu "heben" und zu "senken":
\[t^i \equiv \delta^{ij} t_j, x_i \equiv \delta_{ij}x^j\]
Damit können wir $V$ und $V^\ast$ identifizieren. Wir können auch alle Tensor Indizes beliebig oben oder unten schreiben.
Wir werden zur Vereinfachung weiterhin schreiben
\[\v x \v y = x^i y^i (~\text{eigentlich}~x^i y^j \delta_{ij})\]
(Mehr zum Dualraum in Lineare Algebra)
\end{itemize}
\end{ex}
Für uns: Tensor der Stufe 1: ist Vektor
\[t: \v x \mapsto t^i x^i = \v t \v x \in \mathbb{R}\]
Wichtig für uns: Resultat von Anwendung eines Rang-1-Tensors auf Vektor ist invariant unter Drehungen:
\[x\mapsto Rx; t\mapsto Rt\]
\[x\mapsto x' = Rx, x^{ij} = R^{ij} x^j\]
Invarianz:
\[t^{T} x = t^{T} R^{T} R x\]
Betrachte einfaches, allgemeines Beispiel für Tensor der Stufe 2:
\begin{align*}
t\equiv U\otimes W \in V\otimes V \\
t:(\v x, \v y) \mapsto (u^i w^j) \cdot (x^i y^j) &= (\v u \cdot \v x) \cdot (\v w \cdot \v y) \\
\intertext{mit $t^{ij} \equiv u^i w^j$}
&= t^{ij} x^i y^j \\
\end{align*}
Grob gesagt: $V \otimes V$ ist die Menge aller Linearkombinationen von Elementen wie $U\otimes W$ \\
   Transformation von $t^{ij} = u^i w^j$ unter Drehungen:
\[t^{ij} = u^i w^j \xrightarrow{R} R^{ik} u^k R^{jl} w^l = R^{ik} R^{jl} t^{kl}\]
Invarianz von $t(\v x, \v y):$
\begin{align*}
t(\v x, \v y) \to (R t) (R x, R y) &= (R^{ik} r^{jl} t^{kl}) (R^{ip} x^p) (R^{jq} y^q) \\
&= (R^{ik} R^{ip}) (R^{jl} R^{jq}) t^{kl} x^p y^q \\
&= \delta^{kp} \delta^{lq} t^{kl} x^p y^q \\
&= t^{kl} x^{k} y^{l} \\
&= t(\v x, \v y)
\end{align*}
Allgemeine Transformation eines Tensors unter Drehungen:
\[t \to t' = Rt, t^{\prime i_1 \ldots i_m} = R^{i_1 j_1} \ldots R^{i_m j_m} t^{j_1 \ldots j_m}\]
Invarianz von $t(\v u_{(1)}, \ldots, \v u_{(m)})$ folgt wie oben.

Fortgeschrittener Kommentar: Gruppe wirkt auf Vektoren aus $K \equiv$ Darstellung

Für unser Beispiel der Wirkung von $O(n)$ auf $\mathbb{R}^k$ war das "offensichtlich" mit Tensoren haben wir "nicht triviales Beispiel für Darstellung"
\[\underbrace{R\in O(n)}_{\text{Elemente $R^{ij}$}} \overset{\text{Darst.}}{\mapsto} D(R) \in \underbrace{n^2 \times n^2 \text{-Matrizen}}_{\text{Elemente}~D(R)^{ij,kl} = R^{ik}R^{jl}}\]
Dieses $D(R)$ wirkt wie oben beschrieben auf Tensoren:
\[t^{ij} \overset{D(R)}{\mapsto} D(R)^{ij,kl} t^{kl}\]
$D(R)$ ist eine Darstellung von $O(n)$, die verschieden ist von der "definierenden" Darstellung

\textbf{Transformation von $\delta^{ij}$} \\
   \[\delta^{\prime ij} = R^{ik} R^{jl} \delta^{kl} = R^{ik} R^{jk} = \delta^{ij}\]
$\implies \delta^{ij}$ ist ein \textbf{invarianter Tensor} \\

weiteres Beispiel: (für $m = n$: Levi Civita-Tensor) \\
   Wir schreiben nur $m = n = 3$ Fall aus:
\[\eps(\v x, \v y, \v z) = \eps^{ijk} x^i y^j z^k = x^i \eps^{ijk} y^j z^k = \v x(\v y \times \v z)\]
\textbf{Transformation:}
\[\eps^{\prime i_1 i_2 i_3} = R^{i_1 j_1} R^{i_2 j_2} R^{i_3 j_3} \eps^{j_1 j_2 j_3} = \eps^{i_1 i_2 i_3} \det(R) \underarrow[=]{$R\in SO(3)$} \eps^{i_1 i_2 i_3}\]

\textbf{Fakt:} Falls $t_1, t_2$ Tensoren vom Rang $m_1, m_2$ sind, so ist das folgende ein Tensor vom Rang $m_1 + m_2 - 2l_i$:
\[t_1^{i_1 \ldots i_l i_{l + 1} \ldots i_m} t_2^{i_1 \ldots i_l j_{l + 1} \ldots j_{m_2}} = t^{i_{l + 1} \ldots i_{m_1} j_{l + 1} \ldots j_{m_2}}\]

\textbf{Anwendungen:}
$\v a \times \v v$ ist ein Pseudovektor:
\[(\v a' \times \v b')^i \equiv \eps^{ijk} a^{\prime j} b^{\prime k} = \underarrow[\pm]{falls Spiegelung} \eps^{\prime ijk} a^{\prime j} b^{\prime k} = \pm R^{il} \eps^{ljk} a^j b^k = \pm R^{il} (\v a\times \v b)^l \]
\subsection{Galilei-Transformationen}
\label{sec-6-5}
\textbf{Bisher:} $\mathbb{R}^3$ mit Symmetriegruppe $O(3)$ \\
   \textbf{Jetzt:} Physikalische Raum Zeit: Zusätzlich: $t\in \mathbb{R}$ \\
   Punkt $\v x \in \mathbb{R}^3 \xrightarrow{neu}$ Ereignisse $(t,\v x) \in \mathbb{R} \times \mathbb{R}^3$ \\
   Müssen abschaffen: $\v 0$ im Vektorraum. In der Tat: $\abs{\v x}, \abs{\v y}$ sind unphysikalisch, physikalisch ist nur $\abs{\v x - \v y}$, ebenso ist nur $t_1 - t_2$ physikalisch

$\implies$ Symmetrietransformationen:
\begin{enumerate}
\item Rotationen: $(t,x) \mapsto (t,Rx), R \in O(3)$
\item Translationen: $(t,x) \mapsto (t + s, x + y), s \in \mathbb{R}, y\in\mathbb{R}^3 \implies$ Abschaffung der $0\in\mathbb{R}$ und $\v 0 \in \mathbb{R}^3$"
\item Boosts: $(t, x) \mapsto (t, x + vt),v\in\mathbb{R}^3$ "zeitabhängige Verschiebung"
\end{enumerate}

Die Galilei-Gruppe $G$ ist die von 1., 2. und 3. "generierte" Gruppe. Nicht trivialer Fakt: Jedes $g\in G$ ist schreibbar als $g = \overarrow[g_3]{Boost} \circ \underarrow[g_2]{Trans} \circ \overarrow[g_1]{Rot.}$
Man muss dazu unter anderem zeigen, dass es zu einem $g_2 \circ g_1 \circ g_2'' \in G$ ein $g_2'', g_1''$ gibt, sodass $g_2 \circ g_1 \circ g_2' = g_2'' \circ g_1''$ \\
   "Boost" = Zunahme (der Geschwindigkeit). Boost einer Trajektorie: $(t, \v x(t)) \mapsto (t, \v x(t) + \v v_0 t)$ \\
   $\v v = \dot{\v x}(t) \mapsto \v v' = \dot{\v x}(t) + \v v_0$

Boost zerstören das Konzept der Gleichörtlichkeit: Seien $(t, x), (t', x)$ zwei Ereignisse am gleichen Ort. Boost $\implies (t, x + v t), (t', x + v t')$, \textbf{nicht} mehr am gleichen Ort
\subsection{Affiner Raum}
\label{sec-6-6}
\begin{defn}
$O(3)$ Symmetriegruppe des euklidischen Raumes. "Elegant!". \\
   Besser: Definition des \textbf{affinen Raumes}: Gegeben sein Menge $A$, ein Vektorraum $V$ und eine Abbildung $A\times A \to V, (P, Q) \mapsto \v{PQ}$
sodass $\v{PQ} + \v{QR} = \v{PR}$. Außerdem: Zu jeden $P\in A, \v V$ soll es eindeutig ein $Q\in A$ geben, sodass $\v = \v{PQ}$, das Paar $(A,V)$ heißt affiner Raum
\end{defn}
\begin{ex}
Zu jedem Vektorraum gehört ein affiner Raum: Wähle $A \equiv V, V\times V \to V, (\v x, \v y) \mapsto \v y - \v x$
\end{ex}
Sei $(A^4, V^4)$ ein 4-dimensionaler affiner Raum. (Man denke zum Beispiel an den zu $\mathbb{R} \times \mathbb{R}^3$ gehörigen affinen Raum) \\
   Physikalische Raumzeit: $(A^{(4)}, V^{(4)})$ mit
\begin{enumerate}
\item Eine lineare Abbildung $V^4 \to \mathbb{R}$ ("Zeitfunktion") (im konkreten Beispiel: $((t,x), (t', x')) \mapsto t' - t$)
\item Sei $\tilde V^{(3)} \subset V^{(4)}$ der Raum von Pfeilen zwischen gleichzeitigen Ereignissen ($v\in \tilde V^{(3)}$ heißt $T(\tilde v) = 0$)
Dann hat $\tilde V$ ein Skalarprodukt, "Abstandsfunktion". Im konkreten Beispiel: $(t,x),(t,x') \mapsto \abs{x - x'}$
\end{enumerate}

Zusammen bilden 1. und 2. eine Galileische Struktur. Die physikalische Raumzeit ist $(A^{(4)}, V^{(4)})$ mit galileischer Struktur

$G$ sind die Transformationen des $(A^{(4)}, V^{(4)})$, welche seine Galileische Struktur respektieren.
\subsection{Dynamik}
\label{sec-6-7}
Dynamik soll invariant sein! Betrachte Trajektorie, die die Bewegungsgleichung erfüllt:
\[(t, \v x(t)), m \frac{\d^2 \v x}{\d t^2} = \v F(t, \v x(t))\]
Transformierte Trajektorie:
\[t', \v x'(t') = (t + s, R\v x(t) + \v y + \v v(t + x))\]
Dazu:
\[m \frac{\d^2}{\d t'^2} = m \frac{\d^2 x}{\d t^2} = m \\frac{\d^2}{\d t^2} (R\v x(t) + \v y + \v v(t + s)) = m R \frac{\d^2 \v x}{\d t^2} = R\v F(t, \v x(t))\]
$\implies$ Newtonsche Dynamik ist invariant falls Kräfte wie Vektoren transformieren. (hatten wir schon verlangt)
Bei Systemen von Massepunkten mit Zentralkräften ist die Kraft gleich dem Gradient, sie besitzt automatisch Vektor-Transformationseigenschaften

Wir fordern bei
\[m R \frac{\d^2 \v x}{\d t^2} = R\v F(t,\v x(t))\]
eigentlich
\[R\v F(t,\v x(t)) = \v F'(t', x'(t'))\]
Wichtig: Die Transformation von $\v F$ beinhaltet nicht nur Drehung, sondern auch Transformation über das Argument.
Betrachte zur Vereinfachung $R = \mathbb{1} \implies \v F'(t', \v x') = \v F(t, \v x)$ \\

Geschwindigkeitsabhängige Kräfte: zum Beispiel Reibung
\[\v F_R = -\alpha(\v x - \underarrow[v u]{Medium})\]
\subsection{Zusammenfassung:}
\label{sec-6-8}
Allgemeingültiges Schema:
\begin{itemize}
\item Beschreibung der Bewegung festlegen (Spielfeld) (hier: affiner Raum und Galileische Struktur)
\item Identifikation der Symmetriegruppe (hier Galilei Gruppe)
\item Invarianz der Dynamik prüfen beziehungsweise fordern (Spielregeln) (Newtonsches Grundgesetz)
\end{itemize}
% Emacs 25.1.1 (Org mode 8.2.10)
\end{document}