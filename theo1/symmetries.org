* Symmetrie der Raum zeit
** Matrix, Determinante, Inverse Matrix
   #+ATTR_LATEX: :options [Permutation]
   #+begin_defn latex
   Eine Permutation (Bez: $\sigma$) von $n$ Elementen ist eine umkehrbare Abbildung einer Menge von $n$ Elementen auf sich selbst: \\
   Menge: $\{1, \ldots, n\}$, Abb: $\sigma: \{1, \ldots, n\} \to \{1, \ldots, n\}, i \mapsto \sigma{i}$ \\
   oft nützlich: Man denke an die elementweise Anwendung von $\sigma$ auf $\{1, \ldots, n\}: \to \{\sigma(1),\ldots, \sigma(n)\}$ \\

   Eine Permutation heißt *gerade* $(\sgn(\sigma) = 1)$, falls sie sich aus geradzahlig vielen Vertauschungen von Nachbarn ergibt.
   Zum Beispiel ist $123\to 312$ das Produkt von $123\to 132$ und $123\to 213$:
   \[123\to 132 \to 312\]
   #+end_defn
   #+ATTR_LATEX: :options [Levi-Civita-Tensor]
   #+begin_defn latex
   \[\eps^{\sigma(1) \ldots \sigma(n)} \equiv \sgn(\sigma)\]
   Insbesondere: $\eps^{12\ldots n} = 1$
   #+end_defn

   - Eine $(n\times m)$- Matrix ist ein Schema $A^{ij}$ von Zahlen, die jeweils Eintrag $i$ in Zeile $i$ und Spalte $j$ bezeichnen.
   - Man kann eine $(n\times m)$-Matrix mit einer $(m\times p)$-Matrix multiplizieren:
	 \[(AB)^{ij} = \sum_{k = 1}^{m}A^{ik} B^{kj}\]
	 das Ergebnis ist eine $(n\times p)$-Matrix

   #+ATTR_LATEX: :options [Determinante]
   #+begin_defn latex
   Für quadratische ($(n\times n)$-Matrizen) definieren wir die *Determinante*:
   \[\det A = \frac{1}{n!} \underarrow[\eps]{n-dim. Levi-Civita-Symbol}^{i_1 \ldots i_n} A^{i_1 j_1} A^{i_2 j_2} \ldots A^{i_n j_n} \eps^{j_1 \ldots j_n} \]

   Damit erhält man: \\
   Determinante einer $(1\times 1)$-Matrix: die Zahl selbst.

   Erstes nicht triviales Beispiel: $(2\times 2)$-Matrix
   \begin{align*}
   \det A &= \det \begin{pmatrix} A^{11} & A^{12} \\ A^{21} & A^{22}\end{pmatrix} = \frac{1}{2!} \eps^{ij} A^{ik} A^{jl} \eps^{kl} \\
   &= \frac{1}{2!}(\eps^{12}A^{11}A^{22}\eps^{12} + \eps^{12}A^{12}A^{21}\eps^{21} + \eps^{21}A^{21}A^{12}\eps^{12} + \eps^{21}A^{22}A^{11}\eps^{21}) \\
   &= \frac{1}{2} (A^{11} A^{22} + A^{12} A^{21} - A^{21}A^{12} + A^{22}A^{11}) = A^{11}A^{22} - A^{12}A^{21} \\
   \intertext{Man überlegt sich leicht:}
   \det A = \sum_{\sigma} \sgn{\sigma}A^{1\sigma(1)} A^{2\sigma(2)} \ldots A^{n\sigma(n)} \\
   \end{align*}
   Also: $n!$ Summanden, Jeder ist Produkt von je einem Element aus jeder Zeile und Spalte der Matrix. Vorzeichen ist Vorzeichen der Permutation (siehe unten)
   #+end_defn
   #+ATTR_LATEX: :options [$(3\times 3)$-Matrix]
   #+begin_ex latex
   Rechenschema:
   \begin{align*}
   A&=\begin{pmatrix} A^{11} & A^{12} & A^{13} & A^{11} & A^{12} \\ A^{21} & A^{22} & A^{23} & A^{21} & A^{22} \\ A^{31} & A^{32} & A^{33} & A^{31} & A^{32} \\ \end{pmatrix} \\
   \det A &= A^{11}A^{22}A^{33} + A^{12}A^{23}A^{31} + \ldots
   \end{align*}
   #+end_ex
   Betrachte nun den Ausdruck:
   \begin{align*}
   \eps^{i_1 i_2 \ldots i_n} A^{i_1 j_1} A^{i_2 j_2} \ldots A^{i_n j_n} = \eps^{i_1 i_2 \ldots} A^{i_2 j_2}A^{i_1 j_2} \ldots \\
   = \eps^{i_2 i_1\ldots } A^{i_2 j_2} A^{i_2 j_1}  \ldots = - \eps^{i_1 i_2 \ldots i_n} A^{i_1 j_2} A^{i_2 j_1} \ldots A^{i_n j_n}
   \intertext{Vorzeichenwechsel durch Vertauschen zweier Indizes, obiger Ausdruck ist "total antisymmetrisch"}
   \end{align*}

   Totale Antisymmetrie ist die definierende Eigenschaft von $\eps$. Sie bestimmt jeden Ausdruck mit $u$ Indizes bis auf Vorfaktor. Deshalb:
   \begin{align*}
   \eps^{i_1 \ldots i_n}A^{i_1 j_1} \ldots A^{i_n j_n} = c \eps^{j_1 \ldots j_n} \\
   \intertext{Multipliziere mit $\eps^{j_1 \ldots j_n}$:}
   n! \det A = c \eps^{j_1 \ldots j_n} \eps^{j_1 \ldots j_n} = c n! \\
   \intertext{$\implies$ alternative Formel für $\det A$:}
   e^{i_1 \ldots i_n} A^{i_1 j_1} \ldots A^{i_n j_n} = (\det A) \eps^{j_1 \ldots j_n}
   \end{align*}
   Zentraler Fakt: $A$ invertierbar $\iff \det A \neq 0$

   Inverse Matrix:
   \begin{align*}
   (A^{-1})^{ij} &= \frac{1}{(n - 1)! \det A} \eps^{j i_2 \ldots i_n} \eps^{i j_2 \ldots j_n} A^{i_2 j_2} \ldots A^{i_n j_n} \\
   \intertext{Prüfen:}
   (A^{-1})^{ij} A^{jk} &= \frac{1}{(n - 1)! \det A} \eps^{j i_2 \ldots i_n} \eps^{i j_2 \ldots j_n} A^{j k} A^{i_2 j_2} \ldots A^{i_n j_n} \\
   &= \frac{1}{(n - 1)! \det A} (\det A) \underbrace{\eps^{k j_2 \ldots j_n} \eps^{i j_2 \ldots j_n}}_{(n - 1)! \delta^{ik}} \\
   &= \delta^{ik} \checkmark
   \end{align*}

   Kommentar:
   \begin{align*}
   \frac{1}{(n - 1)!} \eps^{i i_2 \ldots i_n} \eps^{j j_2 \ldots j_n} A^{i_2 j_2} \ldots A^{i_n j_n} \\
   = (-1)^{i + j} \det(\underarrow[M]{Matrix der Cofaktoren}(i,j))
   \intertext{Matrix der Cofaktoren ergibt sich aus $A$ Streichen von Zeile $i$ und Spalte $j$}
   \end{align*}
** Der Euklidische Raum
   physikalischer Raum: $V = \mathbb{R}^3$ mit Skalarprodukt $\v x, \v y \to \v x \cdot \v y = x^i y^i$ \\
   Unser Ziel:
   Symmetrien, also Abbildungen $R: V \to V, \v x \mapsto \v x'$, welche die Struktur des Raumes respektieren.
   Das heißt:
   \begin{align*}
   R(\alpha \v x + \beta \v x) &= \alpha R(\v x) + \beta R(\v y) \\
   R(\v x, \v y) \underarrow[\equiv]{Sagt nur: Zahlen transformieren nicht} \v x  \v y = R(x)R(y)
   \end{align*}
   Zunächst nur Linearitätsbedingung (wird respektiert von allgemeinen linearen Transformationen)
   \[x^i \mapsto x^{\prime i} = R^{i j} x^j\]
   oder
   \[\cvec{x^1 ; \vdots ; x^n} \mapsto \cvec{x^{\prime 1} ; \vdots ; x^{\prime n}} = \begin{pmatrix} R^{11} & \ldots & R^{1n} \\ \vdots & & \vdots \\ R^{n 1} & \ldots & R^{nn}\end{pmatrix}\]

   Symmetrie: Lineare Transformation: $\v x \mapsto R(\v x)$ \\
   konkret: $x^i \mapsto x^{\prime i} = R^{ij}x^j$
   \[\cvec{x^1 ; \vdots ; x^n} \mapsto \cvec{x^{\prime 1} ; \vdots ; x^{\prime n}} = \begin{pmatrix} R^{11} & \ldots & R^{1n} \\ \vdots & & \vdots \\ R^{n 1} & \ldots & R^{nn}\end{pmatrix} \cvec{x^1 ; \vdots ; x^n}\]
   Kurzschreibweise:
   \[x\mapsto x' = R x\]
   - hier: Großbuchstaben = Matrizen
   - Kleinbuchstaben = Vektoren
   #+ATTR_LATEX: :options [n = 2]
   #+begin_ex latex
   \begin{align*}
   x^{\prime 1} &= R^{11} x^1 + R^{12}x^2 \\
   x^{\prime 2} &= R^{21} x^1 + R^{22}x^2 \\
   \end{align*}

   Ganz explizit
   \[R = \begin{pmatrix} 2 & 0 \\ 1 & 1 \end{pmatrix} \implies \cvec{1 ; 0} \mapsto \cvec{2; 1}; \cvec{0 ; 1} \mapsto \cvec{0; 1}\]
   Transformation der beiden Basisvektoren

   Jeder andere Vektor ($\cvec{\alpha; \beta}$) ist schreibbar als $\alpha \cvec{1; 0} + \beta \cvec{0 ; 1}$. Er transformiert demnach gemäß
   \[\cvec{\alpha; \beta} \mapsto \alpha \cvec{2 ; 1} + \beta \cvec{0 ; 1} = \cvec{2 \alpha ; \alpha + \beta}\]
   #+end_ex
   Wichtig: Symmetrietransformationen müssen verknüpfbar sein: \\
   Dazu: Betrachte zwei Transformationen:
   \begin{align*}
   R_1: x\mapsto R_1 x; R_2: x\mapsto R_2 x \\
   \intertext{zusammen:}
   R_1 \circ R_2:x\mapsto R_2 R_1 x \\
   \end{align*}
   Die Komponente $i$ des entstehenden Vektors ist:
   \[(R_2 R_1 x)^i = R_2^{ij}(R_1 x)^j = R_2^{ij}R_1^{jk}x^k\]
   Man kann die Abbildung $x\mapsto R_2 R_2 x$ auch "in einem Schritt" als Transformation durch die Produktmatrix $R_2 R_1$ realisieren:
   \[(R_2 R_1 x)^i = \underbrace{(R_2^{ij}R_1^{jk})}_{\mathclap{\text{Produktmatrix}}}x^k\]
   Die Produktmatrix ist $(R_2 R_1)^{ij} = R_2^{ij} R_1^{jk} \equiv R_3^{ik}$
   Hinweis zum expliziten Rechnen:
   \begin{align*}
   &\begin{pmatrix} R_1^{11} & R_1^{12} & R_1^{13} \\ R_1^{21} & R_1^{22} & R_1^{23} \\ R_1^{31} & R_1^{32} & R_1^{33}\end{pmatrix} \\
   \begin{pmatrix} R_2^{11} & R_2^{12} & R_2^{13} \\ R_2^{21} & R_2^{22} & R_2^{23} \\ R_2^{31} & R_2^{32} & R_2^{33}\end{pmatrix} &\begin{pmatrix} R_3^{11} & R_3^{12} & R_3^{13} \\ R_3^{21} & R_3^{22} & R_3^{23} \\ R_3^{31} & R_3^{32} & R_3^{33}\end{pmatrix} \\
   \end{align*}
   Für den Begriff der Symmetrie brauchen wir Invertierbarkeit. Wir nennen eine Transformation beziehungsweise die entsprechende Matrix $R$ invertierbar, falls es eine zweite Matrix $R^{-1}$ gibt, so dass
   \begin{align*}
   R^{-1} \circ R &= id \tag{Identitätsabbildung} \\
   (R^{-1})^{ij} R^{jk} &= \mathbb{1}^{ik} \equiv \delta^{ik}
   \end{align*}

   Wäre Linearität die einzige wichtige Eigenschaft: dann wären die Symmetriefunktionen alle
   \[R \in \underarrow[GL]{Menge aller invertierbaren $n\times m$ Matrizen}(n)\]
   Wir brauchen zusätzlich:
   \[\v x \v y = R(\v x) R(\v y), R(x)^i = R^{ij} \times j\]
   Dazu wichtige Schreibweise
   \[(M^{T})^{ij} = M^{ji} \tag{T für transponiert}\]
   auch: $x = \cvec{x^i ; \ldots ; x^n}, x^{\v i} = (x^1 \ldots x^n)$
   Es gilt: $\v x \v y = x^{T} y = x^i y^i$ \\
   es gilt weiterhin:
   \[R(\v x) R(\v y) = (Rx)^{T}(Ry) = (x^{T} T^{T})(Ry) = x^{T}R^t Ry\]

   \begin{align*}
   \intertext{Nebenrechnung}
   ((AB)^{T})^{ij} = (AB)^{ji} = A^{jk} B^{jk} = B^{ki} B^{j k} = (B^{T})^{ik} (A^{tilde})^{kj} \\
   = (B^{T} A^{T})^{ij} \\
   \implies (AB)^{T} = B^T A^T \\
   \end{align*}

   Ziel: $x^T R^T Ry = x T_y$ soll gelten für beliebige x,y. Dies gilt genau dann wenn $R^{T} R = \mathbb{1}$
   \[(R^{T})^{ik} R^{kj} = \delta^{ii}\]
   \[R^{ki} R^{kj} = \delta^{ii}\]
   \[R^{ik} R^{jk} = \delta^{ii}\]
   wenn $AB = \mathbb{1}$, so auch $B A = \mathbb{1}$


   Symmetrien des euklidischen Raums:
   $x \t R x$ mit $R^{T} T = \mathbb{1}$
   $R \in O(3) \subset ULU3$
** Symmetriegruppe (M)
   Symmetrien in Physik und Mathe \rightarrow Gruppen \\
   Bisher:
   - Matrixgruppen
	 - $GL(n)$ - Symmetriegruppe des Vektorraums $\mathbb{R}^n$
	 - $O(n)$ - Symmetriegruppe des euklidischen Raumes

   Allgemeiner: Eine Gruppe ist eine Menge $G$ mit einer Binären Operation $G \times G \to G$ für die gilt:
   - $(a \cdot b) \cdot c = a \cdot (b\cdot c)$
   - $\Exists e \in G: a\cdot e = e\cdot a = a \Forall a$ ("Eins")
   - $\Forall a\in G \Exists a^{-1} \in G: a\cdot a^{-1} = a^{-1} \cdot a = e$

   Eine Gruppe heißt "abelsch" falls $a\cdot b = b\cdot a \Forall a,b$
   Beispiele dafür:
   - $\mathbb{Q}\setminus \{0\}, \mathbb{R} \setminus \{0\}, \mathbb{C} \setminus \{0\}$
   Falls sie statt "$\cdot$" die Operation "$+$" zur Gruppenoperation erklären, dann sind
   - $\mathbb{Q}, \mathbb{R}, \mathbb{C}, \mathbb{Z}$
   Gruppen mit $"+"$

   #+ATTR_LATEX: :options [Körper]
   #+begin_defn latex
   $K$ mit Operationen $+,\cdot$ ist ein Körper falls:
   - $(K,+)$ ist abelsche Gruppe (Eins = 0)
   - $(K\setminus\{0\}, \cdot)$ ist auch abelsche Gruppe
   - Distributivität
   #+end_defn

   $GL(n)$ ist eine (nicht abelsche) Gruppe. Müssen prüfen: $A,B$ invertierbar $\implies A\cdot B$ invertierbar. Wir geben das Inverse zu $A\cdot B$ einfach an:
   \[(B^{-1}A^{-1})(AB) = B^{-1}(A A^{-1}) B = B^{-1} B = \mathbb{1}\]

   $GL^+ (n)$ - orientierungserhaltende Untergruppe $\equiv$ alle $A$ in  $GL(n)$ mit $\det A > 0$ \\
   $O(n)$ ist Untergruppe von $GL(n)$. Müssen prüfen dass $A,B$ orthogonal $\implies A\cdot B$ orthogonal. Dazu:
   \[(A\cdot B)^{T} (A\cdot B) = B^{T} A^{T} A B = B^{T} B = \mathbb{1}\]

   Wichtige Untergruppe: Spezielle Orthogonale Transformation $SO(n)$ \\
   Diese Transformationen erfüllen: $\det(R) = 1$

   Dazu zwei Fakten: $\det A^{T} = \det A, \det(AB) = (\det(A))(\det B)$
   Damit folgt aus $R^{T} R =\mathbb{1}$
   \[\det(R^{T} R) = \det(R^{T})(\det R) = (\det R)^2 = \det {\mathbb{R}} = 1, \det R = \pm 1\]
   $\equiv$ Matrizen in $O(n)$ mit $\det = 1$

   Speziell in $n = 3$ (3d-Raum) wird die Reflexion bezüglich y,z Ebene beschrieben durch:
   \begin{align*}
   R_x = \begin{pmatrix} -1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}, \det R_x = -1
   \end{align*}
   Fakt: Jedes Element von $O(3)$ ist schreibbar als $R$ oder $R \cdot R_x$ mit $R \in SO(3)$, $SO(3)$ sind "echte" Drehungen.

   Überlegen Sie sich, dass $R\in SO(2)$ allgemein schreibbar ist als
   \[R = \begin{pmatrix} \cos{\phi} & -\sin{\phi} \\ \sin{\phi} & \cos{\phi}\end{pmatrix}\]
   Identifizieren sie $SO(2)$ mit folgender Menge
   \[\{z \in \mathbb{C} \mid \abs{z} = 1\}\]
   Die Gruppenoperation soll der komplexen Multiplikation entsprechen
** Tensoren
   Ein Tensor von Rang (oder Stufe) $m$ im n-dimensionalen Vektorraum $V = \mathbb{R}^n$ ist eine multilineare Abbildung:
   \[t:\underbrace{V\times V\times \ldots \times V}_{m\text{-mal}} \to \mathbb{R}\]
   Praktisch:
   \[t: (\v x_{(1)}, \v x_{(2)}, \ldots, \v x_{(n)}) \mapsto t_{i_1 \ldots i_m} x_{(1)}^{i_1} \ldots x_{(m)}^{i_m} \]
   #+begin_ex latex
   - Euklidisches Skalarprodukt: $V\times V \to \mathbb{R}$
	 \[\delta: (\v x, \v y) \mapsto \delta_{ij} x^i y^j \equiv \v x \cdot \v y \in \mathbb{R}\]
   - Noch einfacher:
	 \[t: V \to \mathbb{R}: t_i \v x \to t_i x^i \in \mathbb{R}\]
	 Die Menge solcher linearen Abbildungen bildet auch einen n-dimensionalen Vektorraum, den sogenannten Dualraum $V^\ast$ (zu $V$)
	 Notation: $\ubar t = \{t_1, \ldots, t_n\} \in V^\ast$ \\
	 Erinnerung: $\v x = \{x^1, \ldots, x^n\} \in V$ \\
	 Oben(Unten schreiben der Indizes macht die "natürliche Wirkung") von $\ubar t$ auf $\v x$ besonders deutlich: $t_i x^i \in \mathbb{R}$
	 - oben: kontravariant
	 - unten: kovariant ($\rightarrow$ Co-Vektor $\in V^\ast$)
	 Für und: enorme Vereinfachung: \\
	 Wir haben immer euklidischen Raum und damit die besondere Rolle von $\delta_{ij}$ und die inversen Matrix $\delta^{ij}$
	 \[\delta_{ij} \delta^{jk} = \delta_i^k = (\mathbb{1}_i^k)\]
	 Dies erlaubt uns Indizes beliebig zu "heben" und zu "senken":
	 \[t^i \equiv \delta^{ij} t_j, x_i \equiv \delta_{ij}x^j\]
	 Damit können wir $V$ und $V^\ast$ identifizieren. Wir können auch alle Tensor Indizes beliebig oben oder unten schreiben.
	 Wir werden zur Vereinfachung weiterhin schreiben
	 \[\v x \v y = x^i y^i (~\text{eigentlich}~x^i y^j \delta_{ij})\]
	 (Mehr zum Dualraum in Lineare Algebra)
   #+end_ex
   Für uns: Tensor der Stufe 1: ist Vektor
   \[t: \v x \mapsto t^i x^i = \v t \v x \in \mathbb{R}\]
   Wichtig für uns: Resultat von Anwendung eines Rang-1-Tensors auf Vektor ist invariant unter Drehungen:
   \[x\mapsto Rx; t\mapsto Rt\]
   \[x\mapsto x' = Rx, x^{ij} = R^{ij} x^j\]
   Invarianz:
   \[t^{T} x = t^{T} R^{T} R x\]
   Betrachte einfaches, allgemeines Beispiel für Tensor der Stufe 2:
   \begin{align*}
   t\equiv U\otimes W \in V\otimes V \\
   t:(\v x, \v y) \mapsto (u^i w^j) \cdot (x^i y^j) &= (\v u \cdot \v x) \cdot (\v w \cdot \v y) \\
   \intertext{mit $t^{ij} \equiv u^i w^j$}
   &= t^{ij} x^i y^j \\
   \end{align*}
   Grob gesagt: $V \otimes V$ ist die Menge aller Linearkombinationen von Elementen wie $U\otimes W$ \\
   Transformation von $t^{ij} = u^i w^j$ unter Drehungen:
   \[t^{ij} = u^i w^j \xrightarrow{R} R^{ik} u^k R^{jl} w^l = R^{ik} R^{jl} t^{kl}\]
   Invarianz von $t(\v x, \v y):$
   \begin{align*}
   t(\v x, \v y) \to (R t) (R x, R y) &= (R^{ik} r^{jl} t^{kl}) (R^{ip} x^p) (R^{jq} y^q) \\
   &= (R^{ik} R^{ip}) (R^{jl} R^{jq}) t^{kl} x^p y^q \\
   &= \delta^{kp} \delta^{lq} t^{kl} x^p y^q \\
   &= t^{kl} x^{k} y^{l} \\
   &= t(\v x, \v y)
   \end{align*}
   Allgemeine Transformation eines Tensors unter Drehungen:
   \[t \to t' = Rt, t^{\prime i_1 \ldots i_m} = R^{i_1 j_1} \ldots R^{i_m j_m} t^{j_1 \ldots j_m}\]
   Invarianz von $t(\v u_{(1)}, \ldots, \v u_{(m)})$ folgt wie oben.

   Fortgeschrittener Kommentar: Gruppe wirkt auf Vektoren aus $K \equiv$ Darstellung

   Für unser Beispiel der Wirkung von $O(n)$ auf $\mathbb{R}^k$ war das "offensichtlich" mit Tensoren haben wir "nicht triviales Beispiel für Darstellung"
   \[\underbrace{R\in O(n)}_{\text{Elemente $R^{ij}$}} \overset{\text{Darst.}}{\mapsto} D(R) \in \underbrace{n^2 \times n^2 \text{-Matrizen}}_{\text{Elemente}~D(R)^{ij,kl} = R^{ik}R^{jl}}\]
   Dieses $D(R)$ wirkt wie oben beschrieben auf Tensoren:
   \[t^{ij} \overset{D(R)}{\mapsto} D(R)^{ij,kl} t^{kl}\]
   $D(R)$ ist eine Darstellung von $O(n)$, die verschieden ist von der "definierenden" Darstellung

   *Transformation von $\delta^{ij}$* \\
   \[\delta^{\prime ij} = R^{ik} R^{jl} \delta^{kl} = R^{ik} R^{jk} = \delta^{ij}\]
   $\implies \delta^{ij}$ ist ein *invarianter Tensor* \\

   weiteres Beispiel: (für $m = n$: Levi Civita-Tensor) \\
   Wir schreiben nur $m = n = 3$ Fall aus:
   \[\eps(\v x, \v y, \v z) = \eps^{ijk} x^i y^j z^k = x^i \eps^{ijk} y^j z^k = \v x(\v y \times \v z)\]
   *Transformation:*
   \[\eps^{\prime i_1 i_2 i_3} = R^{i_1 j_1} R^{i_2 j_2} R^{i_3 j_3} \eps^{j_1 j_2 j_3} = \eps^{i_1 i_2 i_3} \det(R) \underarrow[=]{$R\in SO(3)$} \eps^{i_1 i_2 i_3}\]

   *Fakt:* Falls $t_1, t_2$ Tensoren vom Rang $m_1, m_2$ sind, so ist das folgende ein Tensor vom Rang $m_1 + m_2 - 2l_i$:
   \[t_1^{i_1 \ldots i_l i_{l + 1} \ldots i_m} t_2^{i_1 \ldots i_l j_{l + 1} \ldots j_{m_2}} = t^{i_{l + 1} \ldots i_{m_1} j_{l + 1} \ldots j_{m_2}}\]

   *Anwendungen:*
   $\v a \times \v v$ ist ein Pseudovektor:
   \[(\v a' \times \v b')^i \equiv \eps^{ijk} a^{\prime j} b^{\prime k} = \underarrow[\pm]{falls Spiegelung} \eps^{\prime ijk} a^{\prime j} b^{\prime k} = \pm R^{il} \eps^{ljk} a^j b^k = \pm R^{il} (\v a\times \v b)^l \]
** Galilei-Transformationen
   *Bisher:* $\mathbb{R}^3$ mit Symmetriegruppe $O(3)$ \\
   *Jetzt:* Physikalische Raum Zeit: Zusätzlich: $t\in \mathbb{R}$ \\
   Punkt $\v x \in \mathbb{R}^3 \xrightarrow{neu}$ Ereignisse $(t,\v x) \in \mathbb{R} \times \mathbb{R}^3$ \\
   Müssen abschaffen: $\v 0$ im Vektorraum. In der Tat: $\abs{\v x}, \abs{\v y}$ sind unphysikalisch, physikalisch ist nur $\abs{\v x - \v y}$, ebenso ist nur $t_1 - t_2$ physikalisch

   $\implies$ Symmetrietransformationen:
   1. Rotationen: $(t,x) \mapsto (t,Rx), R \in O(3)$
   2. Translationen: $(t,x) \mapsto (t + s, x + y), s \in \mathbb{R}, y\in\mathbb{R}^3 \implies$ Abschaffung der $0\in\mathbb{R}$ und $\v 0 \in \mathbb{R}^3$"
   3. Boosts: $(t, x) \mapsto (t, x + vt),v\in\mathbb{R}^3$ "zeitabhängige Verschiebung"

   Die Galilei-Gruppe $G$ ist die von 1., 2. und 3. "generierte" Gruppe. Nicht trivialer Fakt: Jedes $g\in G$ ist schreibbar als $g = \overarrow[g_3]{Boost} \circ \underarrow[g_2]{Trans} \circ \overarrow[g_1]{Rot.}$
   Man muss dazu unter anderem zeigen, dass es zu einem $g_2 \circ g_1 \circ g_2'' \in G$ ein $g_2'', g_1''$ gibt, sodass $g_2 \circ g_1 \circ g_2' = g_2'' \circ g_1''$ \\
   "Boost" = Zunahme (der Geschwindigkeit). Boost einer Trajektorie: $(t, \v x(t)) \mapsto (t, \v x(t) + \v v_0 t)$ \\
   $\v v = \dot{\v x}(t) \mapsto \v v' = \dot{\v x}(t) + \v v_0$

   Boost zerstören das Konzept der Gleichörtlichkeit: Seien $(t, x), (t', x)$ zwei Ereignisse am gleichen Ort. Boost $\implies (t, x + v t), (t', x + v t')$, *nicht* mehr am gleichen Ort
** Affiner Raum
   #+begin_defn latex
   $O(3)$ Symmetriegruppe des euklidischen Raumes. "Elegant!". \\
   Besser: Definition des *affinen Raumes*: Gegeben sein Menge $A$, ein Vektorraum $V$ und eine Abbildung $A\times A \to V, (P, Q) \mapsto \v{PQ}$
   sodass $\v{PQ} + \v{QR} = \v{PR}$. Außerdem: Zu jeden $P\in A, \v V$ soll es eindeutig ein $Q\in A$ geben, sodass $\v = \v{PQ}$, das Paar $(A,V)$ heißt affiner Raum
   #+end_defn
   #+begin_ex latex
   Zu jedem Vektorraum gehört ein affiner Raum: Wähle $A \equiv V, V\times V \to V, (\v x, \v y) \mapsto \v y - \v x$
   #+end_ex
   Sei $(A^4, V^4)$ ein 4-dimensionaler affiner Raum. (Man denke zum Beispiel an den zu $\mathbb{R} \times \mathbb{R}^3$ gehörigen affinen Raum) \\
   Physikalische Raumzeit: $(A^{(4)}, V^{(4)})$ mit
   1. Eine lineare Abbildung $V^4 \to \mathbb{R}$ ("Zeitfunktion") (im konkreten Beispiel: $((t,x), (t', x')) \mapsto t' - t$)
   2. Sei $\tilde V^{(3)} \subset V^{(4)}$ der Raum von Pfeilen zwischen gleichzeitigen Ereignissen ($v\in \tilde V^{(3)}$ heißt $T(\tilde v) = 0$)
	  Dann hat $\tilde V$ ein Skalarprodukt, "Abstandsfunktion". Im konkreten Beispiel: $(t,x),(t,x') \mapsto \abs{x - x'}$

   Zusammen bilden 1. und 2. eine Galileische Struktur. Die physikalische Raumzeit ist $(A^{(4)}, V^{(4)})$ mit galileischer Struktur

   $G$ sind die Transformationen des $(A^{(4)}, V^{(4)})$, welche seine Galileische Struktur respektieren.
   # Zur Anschauung: (für 2d-Welt $\mathbb{R} \times \mathbb{R}^2$)
** Dynamik
   Dynamik soll invariant sein! Betrachte Trajektorie, die die Bewegungsgleichung erfüllt:
   \[(t, \v x(t)), m \frac{\d^2 \v x}{\d t^2} = \v F(t, \v x(t))\]
   Transformierte Trajektorie:
   \[t', \v x'(t') = (t + s, R\v x(t) + \v y + \v v(t + x))\]
   Dazu:
   \[m \frac{\d^2}{\d t'^2} = m \frac{\d^2 x}{\d t^2} = m \\frac{\d^2}{\d t^2} (R\v x(t) + \v y + \v v(t + s)) = m R \frac{\d^2 \v x}{\d t^2} = R\v F(t, \v x(t))\]
   $\implies$ Newtonsche Dynamik ist invariant falls Kräfte wie Vektoren transformieren. (hatten wir schon verlangt)
   Bei Systemen von Massepunkten mit Zentralkräften ist die Kraft gleich dem Gradient, sie besitzt automatisch Vektor-Transformationseigenschaften

   Wir fordern bei
   \[m R \frac{\d^2 \v x}{\d t^2} = R\v F(t,\v x(t))\]
   eigentlich
   \[R\v F(t,\v x(t)) = \v F'(t', x'(t'))\]
   Wichtig: Die Transformation von $\v F$ beinhaltet nicht nur Drehung, sondern auch Transformation über das Argument.
   Betrachte zur Vereinfachung $R = \mathbb{1} \implies \v F'(t', \v x') = \v F(t, \v x)$ \\

   Geschwindigkeitsabhängige Kräfte: zum Beispiel Reibung
   \[\v F_R = -\alpha(\v x - \underarrow[v u]{Medium})\]
** Zusammenfassung:
   Allgemeingültiges Schema:
   - Beschreibung der Bewegung festlegen (Spielfeld) (hier: affiner Raum und Galileische Struktur)
   - Identifikation der Symmetriegruppe (hier Galilei Gruppe)
   - Invarianz der Dynamik prüfen beziehungsweise fordern (Spielregeln) (Newtonsches Grundgesetz)
