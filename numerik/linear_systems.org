* Lineare Gleichungssystem
  Gegeben: $A ∈ ℝ^{m × n} = (a_{ij}), b ∈ ℝ^m$. Gesucht: $x ∈ ℝ^n$ mit $Ax = b$
  $⇒ m$ Gleichungen, $n$ Unbekannte. Das lineare Gleichungssystem $Ax = b$ heißt
  - unterbestimmt, falls $m < n$
  - überbestimmt falls $m > n$
  - quadratisch falls $m = n$
  *Störungsstheorie*:
  - Konditionierung von quadratischen linearen Gleichungssystemen
  - Fehlereinfluss von Datenfehlern und Rundungsfehlern auf Lösung $x$
  *Vektor- und Matrizennormen*: \\
  Sei $\mathbb{K} = ℝ$ oder $\mathbb{K} = ℂ$. Erinnerung: Eigenschaften einer Norm: $\norm{·}: \mathbb{K}^n \to ℝ$
  - Definitheit: $\norm{x} > 0 ∀ x ∈ \mathbb{K}^n \setminus \{0\}$
  - Positive Homogenität: $\norm{α x} = \abs{α} \norm{x} ∀ x ∈ \mathbb{K}^n, α ∈ \mathbb{K}$
  -	Subadditivität: $\norm{x + y} \leq \norm{x} + \norm{y} ∀ x, y ∈ \mathbb{K}^n$
  #+begin_ex latex
  Euklidische Norm: $(l_2)$
  \[\norm{x}_2 = (\sum_{i = 1}^{n} \abs{x_i}^2)^{1/2}\]
  Maximumsnorm $(l_∞)$
  \[\norm{x}_∞ = \max_{i = 1, \dots, n} \abs{x_i}\]
  $l_1$ -Norm:
  \[\norm{x}_1 = \sum_{i = 1}^{n} \abs{x_i}\]
  $l_p$ -Norm, $p \geq 1, p < ∞$
  \[\norm{x}_p = (\sum_{i = 1}^{n} \abs{x_i}^p)^{1/p}\]
  #+end_ex
  Betrachte Vektorraum der $n × n$ -Matrizen $A ∈ \mathbb{K}^{n × n}$
  #+begin_defn latex
  Eine Norm $\norm{·}$ auf $\mathbb{K}^{n × n}$ heißt verträglich mit einer Vektornorm $\norm{·}$ auf $\mathbb{K}^n$, wenn gilt:
  \[\norm{A x} \leq \norm{A} \norm{x} ∀ x ∈ \mathbb{K}^n, A ∈ \mathbb{K}^{n × n}\]
  Sie heißt Matrizennorm, wenn sie submultiplikativ ist
  \[\norm{AB} \leq \norm{A} \norm{B} ∀ A, B ∈ \mathbb{K}^{n × n}\]
  #+end_defn
  #+begin_ex latex
  Die Frobeniusnorm
  \[\norm{A}_{Fr} = (\sum_{i,j = 1}^{n} \abs{a_{ij}}^2)^{1/2}\]
  ist eine mit $\norm{·}_2$ verträgliche Matrizennorm. \\
  Die natürliche Matrizennorm
  \[\norm{A} = \sup_{x ∈ \mathbb{K}^n \setminus \{0\}} \frac{\norm{A x}}{\norm{x}} = \sup_{\substack{x ∈ \mathbb{K}^n\\ \norm{x} = 1}} \norm{A x}\]
  ist eine mit $\norm{·}$ verträgliche Matrizennorm (Übung!). Es gilt
  \[\norm{\mathbb{I}} = \sup_{\substack{x ∈ \mathbb{K}^n\\ \norm{x} = 1}} \norm{\mathbb{I} x} = 1\]
  #+end_ex
  #+begin_lemma latex
  Die natürlichen Matrizennormen zu $\norm{·}_∞$ und $\norm{·}_1$ sind die "maximale Zeilen-/Spaltensumme":
  \begin{align*}
  \norm{A}_∞ &= \max_{j = 1, \dots, n} \sum_{k = 1}^{n} \abs{a_{jk}} \\
  \norm{A}_1 &= \max_{k = 1, \dots, n} \sum_{j = 1}^{n} \abs{a_{jk}}
  \end{align*}
  #+end_lemma
  #+begin_proof latex
  Skript.
  #+end_proof
  Betrachte: $A x = b$ und Störung
  \[\underbrace{(A + δ A)}_{\tilde A}\underbrace{(x + δ x)}_{\tilde x} = \underbrace{b + δ b}_{\tilde b}\]
  #+ATTR_LATEX: :options [Neumann-Reihe]
  #+begin_thm latex
  Gilt $\norm{A} < 1$, so
  \[\mathbb{I - A} \sum_{k= 0}^{∞} A^k = \mathbb{I}\]
  #+end_thm
  #+begin_proof latex
  Für die Partialsummen gilt
  \[(\mathbb{I} - A) \sum_{k = 0}^{n} A^k = \mathbb{I} - A + A - A^2 + A^2 \dots - A^{n + 1} \xrightarrow{n \to ∞} \mathbb{I}\]
  wegen $\norm{A^k} \leq \norm{A}^k \xrightarrow{k \to ∞} 0$.
  #+end_proof
  Wiederholung: Kondition numerischer Aufgabe $y = f(x), y ∈ ℝ^n, x ∈ ℝ^m$.
  \[\frac{Δy_i}{y_i} \overset{·}{=} \sum_{j = 1}^{m} \pp{f_i}{x_j}(x) \frac{Δ x_j}{y_i} = \sum_{j = 1}^{m} \underbrace{\pp{f_i}{x_j} \frac{x_j}{f_i(x)}}_{=: k_{ij}(x)} \frac{Δ x_j}{x_j}\]
  Neumann-Reihe:
  \[\norm{A} < 1 ⇒ (\mathbb{1} - A_a)^{-1} = \sum_{n = 0}^{∞} A^n\]
  Natürliche Matrixnorm:
  \[\norm{A} = \sup_{\norm{x} = 1} \norm{A x}\]
  $\norm{A}_∞$: "Zeilensummennorm" \\
  $\norm{A}_1$: "Spaltensummennorm" \\
  Euklidisches Skalarprodukt auf $\mathbb{K}$
  \[(x, y)_2 = \bar y^T x\]
  #+ATTR_LATEX: :options [Spektralnorm]
  #+begin_lemma latex
  Für $A . \mathbb{K}^{n × n}$ ist
  \[\norm{A}_2 = \max\{\sqrt{\abs{λ}} \mid λ \text{ Eigenwert von } \bar A^T A\}\]
  Für hermitesche $A = \bar A^T$ gilt:
  \[\norm{A}_2= \max \{\abs{λ} \mid λ \text{ Eigenwert von } A\}\]
  #+end_lemma
  #+begin_proof latex
  $B = \bar A^T A$ ist hermitesch. $⇒ B$ hat $n$ reelle Eigenwerte $λ_1, \dots, λ_n$ und eine Orthonormalbasis von Eigenvektoren $\{w_1, \dots, w_n\} ⊂ \mathbb{K}^n$
  $B ω_i = λ_i ω_i$. Jedes $x ∈ \mathbb{K}^n$ hat eine eindeutige Darstellung
  \[x = \sum_{i = 1}^{n} α_i ω_i\]
  \begin{align*}
  ⇒ \norm{x}_2^2 &= (x, x)_2 = \sum_{i, j = 1}^{n} α_i \bar α_j \underbrace{(ω_i, ω_j)_2}_{δ_{ij}} = \sum_{i = 1}^{n} \abs{α_i}^2 \\
  \norm{A x}_2^2 &= (Bx, Bx)_2 = \sum_{i, j = 1}^{n} λ_i α_i \overline{(λ_j α_j)} \underbrace{ω_i, ω_j}_{δ_{ij}} \\
  &= \sum_{i = 1}^{n} \abs{λ_i}^2 \abs{α_i}^2 \\
  \norm{B}_2^2 &= \sup_{x ∈ \mathbb{K}^n \setminus{\{0\}}} \frac{\norm{B x}_2^2}{\norm{x}_2^2} = \sup_{x ∈ \mathbb{K}^n \setminus{\{0\}}} \frac{\sum_{i = 1}^{n} λ_i^2 \abs{α_i}^2}{\sum_{i = 1}^{n} \abs{α_i}^2} \\
  &\leq \max_{i = 1, \dots, n} \abs{λ_i}^2
  \end{align*}
  Mit
  \begin{align*}
  \abs{λ_i} &= \abs{λ_i} \norm{ω_i}_2 = \norm{λ_i ω_i}_2 = \norm{B ω_i}_2 \\
  &\leq \norm{B}_2 \norm{ω_i}_2 = \norm{B}_2, \quad i = 1, \dots, n
  \end{align*}
  #+end_proof
  Betrachte $A x = b$ und Störung
  \[\underbrace{(A + δA)}_{\tilde A} \underbrace{(x + δ x)}_{\tilde x} = \underbrace{b + δ b}_{\tilde b}\]
  #+ATTR_LATEX: :options [Störungssatz]
  #+begin_thm latex
  Die Matrix $A ∈ \mathbb{K}^{n × n}$ sei regulär uns es sei
  \[\norm{δ A} \leq \frac{1}{\norm{A^{-1}}}\]
  Dann ist die gestörte Matrix $\tilde A = A + δ A$ ebenfalls regulär. Für den relativen Fehler der Lösung gilt mit die Konditionszahl von $A$
  \[\cond(A) = \norm{A} \norm{A^{-1}}\]
  die Ungleichung
  \[\frac{\norm{δ x}}{\norm{x}} \leq \frac{\cond(A)}{1 - \cond(A) \frac{\norm{δ A}}{\norm{A}}} \left[\frac{\norm{δ b}}{\norm{b}} + \frac{\norm{δ A}}{\norm{A}}\right]\]
  #+end_thm
  #+begin_proof latex
  \[\norm{A^{-1} δ A} \leq \norm{A^{-1}}\norm{δ A} < 1\]
  Neumann $⇒$ $A + δ A = A[\mathbb{1d} + A^{-1} δ A]$ ist regulär. \\
  $(A + δ A) \tilde x = b + δ  b, (A + δ A) x = b + δ A x$
  \[⇒ (A + δ A) δ x= δ b - δ A x\]
  \begin{align*}
  \norm{(A + δ A)^{-1}} &= \norm{[A(\mathbb{1} + A^{-1})]^{-1}} \\
  &= \norm{(\mathbb{1} + A^{-1} δ A)^{-1} A^{-1}} \leq \norm{\sum_{n = 0}^{∞}(-A^{-1} δ A)^n} \norm{A^{-1}} \\
  &\leq (\sum_{n = 0}^{∞} \norm{A^{-1} S A}) \norm{A^{-1}} = \frac{1}{1 - \norm{A^{-1} δ A}} \norm{A^{-1}}
  \end{align*}
  \begin{align*}
  \norm{b} &= \norm{A x} \leq \norm{A} \norm{x} \\
  \norm{δ x} &\leq \norm{(A + δ A)^{-1}}[\norm{ δ b} + \norm{δ A} \norm{W}] \\
  &\leq \frac{\norm{A^{-1}}}{1 - \norm{A^{-1} δ a}}[\norm{δ B} \norm{b A} \norm{x}] \\
  &\leq \frac{\norm{A^{-1}}}{1 - \norm{A^{-1}}\norm{ δ A} \norm{A} \norm{A}^{-1}} \left[\frac{\norm{ δ b}}{\norm{x}} + \frac{\norm{S A}}{\norm{A}}\right]
  \end{align*}
  \[\frac{\cond(A)}{1 - \cond (A) \frac{\norm{δ A}}{\norm{A}}}\left[\frac{\norm{ S b}}{\norm{b}} + \frac{\norm{δ A}}{A}\right] \norm{x}\]
  #+end_proof
  Ist $\cond(A) \norm{δ A} \ll \norm{A_i}$, so gilt
  \[\frac{\norm{δ x}}{\norm{x}} \overset{·}{\leq} \cond (A) \left[\frac{\norm{δ b}}{\norm{b}} + \frac{\norm{δ A}}{A}\right]\]
  Die Konditionszahl hängt von der verwendeten Norm ab.
  #+begin_ex latex
  1. $\cond_∞(A) = \norm{A}_∞ \norm{A^{-1}}_∞$
  2. Für die Spektralnorm gilt:
	 \[\cond_2 (A) = \norm{A}_2 \norm{A^{-1}} = \sqrt{\frac{\abs{μ_{max}}}{\abs{μ_{min}}}}\]
	 wobei $μ_{max}, μ_{min}$ betragsgrößter beziehungsweise kleinster Eigenvektor von	$\bar A^T A$. Ist $A = A=$. Ist $A = \bar A^T$ so gilt:
	 \[\cond_2(A) = \frac{\abs{λ_{max}}}{\abs{λ_{min}}}\]
	 mit $λ_{max}$ und $λ_{min}$ betragsgrößter beziehungsweise kleinster Eigenvektor von $A$. Regel: Es gelte $\cond (A) \approx 10^s$
	 \[\frac{\norm{δ A}}{\norm{A}} \approx 10^{-k}, \frac{\norm{δ b}}{\norm{b}} \approx 10^{-k}\]
	 Dann muss ein relativer Fehler von
	 \[\frac{\norm{δ x}}{\norm{x}} \approx 10^{s - k}\]
	 erwartet werden. Mit $\norm{·} = \norm{·}_∞$ verliert man $s$ Stellen Genauigkeit.
  #+end_ex
  #+begin_ex latex
  \[A = \begin{pmatrix}1 & 1 \\ 0 & ε\end{pmatrix}, ε ∈ \string(0, 1], A^{-1} = \begin{pmatrix}1 & -ε^{-1} \\ 0 & ε^{-1}\end{pmatrix}\]
  \begin{align*}
  ⇒ \norm{A}_∞ &= 2, \norm{A^{-1}}_∞ = 1 + ε^{-1} \\
  ⇒ \cond_∞ \norm{A} \norm{A^{-1}} = 2 + ε^{-1}
  \end{align*}
  für $ε = 10^{-8}$ kann man bereits 8 Stellen Genauigkeit verlieren.
  #+end_ex
  Ist die Abschätzung im Störungssatz scharf? Sei $A ∈ ℝ^{n × n}$ symmetrisch positiv definit mit Eigenwerten $λ_1 \geq \dots \geq λ_n$.
  Wähle: $δ A = 0, b = ω_1, δ  B = ε w_k, ε \neq 0$
  \begin{align*}
  A x &= b ⇒ x = \frac{1}{λ_1} w_1 \\
  A \tilde x &= b + δ b ⇒ \tilde x = \frac{1}{λ_1} ω_1 + ε \frac{1}{λ_k} ω_k \\
  ⇒ \frac{\norm{δ x}_2}{\norm{x}_2} &= \abs{ε} \frac{λ_1}{λ_n} \frac{\norm{ω_n}_2}{\norm{ω_1}_2} \\
  &= \cond(A) \frac{\norm{δ b}_2}{\norm{b}_2}
  \end{align*}
** Eliminationsverfahren
   Direkte Methode zur Lösung von $A x = b, A ∈ ℝ^{n × n}$. Spezialfall: $A$ obere Dreiecksmatrix $a_{ij} = 0, i > j$
   \[\begin{pmatrix}a_{11} & \dots & \dots & \dots & a_{1n} \\ 0 & a_{22} &   &   &  \vdots \\ \vdots & 0 & \ddots &   & \vdots \\ \vdots & \vdots & \ddots & \ddots & \vdots \\ 0 & 0 & \dots & 0 & a_{nn}\end{pmatrix} \begin{pmatrix}x_1 \\ \vdots \\ \vdots \\ \vdots \\ x_n\end{pmatrix} = \begin{pmatrix}b_1 \\ \vdots \\ \vdots \\ \vdots \\ b_n\end{pmatrix}\]
   Ist $a_{ii} \neq 0, i = 1, \dots, n$ löst man durch Rückwärtseinsetzen
   \[x_j = \begin{cases} \frac{b_n}{a_{nn}} & j = n \\ \frac{1}{a_{jj}}(b_j - \sum_{k = j + 1}^{n} a_{jk} x_k) & j= n - 1, \dots, 1 \end{cases}\]
   Arithmetische Operationen:
   \[\sum_{j = 1}^{n} j = \frac{(n + 1) n}{2} = \frac{n^2}{2} + \mathcal{O}(n)\]
   Eine	Operation: eine Division oder eine Multiplikation und eine Addition.
   Wiederholung: Konditionszahl einer Matrix
   \[A ∈ ℝ^{n × n}: \cond(A) = \norm{A} \norm{A^{-1}}\]
   Störungssatz: $(A + δA)(x + δx) = b + δb$
   \[\frac{\norm{δ x}}{x} \leq \frac{\cond(A)}{1 - \cond(A) \frac{\norm{δ A}}{\norm{A}}}[\frac{\norm{δ A}}{\norm{A}} + \frac{\norm{δ b}}{\norm{b}}]\]
   *Gaußsches Eliminationsverfahren* \\
   Umformung von $A x = b$ auf $R x = c$ mit $R$ obere Dreiecksmatrix mittels
   - Vertauschen von Gleichungen
   - Addition von Vielfachen einer Gleichung zu einer anderen
   Annahme: $A$ hat Vollrang
   0. [@0] Setze $A^{(0)} = A, b^{(0)} = b$
      \[\left[\begin{array}{ccc|c} a_{11}^{(0)} & \dots & a_{1n}^{(0)} & b_{1}^{(0)} \\ \vdots & & \vdots & \vdots \\ a_{n1}^{(0)} & \dots & a_{nn}^{(0)} & b_{n}^{(0)}\end{array}\right]\]
   1. Wähle $r ∈ \{1, \dots, n\}$ mit $a_{r1}^{(0)} \neq 0$ (Pivotelement) und vertausche 1. und r-te Zeile
      \[\left[\begin{array}{ccc|c} \tilde a_{11}^{(0)} & \dots & \tilde a_{1n}^{(0)} & \tilde b_{1}^{(0)} \\ \vdots & & \vdots & \vdots \\ \tilde a_{n1}^{(0)} & \dots & \tilde a_{nn}^{(0)} & \tilde b_{n}^{(0)}\end{array}\right] := [\tilde A^{(0)} \mid \tilde b^{(0)}]\]
   2. Für $j = 2, \dots, n$ eliminiere $\tilde a_{j1}^{(0)}$ durch Subtraktion von $\frac{\tilde a_{j1}^{(0)}}{\tilde a_{11}^{(0)}} := q_{j1}$
	  mal der ersten Zeile von den Zeilen $2, \dots, n$:
      \[\left[\begin{array}{cccc|c} \tilde a_{11}^{(0)} & \tilde a_{12}^{(0)} \dots & \tilde a_{1n}^{(0)} & \tilde b_{1}^{(0)} \\ 0 & \tilde a_{22}^{(1)} \dots & \tilde a_{2n}^{(1)} & \tilde b_{2}^{(1)} \\ \vdots & \vdots & & \vdots & \vdots \\ 0 & \tilde a_{n2}^{(1)} & \dots & \tilde a_{nn}^{(1)} & \tilde b_{n}^{(1)}\end{array}\right] := [A^{(1)} \mid b^{(1)}]\]
	  Fahre fort auf kleinerem System $⇒ [A^{(0)} \mid b^{(0)}] \to [A^{(1)}\mid b^{(1)}] \to \dots \to [A^{(n - 1)} \mid b^{(n - 1)}] =: [R\mid c]$
   Wird im $k$ -ten Schritt $[A^{(k - 1)} \mid b^{(n - 1)}] \to [\tilde A^{(k - 1)} \mid \tilde b^{(n - 1)}] \to [A^{(k)} \mid b^{(k)}]$ das Pivot-Element $q_{r_k k}^{k - 1}$ gewählt, so gilt
   $[\tilde A^{(k -1 )} \mid \tilde b^{(k - 1)}] = P_k[A^{(k - 1)} \mid b^{(k - 1)}]$ mit der Permutationsmatrix
   \begin{equation*}
   P_k = \begin{pmatrix}
   1 & & & & & & & & & & \\
   & \ddots & & & & & & & & & \\
   & & 1& & & & & & & & \\
   & & & 0 & \dots & \dots & \dots & 1 & & & \\
   & & & & 1 & & & & & & \\
   & & & & & \ddots& & & & & \\
   & & & & & & 1 & & & & \\
   & & & 1 & \dots & \dots & \dots & 0 & & & \\
   & & & & & & & & 1 & & \\
   & & & & & & & & & \dots & \\
   & & & & & & & & & & 1 \\
   \end{pmatrix} \quad
   G_k = \begin{pmatrix}
   1 & & & & \\
   & \ddots & & & \\
   & -q_{k + 1, k}^{(k)} & 1& & \\
   & \vdots & & \ddots & \\
   & -q_{n,k}^{(k)} & & & 1 \\
   \end{pmatrix}
   \end{equation*}
   Mit den Fehlstellungen von $P_k$ an $k$ und $r_k$ und der Fehlspalte von $G_k$ bei $k$. Weiterhin gilt:
   $[A^{(k)} \mid b^{(k)}] = G_k [\tilde A^{(k - 1)} \mid \tilde b^{(k - 1)}]$ mit
   $q_{jk}^{(k)} = \tilde a_{jk}^{(k - 1)} / \tilde a_{kk}^{(k - 1)}$. $G_k$ heißt Frobenius Matrix.
   Wegen $P_k^{-1} = P_k$ und
   \begin{equation*}
   G_k^{-1} = \begin{pmatrix}
   1 & & & & \\
   & \ddots & & & \\
   & q_{k + 1, k}^{(k)} & 1& & \\
   & \vdots & & \ddots & \\
   & q_{n,k}^{(k)} & & & 1 \\
   \end{pmatrix}
   \end{equation*}
   haben $A x = b$ und $A^{(k)} x = b^{(k)}$ dieselbe Lösung:
   \[A x = b ⇔ A^{(k)} x = G_{n - 1} P_{n - 1} \dots G_1 P_1 A x = G_{n - 1} P_{n - 1} \dots G_1 P_1 b = b^{(k)}\]
   *Wahl des Pivot-Elementes* \\
   Ziel: Numerische Stabilität.
   1. Spaltenpivotierung:
	  \[\abs{a_{r_k, k}^{(k - 1)}} = \max_{j = k, \dots, n} \abs{a_{j k}^{(k - 1)}}\]
   2. Totalpivotierung
	  \[\abs{a_{r_k, s_k}^{(k - 1)}} = \max_{i,j = k, \dots, n} \abs{a_{ij}^{(k - 1)}}\]
	  - bessere Stabilität
	  - teurer
	  - Permutationsmatrizen $Q_k$ für $x$
		\[\underbrace{G_k P_k \dots G_1 P_1 A Q_1 \dots Q_k}_{A^{(k)}} \underbrace{Q_k \dots Q_1 x}_{Q x} = G_k P_k \dots G_1 P_1 b\]
   *Speicherausnutzung* \\
   Die $q_{jk}^{(k)}$ können an den eliminierten Stellen im unteren	Dreieck von $A$ gespeichert werden. Das obere Dreieck von $A$ wird während der Rechnung ersetzt.
   Nach $k$ Eliminationsschritten
   \begin{equation*}
   \left[
   \begin{array}{ccccccc|c}
   r_{11} & r_{12} & \dots & r_{1k} & r_{1, k + 1} & \dots  & r_{1n} & c_1 \\
   λ_{21} & r_{22} & \dots & r_{2k} & r_{2, k + 1} & \dots  & r_{2n} & c_2 \\
   λ_{31} & λ_{32} & \ddots &       & \vdots & & \vdots & \vdots \\
   \vdots & \vdots & \ddots &       & \vdots & & \vdots & \vdots \\
   λ_{k1} & \dots  & λ_{kk} & r_{kk} & r_{k, k + 1} & \dots & r_{kn} & c_k \\
   λ_{k1} & \dots  & \dots & λ_{k + 1, k} & a_{k +1, k + 1}^{(k)} & \dots & a_{k + 1, n}^{(k)} & b^{(k)}_{k + 1} \\
   \vdots &        &       & \vdots       & \vdots  & & \vdots & \vdots \\
   λ_{n1} & \dots  & \dots & λ_{n, k} & a_{n, k + 1}^{(k)} & \dots & a_{n, n}^{(k)} & b^{(k)}_{n} \\
   \end{array}
   \right]
   \end{equation*}
   mit $λ_{i + 1, 1}, \dots, λ_{n i}$ Permutationen von $q_{i + 1,i}^{(k)}, \dots , q_{ni}^{(k)}$. Endresultat ($k = n - 1$)
   \begin{equation*}
   \left[
   \begin{array}{cccc|c}
   r_{11} & \dots & \dots & r_{1n} & c_1 \\
   l_{21} & r_{22} & \dots & r_{2n} & \vdots \\
   \vdots & \ddots  & \ddots & \vdots & \vdots \\
   l_{n1} & \dots & l_{n,n-1} & r_{nn} & c_n \\
   \end{array}
   \right]
   \end{equation*}
   #+ATTR_LATEX: :options [LR-Zerlegung]
   #+begin_thm latex
   Die Matrizen
   \begin{equation*}
   L =
   \left[
   \begin{array}{cccc}
   1 & & &   \\
   l_{21} & 1 & & \\
   \vdots & \ddots  & \ddots & \\
   l_{n1} & \dots & l_{n,n-1} & 1 \\
   \end{array}\right], R =
   \left[
   \begin{array}{ccc}
   r_{11} & \dots & r_{1n} \\
   & \ddots & \vdots \\
   & & r_{nn}
   \end{array}
   \right]
   \end{equation*}
   bilden eine LR-Zerlegung der Matrix $PA$. $PA = LR$, mit $P = P_{n - 1} \dots P_{1}$.
   Für $P = \mathbb{1}$ ist die Zerlegung eindeutig.
   #+end_thm
   #+begin_proof latex
   (für $P = \mathbb{1}$).
   \[R = G_{n - 1} \dots G_1 A ⇔ \underbrace{G_1^{-1} \dots G_{n - 1}^{-1}}_{L} R = A\]
   Eindeutigkeit: Übung.
   #+end_proof
   Aufwand:
   $k$ -ter Eliminationsschritt
   \begin{align*}
   a_{ij}^{(k)} &= a_{ij}^{(k - 1)} - \frac{a_{ik}^{(k - 1)}}{a_{kk}^{(k - 1)}} a_{kj}^{(k - 1)} \\
   b_i^{(k)} &= b_i^{(k - 1)} - \frac{a_{ik}^{(k - 1)}}{a_{kk}^{(k - 1)}} b_k^{(k - 1)}
   \end{align*}
   $i, j = k + 1, \dots, n$ $⇒$ $n - k$ Divisionen, $(n - k) + (n - k)^2$ Multiplikationen und Additionen
   \[⇒ N_{\text{Gauß}}(n) = \frac{1}{3} n^3 + \mathcal{n^2}\]
   Gilt für Lösung von $Ax = b$ und für die Berechnung der Zerlegung $PA = LR$
   #+begin_ex latex
   \[A = \begin{pmatrix}3 & 1 & 6 \\ 2 & 1 & 3 \\ 1 & 1 & 1\end{pmatrix}, b = \cvec{2; 7; 4}\]
   Pivotierung:
   \begin{equation*}
   \left[
   \begin{array}{ccc|c}
   3 & 1 & 6 & 2 \\
   2 & 1 & 3 & 7 \\
   1 & 1 & 1 & 4
   \end{array}\right] \to
   \left[
   \begin{array}{ccc|c}
   3 & 1 & 6 & 2 \\
   2/3 & 1/3 & -1 & 17/3 \\
   1/3 & 2/3 & -1 & 10/3
   \end{array}\right] \to
   \left[
   \begin{array}{ccc|c}
   3 & 1 & 6 & 2 \\
   1/3 & 2/3 & -1 & 10/3 \\
   2/3 & 1/3 & -1 & 17/3 \\
   \end{array}\right] \to
   \left[
   \begin{array}{ccc|c}
   3 & 1 & 6 & 2 \\
   1/3 & 2/3 & -1 & 10/3 \\
   2/3 & 1/2 & -1/2 & 4 \\
   \end{array}
   \right]
   \end{equation*}
   $\to$
   \begin{align*}
   x_3 &= - 8 \\
   x_2 &= \frac{3}{2}(\frac{10}{3} + x_3) = -7 \\
   x_1 &= \frac{1}{3}(2 - x_2 - 6x_3) = 19
   \end{align*}
   LR-Zerlegung:
   \[P_1 = E_3, P_2 = \begin{pmatrix}1 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 1 & 0\end{pmatrix}\]
   \[PA = \begin{pmatrix}3 & 1 & 6 \\ 1 & 1 & 1 \\ 2 & 1 & 3\end{pmatrix} = LR= \begin{pmatrix}1 & 0 & 0 \\ 1/3 & 1 & 0 \\ 2/3 & 1/2 & 1\end{pmatrix} \begin{pmatrix}3 & 1 & 6 \\ 0 & 2/3 & -1 \\ 0 & 0 & -1/2\end{pmatrix}\]
   Für die numerische Stabilität der Gauß-Elimination ist im Allgemeinen eine Pivotierung sehr wichtig.
   #+end_ex
   Rückwärtsanalyse nach Wilkinson $A ∈ ℝ^{n × n}$, löse $A x = b$ mit Gauß-Elimination mit Spaltenpivotierung. Die berechnete Lösung $\tilde x$ ist
   die exakte Lösung eines gestörten Systems $(A + δ A) \tilde x = b$ mit
   \[\frac{\norm{δ A}_∞}{\norm{A}_∞} \leq 1.01 · 2^{n - 1}(n^3 + 2 n^2) eps\]
   (ohne Beweis) \\
   Störungssatz $⇒$
   \[\frac{\norm{δ x}}{\norm{x}} \leq \frac{\cond_∞ (A)}{1 - \cond_∞ (A) \norm{δ A} / \norm{A}} · 1.01 2^{n - 1}(n^3 + 2n^2) eps\]
   Diese Abschätzung deckt pathologische Fälle ab. In der Praxis ist das Verhalten gutartig, das heißt die Gaußelimination mit Spaltenpivotierung ist ein stabiler Algorithmus.
   Wiederholung: Gauß-Elimination $Ax = b, A ∈ ℝ^{n × n}$
   \begin{align*}
   A^{(0)} &= A, A^{(k)}= G_k P_k A^{(k - 1)}, k = 1, \dots, n - 1 \\
   R &= A^{(n - 1)}a = G_{n - 1} P_{n - 1} \dots G_1 P_1 A \\
   \intertext{setze}
   \tilde G_{n - 1} &= P_{n - 1}, \tilde G_k = P_{n - 1} \dots P_{k - 1} G_k P_{k + 1} \dots P_{n - 1} \\
   ⇒ P_{k + 1} \dots P_{n - 1} \tilde G_k &= G_k P_{k + 1} \dots p_{n - 1} \\
   ⇒ R &= \underbrace{\tilde G_{n + 1} \dots \tilde G_1}_{L^{-1}} \underbrace{P_{n - 1} \dots P_1}_{P} A \\
   ⇒ LR &= P A
   \end{align*}
   Löse $R x = c$ oder $L y = b, R x = y$.
** Nachiteration
   Wegen Rundungsfehlern bei der Gauß-Elimination gilt: $P A = L R$ nicht exakt. Damit gilt mit einer Näherungslösung $x^0$ gewonnen aus $L R x^0 = P b$ für den
   sogenannten Defekt
   \[d^0 = b - A x^0 \neq 0\]
   Man kann man eine iterative Defektkorrektur betreiben.
   \begin{align*}
   d^k &= b - A x^k , L R δ x^k = P d^k \\
   x^{k + 1} = x^k + δ x^k, k = 0, 1, \dots
   \end{align*}
   #+begin_lemma latex
   \[x^k = (\sum_{k= 0}^{k} (\mathbb{1} - R^{-1} L^{-1} P A)^n) R^{-1} L^{-1} P b\]
   #+end_lemma
   #+begin_proof latex
   per Induktion über $k$: \\
   $k = 0$ \checkmark \\
   $k \geq 0$:
   \begin{align*}
   δ x^k&= R^{-1} L^{-1}(b - A x^k) \\
   x^{k + 1} &= x^k + δ x^k = (\mathbb{1} - R^{-1} L^{-1} P A) x^k + R^{-1} L^{-1} P b \\
   &= (\sum_{n = 0}^{k}(\mathbb{1} - R^{-1} L^{-1} P A)^{n + 1}) R^{-1} L^{-1} P b + R^{-1} L^{-1} P b \\
   &= (\sum_{n = 0}^{k + 1}(\mathbb{1} - R^{-1} L^{-1} P A)^{n}) R^{-1} L^{-1} P b
   \end{align*}
   #+end_proof
   Ist $\norm{\mathbb{1} - R^{-1} L^{-1} P A} < 1$, so gilt
   \begin{align*}
   \sum_{n = 0}^{∞}(\mathbb{1} - R^{-1} L^{-1} P A)^n &= (\mathbb{1} - \mathbb{1} + R^{-1} L^{-1} P A)^{-1} \\
   &= (PA)^{-1} L R
   \end{align*}
   $⇒ x^k$ konvergiert gegen
   \[x^{\ast} = (PA)^{-1} L R R^{-1} L^{-1} P b = A^{-1} b\]
   Wichtig: In der Praxis muss der Defekt $d^k$ mit höherer Genauigkeit berechnet werden.
   #+begin_ex latex
   Skript.
   #+end_ex
** Determinantenbestimmung
   \[A, B ∈ R^{n × n} ⇒ \det (A · B) = \det A \det B\]
   $A = P^T L R$
   \[\det A = \underbrace{\det (P^T)}_{\pm 1} \underbrace{L}_{1} \underbrace{\det R}_{\prod_{i = 1}^n r_{ii}} = \pm \prod_{i = 1}^n r_{ii}'\]
   Bei $k$ Vertauschungen von Zeilen ist das Vorzeichen $(-1)^k$.
** Rangbestimmung
   $\to$ Totalpivotierung $P A Q^T= LR$
   Gilt nach dem $i$ -ten Eliminationsschritt
   \[a_{k,j}^{(i)} = 0 ∀ j, k= i + 1, \dots, n\]
   so ist $\Rang(A) = i$ (Geht auch bei nicht quadratischen Matrizen, einfach mit Nullen auffüllen)
** Spezielle Gleichungssysteme
*** Bandmatrizen
	Eine Matrix $A ∈ ℝ^{n × n}$ heißt Bandmatrix vom Bandtyp $(m_l, m_r) ∈ \{0, \dots, n - 1\}^2$, wenn gilt
	\[a_{jk} = 0 ∀ k < j - m_l ∨ k > j + m_r, j, k = 1, \dots, n\]
	Die Größe $m = m_l + m_r$ heißt Bandbreite.
	| Typ          | Name                  |
	|--------------+-----------------------|
	| $(0, 0)$     | Diagonalmatrix        |
	| $(1, 1)$     | Tridiagonalmatrizen   |
	| $(n - 1, 0)$ | Untere Dreiecksmatrix |
	| $(0, n - 1)$ | Obere Dreiecksmatrix  |
	#+ATTR_LATEX: :options [Bandmatrix]
	#+begin_thm latex
	Ist $A ∈ ℝ^{n × n}$ eine Bandmatrix vom Typ $(m_l, m_r)$, Für die Gauß-Elimination $A = LR$ ohne Zeilenvertauschung durchführbar ist,
	dann sind alle reduzierten Matrizen $A^{(i)}$ desselben Typs und $L$ beziehungsweise $R$ sind vom Typ $(m_l, 0)$ beziehungsweise
	$(0, m_r)$. Aufwand:
	\[N = \frac{1}{3} n m_l m_r + \mathcal{O}(n(m_l + m_r))\]
	#+end_thm
	(Ohne Beweis)
	#+begin_ex latex
	Typ $(1, 1)$:
	\[A = \begin{pmatrix}a_1 & b_1 &   &   \\ c_2 & \ddots & \ddots &   \\ & \ddots & \ddots & b_{n - 1} \\   &   & c_n & a_n\end{pmatrix} = L R, L = \begin{pmatrix}1 &   &   &   \\ γ_1 & \ddots &   &   \\   & \ddots & \ddots &   \\  &   & γ_n & 1\end{pmatrix}, R = \begin{pmatrix}α_1 & β_1 &   &   \\   & \ddots & \ddots &   \\   &   & \ddots & β_{n - 1} \\   &   &   & α_n\end{pmatrix}\]
	Rekursive Bestimmung
	\begin{align*}
	α_1 &= a_1, β_1 = b_1 \\
	γ_i &= c_i / α_{i - 1}, α_i = a_i - γ_i β_{i - 1}, β_i = b_i \\
	γ_n = c_n / α_{n - 1}, α_n = a_n - γ_n β_{n - 1}
	\end{align*}
	Aufwand: $3n - 2$ Speicher,$2n - 2$ arithmetische Operationen. \\
	Vorsicht: (Beispiel Typ $(4, 4)$): Band "füllt auf"
	#+end_ex
*** Diagonaldominante Matrizen
	#+begin_defn latex
	$A ∈ ℝ^{n × n}$ heißt diagonaldominant, wenn
	\[\sum_{\substack{k = 1\\ k \neq j}}^{n} \abs{a_{jk}} \leq \abs{a_{jj}}, \quad j = 1, \dots, n\]
	#+end_defn
	#+begin_thm latex
	$A ∈ ℝ^{n × n}$ regulär und diagonaldominant $⇒ A = LR$ kann mit Gauß-Elimination ohne Zeilenvertauschungen berechnet werden. (Beweis: Skript)
	#+end_thm
*** Positiv definite Matrizen
	#+begin_defn latex
	$A ∈ R^{n × n}$ mit $A^T = A$ heißt positiv definit, wann
	\[x^T A x > 0 ∀ x ∈ ℝ^n \setminus \{0\}\]
	#+end_defn
	#+begin_thm latex
	$A ∈ ℝ^{n × n}$ symmetrisch positiv definit $⇒ A = LR$ kann mit Gauß-Elimination ohne Zeilenvertauschung berechnet werden mit Pivots $a_{ii}^{(i)} > 0$
	#+end_thm
	#+begin_proof latex
	\[0 < e_1^T A e_1 = a_{11}\]
	\begin{align*}
	a_{jk}^{(1)} &= a_{jk} - \frac{a_{j1}}{a_{11}} a_{1k} = a_{kj} - \frac{a_{k1}}{a_{11}} a_{1j} = a_{kj}^{(1)} \\
	⇒ A^{(1)} &= (a_{jk}^{(1)})_{j,k = 2}^n \text{ ist symmetrisch}
	\end{align*}
	Ist $A^{(1)}$ positiv definit, so beweist Induktion die Behauptung. Setze dafür $\tilde x = (x_2, \dots, x_n)^T ∈ ℝ^{n - 1}, x ∈ ℝ^n$, sodass
	\[x_1 = -\frac{1}{a_{11}} \sum_{k = 2}^{n} a_{1k} x_k\]
	\begin{align*}
	⇒ 0 < x^T A x &= \sum_{j,k = 1}^{n} a_{jk} x_j x_k \\
	&= \sum_{j,k= 2}^{n} a_{jk} x_j x^k + 2 x_1 \sum_{k = 2}^{n} a_{1k} x_k + a_{11} x_1^2 - \underbrace{\frac{1}{a_{11}} \sum_{j,k = 2}^{n} a_{k1} a_{j1} x_k x_j + \frac{1}{a_{11}}(\sum_{k = 2}^{n} a_{1k} x_k)^2}_{0} \\
	&= \underbrace{\sum_{j,k = 2}^{n} (a_{jk} - \frac{a_{k1} a_{j1}}{a_{11}}) x_k x_j}_{= a_{jk}^{(1)}} + a_{11}\underbrace{(x_1 + \sum_{1}^{a_{11}} \sum_{k= 2}^{n} a_{1k} x_k)}_{0}^2 \\
	&= \tilde x^T A^{(1)} \tilde x.
	\end{align*}
	#+end_proof
	$\to A = LR, r_{ii} = a_{ii}^{(i)} > 0$
	\[A = A^T = (L R)^T = (LD \underbrace{D^{-1} R}_{=: R})^T = \tilde R^T D L^T\]
	mit $A = \diag(r_1,\dots, r_{nn})$ und
	\[\tilde R= \begin{pmatrix}1 & r_{12} / r_{11} & \dots & r_{1n}/r_{11} \\ \ddots &   &   & \vdots \\   & \ddots &   & r_{n - 1, n} / r_{n - 1, n - 1} \\   &   &   & 1\end{pmatrix}\]
	Eindeutigkeit der LR-Zerlegung
	\[LR = \tilde R^T D L^T ⇒ L = \tilde R^T, R = D L^T\]
	#+begin_thm latex
	Jede symmetrisch positiv definite Matrix $A ∈ ℝ^{n × n}$ hat eine sogenannte Cholesky-Zerlegung
	\[A = L D L^T = \tilde L \tilde L^T\]
	#+end_thm
	Aufwand: $N_{\text{Cholesky}}(n) = \frac{n^3}{6} + \mathcal{O}(n^2)$. \\
	Algorithmus von Cholesky:
	\[\begin{pmatrix}\tilde l_{11} &   &   \\ \vdots & \ddots &   \\ \tilde l_{n1} & \dots & \tilde l_{nn}\end{pmatrix} \begin{pmatrix}\tilde l_{11} & \dots & \tilde l_{n1} \\ \ddots &   & \vdots \\   &   & \tilde l_{nn}\end{pmatrix} = \begin{pmatrix}a_{11} & \dots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{n1} & \dots & a_{nn}\end{pmatrix}\]
	\[i \geq j: a_{ij} = \sum_{k = 1}^{j} \tilde l_{ik} \tilde l_{jk} = \sum_{k = 1}^{j - 1} \tilde l_{ik} \tilde l_{jk} + \tilde l_{ij} \tilde l_{jj}\]
	für $i = 1,\dots, n$:
	\[\tilde l_{ii} = \sqrt{a_{ii} - \sum_{k = 1}^{i - 1}\tilde l_{ik}^2}\]
	Für $j = i + 1, \dots, n$:
	\[\tilde l_{ij} = \frac{1}{\tilde l_{ii}}(a_{ij} - \sum_{k = 1}^{i -1 }\tilde l_{ik} \tilde l_{jk})\]
	Wiederholung: Spezielle Matrizen, LR-Zerlegung
	- Bandmatrizen: Nullen nicht speichern / berechnen
	- Diagonal-dominante Matrizen: keine Pivotierung notwendig
	- Symmetrisch, positiv definite Matrizen
	  keine Pivotierung notwendig
	  \[A = \tilde L \tilde L^T = L D L^T\]
	  (billiger als $A = LR$)
** Nicht reguläre Systeme
   Wir betrachten $A ∈ ℝ^{m × n}$ (nicht notwendig quadratisch).
   Das Lineare Gleichungssystem $A x = b$ hat
   - keine Lösung, wenn $b \not ∈ \im(A)$
   - unendlich viele Lösungen $\bar x + Δ x$ wenn $A \bar x = b, Δ x ∈ \ker (A) \neq \{0\}$
   Verallgemeinerter Lösungsbegriff:
   Finde $\bar x ∈ ℝ^n$ mit minimalem Defekt $d = b - A \bar x$ (Für $d = 0$ löst $\bar x \quad A x = b$)
   #+ATTR_LATEX: :options [Least-Squares-Lösung]
   #+begin_thm latex
   Es gibt immer eine "Lösung" $\bar x ∈ ℝ^n$ mit kleinsten Fehlerquadraten:
   \[\norm{A \bar x - b}_2 = \min_{x ∈ ℝ^n} \norm{A x - b}_2\]
   Das gilt genau dann, wenn
   \[A^T A \bar x = A^T b\]
   (Normalgleichung). Für $\Rang(A) = n$ ist $\bar x$ eindeutig bestimmt. Ansonsten hat jede weitere Lösung die Form
   $\bar x + y$ mit $y ∈ \ker (A)$
   #+end_thm
   #+begin_lemma latex
   Sei $A ∈ \mathbb{K}^{m × n}$. Dann ist $\bar A^T A$ hermitesch positiv semidefinit.
   Ist $\Rang(A) = n$, so ist $\bar A^T A$ positiv definit.
   #+end_lemma
   #+begin_proof latex
   1. $\overline{\bar A^T A}^T= (A^T \bar A)^T = \bar A^T A$
   2. $\bar x^T \bar A^T A x = \overline{(Ax)}^T (A x) = \norm{A x}_2^2\geq 0$
   3. $\Rang(A) = n ⇒ m \geq n$ und $A: ℝ^n \to ℝ^m$ injektiv
	  $⇒$ Aus $\norm{A}_2 = 0 ⇒ A x = 0 ⇒ x = 0$  \\
	  $⇒$ $\bar x^T \bar A^T A x > 0 ∀ x ∈ \mathbb{K}^{n} \setminus \{0\}$
   #+end_proof
   #+begin_proof latex
   1. Es gelte $A^T A \bar x = A^T b$
	  \begin{align*}
	  ⇒ A^T (A \bar x - b) &= 0 \\
	  ⇒ \norm{b - A x}_2^2 &= \norm{b - A \bar x + A(\bar x - x)}_2^2 \\
	  &= \norm{b - A \bar x}_2^2 + 2(b - A \bar x, A(\bar x - x))_2 + \norm{A(\bar x - x)}_2^2 \\
	  \string(A(\bar x - b), b - A \bar x\string)_2 &= (\bar x - x)^T A^T(b - A\bar x) 0 \\
	  ⇒ \norm{b - Ax}_2^2 > \norm{b - A \bar x}_2^2 + \norm{A(\bar x - x)}_2^2
      \end{align*}
	  $⇒$ $\bar x$ ist minimal. Umgekehrt: Sei $\bar x$ minimal
	  \begin{align*}
	  0 &= \pp{}{x_i}\norm{A x - b}_2^2 \big|_{x = \bar x} \\
	  &= \pp{}{x_i}(\sum_{j = 1}^{m}(\sum_{k = 1}^{n} a_{jk} x_k - b_j)^2)\Big|_{x = \bar x} \\
	  &= \sum_{j = 1}^{m} 2(\sum_{k = 1}^{n} a_{jk} \bar x_k - b_j) \\
      a_{ji} &= (2 A^T(A \bar x - b))_i \\
	  ⇒ A^T A \bar x = A^T b
      \end{align*}
   2. Lösbarkeit: Wegen $\im (A)^{\perp} = \ker (A^T)$ hat $b$ eine eindeutige Zerlegung $b = r + s, r ∈ \ker (A^T), s ∈ \im(A)$
	  Sei $\bar x ∈ ℝ^n$ so, dass $A \bar x = s ⇒ A^T A \bar x = A^T s + A^T r = A^T b$
   3. $\Rang(A) = n$: $A^T A$ positiv definit $⇒ \bar x$ eindeutig. $Rang(A)< n: A^T A x_1 = A^T b$. Wegen
	  \[b= A x_1 + (b - A_{x_1}) ∈ \im A + \ker A^T\]
	  und Eindeutigkeit von $b = r + s$ gilt $A \bar x = A x_1 ∀ \bar x - x_1 ∈ \ker A$
   #+end_proof
   Numerische Lösung: Cholesky für Normalgleichung. Vorsicht:
   Im Fall $\Rang(A) = n = m$ gilt
   \[\cond_2(A^T A) = \cond_2(A)^2\]
   Merke: Normalgleichungen sind häufig schlecht konditioniert. Abhilfe: QR-Zerlegung von $A$
   #+begin_thm latex
   Sei $A ∈ \mathbb{K}^{m × n}$ mit $\Rang(A) = n \leq m$. Dann existiert eine eindeutig bestimmte Matrix $Q ∈ \mathbb{K}^{m × n}$ mit $\bar Q^T Q = E_n$ und
   eine eindeutig bestimmte obere Dreiecksmatrix $R ∈ \mathbb{K}^{n × n}$ mit reellen Diagonalelementen $r_{ii} > 0, i = 1, \dots, n$, sodass
   \[A = Q R\]
   Bezeichnung: $Q$: orthonormale Matrix ($m = n$: unitär)
   #+end_thm
   #+begin_proof latex
   Konstruktion der Spalten $q_k$ von $Q$ mittels Gram-Schmidt aus den Spalten $a_k$ von $A$
   \[q_i = \begin{cases} q_i = \norm{a_1}_2^{-1} a_1 & i = 1 \\ q_i = \norm{\tilde q_i}_2^{-1} \tilde q_i, \tilde q_i = a_i - \sum_{k = 1}^{i - 1}(a_i, q_k)_2 q_k & i = 2, \dots, n \end{cases}\]
   Wegen $\Rang(A) = n$ sind die $a_k$ linear unabhängig und $\norm{\tilde q_k}_2 \neq 0, k = 1, \dots, n$. Betrachte:
   \begin{align*}
   a_k &= \tilde q_k + \sum_{i = 1}^{k - 1}(a_k, q_i)_2 q_i \\
   &= \norm{\tilde q_k}_2 q_k + \sum_{i = 1}^{k - 1}(a_k, q_i) q_i \\
   &= \sum_{i = 1}^{k} r_{ik} q_i
   \end{align*}
   $r_{kk} = \norm{\tilde q_k}_2 ∈ ℝ_+, r_{ik} = (a_k, q_i)_2$. Setze $r_{ik} = 0, i > k, R = (r_{ik}) ∈ \mathbb{K}^{n × n} ⇒ A = Q R$. \\
   Eindeutigkeit: Sei $Q_1 R_1 = A = Q_2 R_2$. Setze
   \begin{align*}
   Q &= \bar Q_2^T Q_1 = \bar Q_2^T A R_1^{-1} = R_2 R_1^{-1} \tag{obere Dreiecksmatrix} \\
   \bar Q^T &= \bar Q_1^T Q_2 = \bar Q_1^T A R_2^{-1} = R_1 R_2^{-1} \tag{obere Dreiecksmatrix} \\
   \bar Q^T Q &= R_1 R_2^{-1} R_2 R_1^{-1} = \mathbb{I}
   \end{align*}
   $Q$ ist orthonormal und diagonal. Ihre Eigenwerte $λ_i$ erfüllen $\abs{λ_i} = 1$
   \begin{align*}
   Q R_1 &= R_2 R_1^{-1} R_1 = R_2 ⇒ λ_i \underbrace{(R_1)_{ii}}_{> 0} = (R_2)_{ii} > 0 \\
   ⇒ λ_i ∈ ℝ, λ_i &= 1 \\
   Q &= E_n ⇒ R_1 = R_2, Q_1 = A R^{-1}_1 = A R_{2}^{-1} = Q_2
   \end{align*}
   #+end_proof
   Least-Squares-Lösung mit
   \[A = Q_1 R = (Q_1 \mid Q_2)\cvec{R; 0}\]
   mit $Q = (Q_1 \mid Q_2) ∈ ℝ^{m × n}, R = \cvec{R; 0} ∈ ℝ^{m × n}, \Rang(A) = n$
   - $\norm{Q v}_2^2 = v^T Q^T Q v = \norm{v}^2_2$
   - $\norm{A x - b}_2^2 = \norm{Q \tilde R x - Q Q^T b}_2^2 = \norm{Q(\tilde R x - Q^T b)}_2^2$
	 \begin{align*}
	 = \norm{\tilde R x - Q^T b}_2^2 &= \norm{\vec{R; 0}x - \cvec{Q_1^T; Q_2^T}b}_2^2 \\
	 &= \norm{R x - Q_1^T b}_2^2 + \norm{Q_2^T b}_2^2
     \end{align*}
	 minimal für $x = R^{-1} Q_1^T b$.
   - $A^T A = (Q_1 R)^T Q_1 R = R^T Q_1^T Q_1 R = R^T R$ (Cholesky-Zerlegung) \\
	 $A^T A x = A^T b = R^T R x = R^T Q_1^T b$
   - Lösung mit $R$ ist besser konditioniert als Lösung mit $R^T R: \cond_2 (R^T R) = \cond_2(R)^2$
   Wiederholung: $A ∈ M(n × m, \mathbb{K})$
   - $A = Q R = \tilde Q \tilde R$, $\tilde Q = Q|\tilde Q_2$, $\bar Q^T Q = E_n, \bar{\tilde Q}^T \tilde Q = E_m$
   - Eindeutigkeit mit $r_{ij} > 0$
   - $\norm{Ax - b}_2^2 = \norm{\tilde Q \tilde R x - \tilde Q \bar{\tilde Q}^T b}_2^2 = \norm{\tilde Q(\tilde R x - \tilde Q^T b)}_2^2 = \norm{R x - Q^T b}_2^2 + \norm{\tilde Q_2 b}$.
	 $\Rang(A) = n$: $x = \tilde R^{-1} Q^T b$
   - verhindert schlechte Konditionierung der Normalgleichung
	 \[\cond_2(A^T A) = \cond_2(A)^2 = \cond_2(R)^2\]
   Problem: Gram-Schmidt zur Berechnung von Orthogonalbasis ist nicht stabil.
   Stabile Variante: Householder-Verfahren
   #+begin_defn latex
   Für $v ∈ \mathbb{K}$ mit $\norm{v}_2 = 1$ heißt
   \[I = E_n - 2 v \bar v^T ∈ \mathbb{K}^{n × n}\]
   "Householder-Transformation".
   #+end_defn
   \[v \bar v^T = \begin{pmatrix}v_1 \bar v_1 & \dots & v_1 \bar v_n \\ \vdots & \ddots & \vdots \\ v_n \bar v_1 & \dots & v_n \bar v_n\end{pmatrix}\]
   Eigenschaften von $S$:
   - $\bar S^T = S$ hermitesch
   - $\bar S^T S = (E_n - 2 v \bar v^T)(E_n - 2 v \bar v^T) = E_n - 4 v \bar v^T + 4 v \underbrace{\bar v^T v}_{1} \bar v^T = E_n$ (unitär)
   - Spiegelung: Sei $u ∈ \mathbb{K}^n$. Zerlege $u = (v, u)_2 v + [u - (v, u)_2 v] = u_1 + u_2$
	 \begin{align*}
	 S u_1 &= (E_n - 2 v \bar v^T)(v, u)_2 v \\
	 &= (v, u)_2(v - 2v \bar v^T v) = - u_1 \\
	 S U_2 &= (E_n - 2 v \bar v^T)(u - u_1) \\
	 &= u - 2(v, u)_2 v + u_1 = u - u_1 = u_2
     \end{align*}
     $v$: Normale der Spiegelungshyperebene
   Householder-Verfahren:
   \[A = A^{(0)} \to \dots \to A^{(i - 1)} \to \dots \to A^{(n)} = \tilde R\]
   mit
   \begin{equation*}
   A^{(i)} = \begin{pmatrix}
   a_{11}^{(i)} & \dots & \dots & \dots & \dots & a_{1n}^{(i)} \\
   & \ddots & & & & \vdots \\
   & & a_{ii}^{(i)} & \dots & \dots & a_{in}^{(i)} \\
   & 0 & \vdots & & & \vdots \\
   & & a_{im}^{(i)} & & & a_{nm}^{(i)} \\
   \end{pmatrix}
   \end{equation*}
   Schritt $i$: Householder-Transformation
   \[S_i A^{(i - 1)} = A^{(i)}\]
   \begin{align*}
   ⇒ \tilde R &= A^{(n)} = S_n S_{n - 1} \dots S_1 A = \bar{\tilde Q}^T A \\
   ⇒ \tilde Q \tilde R &= A, \tilde Q = \bar S_1^T \dots \bar S_n^T = S_1 \dots S_n
   \end{align*}
   Achtung: $\tilde r_{ii} > 0$ wird nicht garantiert. (keine Eindeutigkeit).
   Bezeichnung: $\tilde A^{(i)}= (\tilde a_i^{(i)} \mid \dots \mid \tilde a_n^{(i)})$
   Setze
   \[v_i = \cvec{0; \vdots; 0; \tilde v_i}, \tilde v_i ∈ \mathbb{K}^{m - i}\]
   \[⇒ S_i = E_m - 2 v_i \bar v_i^T = \begin{pmatrix}E_{i - 1} & 0 \\ 0 & E_{m - i} - 2 \tilde v_i \bar{\tilde v_i}^T \end{pmatrix} = \begin{pmatrix}\mathbb{1} & 0 \\ 0 & \tilde S_i\end{pmatrix}\]
   $⇒$ Die ersten $i - 1$ Zeilen von $A^{(i - 1)}$ bleiben unverändert. Wähle $\tilde v_i$ so, dass
   \[\tilde S_i \tilde s_i^{(i)} ∈ \Lin \{e_1^i\}, e_1^i = \cvec{1; 0; \vdots; 0} ∈ ℝ^{m - i}\]
   2 Möglichkeiten:
   \begin{align*}
   \tilde v_i &= \frac{\tilde a_i^{(i)} - \norm{\tilde a_i^{(i)}}_2 e_1}{\norm{\tilde a_i^{(i)} - \norm{\tilde a_i^{(i)}}_2 e_1}_2} \\
   \tilde v_i &= \frac{\tilde a_i^{(i)} + \norm{\tilde a_i^{(i)}}_2 e_1}{\norm{\tilde a_i^{(i)} + \norm{\tilde a_i^{(i)}}_2 e_1}_2} \\
   \end{align*}
   Zur Vermeidung von Auslöschung:
   \[\tilde v_i = \frac{\tilde a_i^{(i)} + \sgn(\tilde a_{ii}^{(i)}) \norm{\tilde a_i^{(i)}}_2 e_1}{\norm{\tilde a_i^{(i)} + \sgn(\tilde a_{ii}^{(i)}) \norm{\tilde a_i^{(i)}}_2 e_1}_2}\]
   \[⇒ \tilde S_i \tilde A^{(i)} = \begin{array}{c|c}\pm \norm{\tilde a_i^{(i)}}_2 &   \\ 0 & \tilde a_i^{(i)} - 2(\tilde a_{i + j, i}^{(i)}, \tilde v_i) \tilde v_i \\ \vdots & \\ 0 &  \end{array}, j = 2, \dots, m - i\]
   Insgesamt ergibt sich für die Spalten von $A^{(i)} = S_i A^{(i - 1)}$
   \begin{align*}
   a_k^{(i)} &= a_k^{(i - 1)}, k = 1, \dots, i - 1 \\
   a_i^{(i)} &= (a_{i,1}^{(i - 1)}, \dots, a_{i - 1, i}^{(i - 1)}, \norm{\tilde a_i^{(i - 1)}}_2, 0, \dots, 2)^T \\
   a_k^{(i)} &= a_k^{(i - 1)} - 2 (\tilde a_k^{(i - 1)}, \tilde v_i) v_i, k = i + 1, \dots, n
   \end{align*}
** Singulärwertzerlegung
   #+begin_thm latex
   Es sei $A ∈ ℝ^{m × n}$. Dann existieren orthogonale Matrizen $V ∈ ℝ^{n × n}$ und $U ∈ ℝ^{m × m}$, sodass
   \[U^T A V = Σ = \diag(σ_1, \dots, σ_p) ∈ ℝ^{m × n}, p = \min(m, n)\]
   mit $σ_1 \geq \dots \geq σ_p \geq 0$
   #+end_thm
   #+begin_proof latex
   Skript,
   #+end_proof
   $A = U Σ V^T$. Nützliche Folgerungen: ($σ_1 \geq \dots \geq σ_r > σ_{r + 1} = \dots = σ_p = 0$)
   - $\Rang(A) = r$
   - $\ker A = \Lin \{v_{r + 1}, \dots, v_n\}$
   - $\im A = \Lin \{u_1, \dots, u_r\}$
   - $A = U Σ V^T = \sum_{i = 1}^{r} σ_1 u_i v_i^T$
   - $\norm{A}_2 = σ_1$
   - $\cond_2(A) = \frac{σ_1}{σ_p}$
   - $\norm{A}_F = \sqrt{\sum_{i = 1}^{r} σ_i^2} = \norm{\cvec{σ_1; \vdots; σ_p}}_2$
   - $\norm{A - \sum_{i = 1}^{k} σ_i u_i v_i^T}_2 = \norm{\sum_{i = k + 1}^{r} σ_i u_i v_i^T}_2 = σ_{k + 1}$
