#+AUTHOR: Robin Heinemann
#+TITLE: Einführung in die Numerik (Potschka)

#+INCLUDE: "../header.org" :minlevel 1
#+LATEX_HEADER:	\setcounter{section}{-1}

# #+BEGIN_SRC	python
# def fib(n):
#     if n < 2:
#         return 1
#     else:
#         return fib(n - 1) + fib(n - 2)

# for n in range(10):
#     print(fib(n))
# #+END_SRC
# Andreas Potschka: INF 205, Raum 2.418
# Keine Vorlesung an Feiertagen
# - Do 25.05 (Himmelfahrt)
# - Do 15.06 (Fronleichnam)
# Webseite: [[http:]]//goo.gl/dzaGPd
# Klausurtermin: 27.07.2017 14-16 Uhr
# Klausurtermin: 21.09.2017 ? Uhr
# Zulassung: 50% der Punkte der Übungsaufgaben, einmal vorrechnen
# Übungsblatt Donnerstag, Beginn der Übungsgruppen 24.04, Abgabe im Mathematikon

# #+begin_src python :results	file :exports both
# import matplotlib, numpy
# matplotlib.use('Agg')
# import matplotlib.pyplot as plt
# fig = plt.figure(figsize=(4,2))
# x=numpy.linspace(-15,15)
# plt.plot(numpy.sin(x)/x)
# fig.tight_layout()
# plt.savefig('/tmp/test.pgf')
# #+end_src

# #+RESULTS:
# [[file:/tmp/test.png]]

#+INCLUDE: "introduction.org" :minlevel 1
#+INCLUDE: "error_analysis.org" :minlevel 1
* Interpolation und Approximation
  Grundproblem: \\
  Darstellung und Auswertung von Funktionen. \\
  Aufgabenstellung:
  1. Eine Funktion $f(x)$ ist nur auf einer diskreten Menge von Argumenten $x_0, \dots, x_n$ bekannt und soll rekonstruiert werden
	 (zum Beispiel für Graph Ausgabe)
  2. Eine analytisch gegebene Funktion $f(x)$ soll auf dem Rechner so dargestellt werden, dass jederzeit Funktionswerte
	 zu beliebigen Argument $x$ berechnet werden können.
  $\to$ System mit unendlich vielen Freiheitsgraden $y = f(x)$. "Simulation" durch endlich viele Datensätze in Klassen $P$ von einfach strukturierten Funktionen
  - Polynome: $p(x) = a_0 + a_1 x + \dots + a_n x^n$
  - rationale Funktionen:
	\[r(x) = \frac{a_0 + a_1 x + \dots + a_n x^n}{b_0 + b_1 x + \dots + b_m x^m}\]
  - trigonometrische Funktionen
	\[t(x) = \frac{1}{2} a_0 + \sum_{k = 1}^{n} (a_k \cos(k x) + b_k \sin(k x))\]
  - Exponentialsummen
	\[e(x) = \sum_{k = 1}^{n} a_k \exp(b_k x)\]
  #+begin_defn latex
  Geschieht die Zuordnung eines Elementes $g ∈ P$ zur Funktion $f$ durch Fixieren von Funktionswerten
  \[g(x_i) = y_i = f(x_i), i = 0, \dots, n\]
  so spricht man von *Interpolation*.
  Ist $g$ im gewissen Sinne die beste Darstellung von $f$, zum Beispiel: \\
  $\max_{a \leq x \leq b} \abs{f(x) - g(x)}$ minimal für $g ∈ P$, oder \\
  $(∫_a^b \abs{f(x) - g(x)}^2 \d x)^{1/2}$ minimal für $g ∈ P$
  so spricht man von *Approximation*. Die Wahl der Konstruktion von $g ∈ P$ hängt von der zu erfüllenden Aufgabe ab.
  Offenbar ist die Interpolation eine Approximation mit
  \[\max_{i = 0, \dots, n} \abs{f(x_i) - g(x_i)}\]
  für $g ∈ P$
  #+end_defn
  Wiederholung: Interpolation und Approximation
  - Stützstellen $x_i$ mit Werten $y_i, i = 0, \dots, n$
  -	Klassen $P$ von Funktion
  *Polynominterpolation* \\
  Wir bezeichnen mit $P_n$ den Vektorraum der Polynome vom Grad $\leq n$:
  \[P_n = \{p(x) = a_0 + a_1 x + \dots + a_n x^n \mid a_i ∈ ℝ, i = 0, \dots, n\}\]
  #+ATTR_LATEX: :options [Lagrangasche Interpolationsaufgabe]
  #+begin_defn latex
  Die Lagrangsche Interpolationsaufgabe besteht darin zu $x + 1$ paarweise verschiedenen Stützstellen (auch Knoten genannt) $x_0, \dots, x_n ∈ ℝ$ und
  gegebenen Knotenwerten $y_0, \dots, y_n ∈ ℝ$ ein Polynom $p ∈ P_n$ zu bestimmen mit der Eigenschaft $p(x_i) = y_i$
  #+end_defn
  #+begin_thm latex
  Die Lagrangsche Interpolationsaufgabe ist eindeutig lösbar.
  #+end_thm
  #+begin_proof latex
  *Eindeutigkeit*: Sind $p_1, p_2 ∈ P_n$ Lösungen, so gilt für $p = p_1 - p_2$, dass
  \[p(x_i) = p_1(x_i) - p_2(x_i) = y_i - y_i = 0, i = 0, \dots, n\]
  Also hat $p$ $n + 1$ Nullstellen und ist folglich identisch Null. $⇒ p_1 = p_2$ \\
  *Existenz:* Wir betrachten die Gleichungen
  \[p(x_i) = y_i \qquad i = 0, \dots, n\]
  Dies ist ein lineares Gleichungssystem mit $n + 1$ Gleichungen und $n + 1$ Freiheitsgraden.
  \[\begin{pmatrix}x_0^0 & x_0^1 & \dots & x_0^n \\ x_1^0 & x_1^1 &   & x_1^n \\ \vdots &   & \ddots & \vdots \\ x_n^0 & x_n^1 & \dots & x_n^n\end{pmatrix} \cvec{a_0; a_1; \vdots; a_n} = \cvec{y_0; y_1; \vdots; y_n}\]
  Wegen der Eindeutigkeit von $p$ ist $\ker V = \{0\}$. Mit dem Rangsatz ($\dim ℝ^{n + 1} = \dim \ker V + \dim \im V$) liefert $V$ eine surjektive Abbildung.
  Damit existiert eine Lösung.
  #+end_proof
  Zur Konstruktion des Interpolationspolynoms $p ∈ P_n$ verwenden wir die sogenannten Lagrangschen Basispolynome.
  \[L_i^{(n)}(x) = \prod_{\substack{j = 0\\ j \neq i}}^n \frac{x - x_j}{x_i - x_j} ∈ P_n, i = 0, \dots, n\]
  #+begin_lemma latex
  $\{L_i^{(n)}, i = 0, \dots, n\}$ ist eine Basis von $P_n$
  #+end_lemma
  #+begin_proof latex
  Übung.
  #+end_proof
  Offensichtlich gilt:
  \[L_i^{(n)}(x_k) = δ_{ij} = \begin{cases} 1 & i = j \\ 0 & i \neq j\end{cases}\]
  #+begin_defn latex
  Das Polynom
  \[p(x) = \sum_{i = 0}^{n} y_i L_i^{(n)}(x) ∈ P_n\]
  hat die gewünschten Eigenschaften
  \[p(x_j) = y_j, j = 0, \dots, n\]
  und wird die Lagrangsche Darstellung des (Lagrangschen) Interpolationspolynoms zu dem Stützpunkten $(x_i, y_i), i = 0, \dots, n$ genannt.
  #+end_defn
  Nachteil: Bei Hinzunahme eines weiteren Stützpunktes $(x_{n+1}, y_{n + 1})$ ändern sich die Basispolynome völlig. \\
  Abhilfe: Newtonsche Basispolynome
  \[N_0(x) = 1, N_i(x) = (x - x_{i - 1})N_{i - 1}(x) = \prod_{j = 0}^{i - 1}(x - x_j)\]
  Für den Ansatz
  \[p(x) = \sum_{i = 0}^{n} a_i N_i(x)\] erhält man durch Auswertung von $x_0, \dots, x_n$ das gestaffelte Gleichungssystem
  \begin{align*}
  y_0 &= p(x_0) = a_0 \\
  y_1 &= p(x_1) = a_0 + a_1 (x_1 - x_0) \\
  &\vdots \\
  y_0 &= p(x_0) = a_0 + a_1 (x_1 - x_0) + \dots + a_n(x_n - x_0) · \dots · (x_n - x_{n - 1}) \\
  \end{align*}
  aus dem sich die Koeffizienten $a_i$ rekursiv berechnen lassen. Bei Hinzunahme eines weiteren Stützpunktes $(x_{n + 1}, y_{n + 1})$
  setzt man den Prozess mit der Basisfunktion $N_{n + 1}$ fort. In der Praxis verwendet man folgende stabilere und effizientere Methode
  #+ATTR_LATEX: :options [Newtonsche Darstellung]
  #+begin_thm latex
  Das Lagrangsche Interpolationspolynom zu den Punkten $(x_0, y_0), \dots, (x_n, y_n)$ lässt sich bezüglich der Newtonschen Polynombasis
  schreiben in der Form
  \[p(x) = \sum_{i = 0}^{n} y[x_0, \dots, x_i]N_i(x)\]
  Dabei bezeichnen $y[x_0, \dots, x_i]$ die zu den Punkten $(x_i, y_i)$ gehörenden "dividierten Differenzen", welhce rekursiv definiert sind durch
  \begin{align*}
  \text{für } j = 0, \dots, n: &y[x_j] = y_j \\
  L \text{für } k = 1, \dots, j: i = k - j: y\underbrace{[x_i, \dots, x_{1 + k}]}_{k + 1} = \frac{y\underbrace{[x_{i + 1}, \dots, x_{1 + k}]}_{k} - y\underbrace{[x_i, \dots, x_{x_1 + k - 1}]}_{k}}{x_{i + k} - x_i}
  \end{align*}
  #+end_thm
  #+begin_proof latex
  Es bezeichne $p {i, i + k} ∈ P_k$ das Polynom, welches die Punkte $(x_i, y_i), \dots, (x_{i + k}, y_{i + k})$ interpoliert.
  Speziell ist $p_{0, n} = p$ das gesuchte Interpolationspolynom. Wir zeigen
  \[p_{i, i + k}(x) = y[x_i] + y[x_i, x_{i + 1}](x - x_i) + \dots + y[x_i, \dots, x_{i + k}](x - x_i) · \dots · (x - x_{i + k})\]
  was für $i = 0$ und $k = n$ den Satz beweist. Der Beweis wird durch Induktion über die Indexdifferenz $k$ geführt. Für $k = 0$ ist
  $p_{i,i} = y_i = y[x_i], i = 0, \dots, n$. Sei die Behauptung richtig für $k - 1 \geq 0$. Nach Konstruktion gilt für ein $a ∈ ℝ$
  \[p_{i, i + k}(x) = p_{i,i + k - 1}(x) + a(x - x_1) · \dots · (x - x_{i + k - 1}) = 0\]
  für $x = x_j, j = i, \dots, i + k - 1$. Zu zeigen: $a = y[x_i, \dots, x_{i + k}]$.
  Offenbar ist $a$ der Koeffizient von $x^k$ in $p_{0, i + k}$. Nach Induktionsannahme ist also
  \begin{align*}
  p_{i,i + k - 1}(x) &= \dots + y[x_i, \dots, x_{i + k - 1}]x^{k - 1} \\
  p_{i + 1,i + k - 1}(x) &= \underbrace{\dots}_{\mathclap{\text{Grad } \leq k - 2}} + y[x_{i + 1}, \dots, x_{i + k}]x^{k - 1} \\
  \end{align*}
  Ansatz:
  \begin{align*}
  q(x) &= \frac{(x - x_i)p_{i + 1, i + k}(x) - (x - x_{i + k})p_{i,i + k - 1}(x)}{x_{i + k} - x_i} \\
  &= p_{i,i + k - 1}(x) + \frac{(x - x_i)p_{i + 1, i + k}(x) - (x - x_{i + k} + x_{i + k} - x i)p_{i, i + k - 1}(x)}{x_{i + k} - x_i} \\
  &= p_{i,i + k - 1}(x) + (x - x_i)\frac{p_{i + 1, i + k}(x) - p_{i,i + k - 1}(x)}{x_{i + k} - x_i} \\
  \end{align*}
  Ex gilt:
  \[q(x_i) = y_i, q(x_{i + k}) = \frac{(x_{i + k} - x_i)y_{i + k} + 0}{x_{i + k} - x_i} = y_{1 + k}\]
  \[q(x_j) = \frac{(x_j - x_i)y_j - (x_j - x_{i + k})y_j}{x_{i + k} - x_i} = y_j, j = i +1 , \dots, i + k - 1\]
  $⇒ q$ interpoliert die Stützpunkte $(x_i, y_i), \dots, (x_{i + k}, y_{i + k}) ⇒ q \equiv p_{i, i + k}$ (Eindeutigkeit des Interpolationspolynoms).
  Der führende Koeffizient in $p_{i, i + k}(x)$ ist demnach
  \begin{align*}
  q &= \frac{y[x_{i + 1}, \dots, x_{i + k}] - y[x_i, \dots, x_{i + k - 1}]}{x_{i + k} - x_i} \\
  &= y[x_i, \dots, x_{i + k}]
  \end{align*}
  #+end_proof
  #+begin_korollar latex
  Sei $σ: \{0, \dots, n\} \to \{0, \dots, n\}$ eine
  beliebige Permutation. Dann gilt für die Stützpunkte $(\tilde x_i, \tilde y_i) = (x_{σ(j)}, y_{σ(j)})$
  \[y[\tilde x_0, \dots, \tilde x_n] = y[x_0, \dots, x_n]\]
  #+end_korollar
  #+begin_proof latex
  Koeffizient des Monoms $x^n$ ist $y[x_0, \dots, x_n]$ unabhängig von der Reihenfolge.
  #+end_proof
  Wiederholung: Lagrange-Interpolation: \\
  Gegeben: $(x_i, y_i), i = 0, \dots, n$ \\
  Suche $p ∈ P_n: p(x_i) = y_i, i = 0, \dots, n$ \\
  Lösung:
  \begin{align*}
  p(x) &= \sum_{i = 0}^{n} y_i L_i^{(n)}(x) \\
  &= L_i^{(n)}(x) &= \prod_{\substack{j = 0 \\ j \neq i}}^n \frac{x - x_j}{x_i - x_j} ∈ P_n
  \end{align*}
  $⇒ L_i^{(n)}(x_j) = δ_{ij}$ \\
  Andere Darstellung: Newton-Neville
  \begin{align*}
  N_i(x) &= \prod_{j = 0}^{n - 1}(x - x_j) \\
  p(x) &= \sum_{i = 0}^{N}y[x_0, \dots, x_i]D_i(x) \\
  y[x_i] &= q_i \\
  y[x_i, \dots, x_{i + k}] = \frac{y[x_{i + 1}, \dots, x_{i + k}] - y[x_{i}, \dots, x_{i + k - 1}]}{x_{i + k} - x_i}
  \end{align*}
  #+begin_defn latex
  Das durch die Rekursion $j = 0, \dots, n, p_{j,j}(x) = y_j$ für $k = 1, \dots, j: i = k - j$
  \[p_{i, i + k}(x) = p_{i, i + k - 1}(x) + (x - x_i) \frac{p_{i + 1, i + k}(x) - p_{i, i + k - 1}(x)}{x_{i + k} - x_{i}}\]
  erzeugte Polynom $p_{0, 1}$ ist die sogenannte Nevellsche Darstellung des Interpolationspolynom zu den Stützstellen $(x_0, y_0), \dots, (x_n, y_n)$
  #+end_defn
  Schema:
  #+begin_export latex
  \begin{equation*}
  \begin{matrix}
  & k = 0 & & k = 1	& & k = 2 & \dots & k = n - 1 & k = n \\
  x_0 & y_0 & \to & p_{0,1} & \to & p_{0, 2} & \dots & p_{0, n - 1} & \to & p_{0, n} \\
  x_1 & y_1 & \nearrow \to & p_{1, 2} & \nearrow\to & p_{1,3} & \dots & p_{1, n} & \nearrow & \\
  x_2 & y_2 & \nearrow & & & & & & & \\
  \vdots & & \vdots & \iddots & & & & & & \\
  x_{n - 1} & y_{n - 1} & \to & p_{n - 1, n} & & & & & & \\
  x_n & y_n & \nearrow & & & & & & &
  \end{matrix}
  \end{equation*}
  #+end_export
  Die Hinzunahme eines weiteren Stützpunktes ist problemlos.
  Die Auswertung von $p_{0, n}(x)$ an einer Stelle $ξ \neq x_i$ ohne vorige Bestimmung der Koeffizienten der Newton-Darstellung ist damit sehr
  einfach und numerisch effizient und stabil möglich. Dazu wird im Schema $x$ mit $ξ$ ersetzt. \\
** Auswertung von Polynomen und deren Ableitungen
   Sei $p ∈ P_n$ gegeben in der Darstellung
   \[p(x) = a_0 + a_1 x + \dots + a_n x^n\]
   Wiederhohlung: Auswertung von $p(ξ)$ mittels Horner-Schema
   \[b_k = \begin{cases} a_n & k = n \\ a_k + ξ b_{k + 1} & k = n - 1, \dots, 0 \end{cases}\]
   $⇒ p(ξ) = b_0$. \\
   Zu $p_n = p ∈ P_n$ und festem $ξ$ wird durch
   \[p_{n - 1}(x) = b_1 + b_2 x + \dots + b_n x^{n - 1}\]
   ein Polynom $p_{n - 1} ∈ P_{n - 1}$ definiert.
   Wegen $a_k = b_k - ξ b_{k + 1}, k = 0, \dots, n - 1, a_n = b_n$:
   \begin{align*}
   p_n(x) &= \sum_{k = 0}^{n} b_k x^k - ξ \sum_{k = 0}^{n - 1} b_{k + 1} x^k \\
   &= b_0 + x \sum_{k = 1}^{n}b_k x^{k - 1} - ξ \sum_{k = 1}^{n} b_k x^{k - 1} \\
   &= r_0 + (x - ξ)p_{n - 1}(x) \quad r_0 = p(ξ) = b_0
   \end{align*}
   $⇒$ Für eine Nullstelle $ξ$ von $p_n$ leistet das Horner-Schema die Abspaltung des Linearfaktors $(x - ξ)$ vom Polynom $p_n$.
   Weiter ist dann für $x \neq ξ$
   \begin{align*}
   \frac{p_n(x) - p_n(ξ)}{x - ξ} &= p_{n - 1}(x) \\
   \intertext{$x \to ξ$}
   p'_n(ξ) = p_{n - 1}(ξ)
   \end{align*}
   Zur Berechnung von $p'_n(ξ)$ wird das Horner-Schema auf $p_{n - 1}$ angewendet.
   \[p_{n - 2} ∈ P_{n - 2}, p_{n - 1}(x) = r_1 + (x - ξ)p_{n - 2}(x), r_1 = p_{n - 1}(ξ)\]
   Fortsetzen $\to$ endliche Folge von Polynomen $p_n, p_{n - 1}, \dots, p_0$ mit
   \begin{align*}
   p_{n - j}(x) &= (x - ξ) p_{n - j - 1}(x) + r_j, \quad j = 0, \dots, n \\
   p_n(x) = r_0 + r_1(x - ξ) + \dots + r_n(x - ξ)^n
   \end{align*}
   Vergleich mit der Taylorentwicklung von $p_n$ um $ξ$ ergibt
   \[r_j = \frac{1}{j!} p_{n}^{(j)}(ξ)\]
   Die Koeffizienten von $p_{n - j}$ seien
   \[p_{n - j}(x) = a_j^{(j)} + a_{j + 1}^{(j)} x + \dots + a_n^{(j)} x^{n - j}, j = 0, \dots, n\]
   Es gilt die Rekursion:
   \[a_k^{(j + 1)} = \begin{cases} a_n^{(j)} & k = n \\ a_k^{(j)} + ξa_{k + 1}^{(j + 1)}\end{cases}\]
   und es gilt
   \[p^{(j)}(ξ) = j! a_j^{j + 1}, j = 0, \dots, n\]
   Dieses "vollständige Horner-Schema" kann leicht zur Auswertung von Polynomen in Newton-Darstellung modifiziert werden:
   \[p(x) = a_0 + a_1(x - x_0) + \dots + a_n(x - x_0) · \dots · (x - x_{n - 1})\]
** Interpolation von Funktionen
   Stützstellen $x_0, \dots, x_n ∈ [a, b]$. Werte gegeben durch Funktion $y_i = f(x_i), i = 0, \dots, n$ \\
   *Frage:* Wie gut approximiert das Interpolationspolynom $p ∈ P_n$ die Funktion $f$ auf $[a, b]$? \\
   *Bezeichnungen:*
   - $\overline{(x_0, \dots, x_n)} =$ kleinstes Intervoll, das alle $x_i$ enthält.
   - $C[a, b]:$ Vektorraum der über $[a, b]$ stetigen Funktionen
   - $C^k[a, b]:$ Vektorraum über $[a, b]$ k-mal stetig differenzierbarer Funktionen.
   #+ATTR_LATEX: :options [Interpolationsfehler 1]
   #+begin_thm latex
   Sei $f ∈ C^{n + 1}[a, b]$. Dann gibt es zu jedem $x ∈ [a, b]$ ein $ξ_x ∈ \overline{(x_0, \dots, x_n, x)}$, sodas gilt
   \[f(x) - p(x) = \frac{f^{(n + 1)}(ξx)}{(n + 1)!} \prod_{j = 0}^n (x - x_j)\]
   #+end_thm
   #+begin_proof latex
   Für $x ∈ \{x_0, \dots, x_n\}$ ist alles klar. Sei $x ∈ [a, b] \setminus \{x_0, \dots, x_n\}$. Wir setzen
   \[l(t) = \prod_{j = 0}^n (t - x_j), \quad c(x) = \frac{f(x) - p(x)}{l(x)}\]
   Die Funktion
   \[F(t) = f(t) - p(t) - c(x) l(t)\]
   besitzt dann mindestens die $n + 2$ Nullstellen $x_0, \dots, x_n, x$ in $[a, b]$.
   Durch wiederhohlte Anwendung des Satzes von Rolle schließt man, dass die Ableitung $F^{n + 1}$ eine Nullstelle $ξ_x ∈ \overline{(x_0, \dots, x_n, x)}$. Es
   \begin{align*}
   0 &= F^{(n + 1)}(ξ_x) = f^{(n + 1)}(ξ) - p^{(n + 1)}(ξ) - c(x) l^{(n + 1)}(t) \\
   &= f^{(n + 1)}(ξ) - c(x)(n + 1)!
   \end{align*}
   #+end_proof
   Wiederholung:
   - Neville-Schema für $p ∈ P_n$:
     \[p(x_i) = y_i, i = 0, \dots, n\]
   - Vollständiges Horner-Schema
   - Interpolation von Funktionen $y_i = f(x_i)$
   Interpolationsfehler 1: Sei $f ∈ C^{n + 1}[a, b] ⇒ ∀ x ∈ [a, b] ∃ ξ_x ∈ \overline{(x_0, \dots, x_n, x)}$:
   \[f(x) - p(x) = \frac{f^{(n + 1)}(ξ_x)}{(n + 1)!} \prod_{j = 0}^n (x - x_j)\]
   #+ATTR_LATEX: :options [Interpolationsfehler 2]
   #+begin_thm latex
   Sei $f ∈ C^{n + 1}[a, b]$. Dass gilt für $x ∈ [a, b] \setminus \{x_0, \dots, x_n\}$:
   \[f(x) - p(x) = f[x_0, \dots, x_n, x] \prod_{j = 0}^n (x - x_j)\]
   mit der Notation
   \[f[x_i, \dots, x_{i + k}] = y[x_i, \dots, x_{i + k}]\]
   und es ist
   \[f[x_0, \dots, x_n, x] = ∫_0^1 ∫_0^{t_1} \dots ∫_0^{t_n} f^{(n + 1)}(x_0 + t_1(x_1 - x_0) + \dots + t_n(x_n - x_{n - 1}) + t(x - x_n)) \d t \d t_n \dots \d t_1\]
   #+end_thm
   #+begin_proof latex
   Per Induktion über $n$. \\
   IA: $n = 0$:
   \[f(x) - p_0(x) = f(x) - f(x_0) = \begin{cases} f[x_0, x](x - x_0) \\ \string(x - x_0\string)∫_0^1 f'(x_0 + t(x - x_0))\d t\end{cases}\]
   wobei ein
   \[∫_0^1 g'(t) \d t = g(1) - g(0)\]
   für $g(t) = f(x_0 + t(x - x_0)) ⇒ g'(t) = f'(t)(x - x_0)$ \\
   Sei die Behauptung richtig für $n - 1 \geq 0$. Dann ist
   \begin{align*}
   f(x) - p_n(x) &= f(x) - \sum_{i = 0}^{n}f[x_0, \dots, x_n]\prod_{j = 0}^{i - 1}(x - x_j) \\
   &= f(x) - p_{n - 1}(x) - f[x_0, \dots, x_n]\prod_{j = 0}^{n - 1}(x - x_j) \\
   &=  f[x_0, \dots, x_{n - 1}, x]\prod_{j = 0}^{n - 1}(x - x_j)- f[x_0, \dots, x_n]\prod_{j = 0}^{n - 1}(x - x_j) \\
   &= (f[x_0, \dots, x_{n - 1}, x] - f[x_0, \dots, x_n])\prod_{j = 0}^{n - 1}(x - x_j) \\
   &= \frac{f[x, x_0, \dots, x_n] - f[x_0, \dots, x_n]}{x - x_n}\prod_{j = 0}^{n - 1}(x - x_j) \\
   &= f[x_0, \dots, x_n, x]\prod_{j = 0}^{n - 1}(x - x_j)
   \end{align*}
   Weiterhin gilt:
   \begin{align*}
   f[x_0, \dots, x_{n - 1}, x] - f[x_0, \dots, x_n] &= ∫_0^1 ∫_0^{t_1} \dots ∫_0^{n - 1}[f^{(n)}(x_0 + t_1(x_1 - x_0) + \dots + t_n(x - x_{n + 1})) - f^{(n)}(x_0 + t_1(x_1 - x_0) + \dots + t_n(x_n - x_{n - 1}))] \d t_n \dots \d t_1 \\
   \intertext{Setze $g(t) = f^{(n)}(x_0 + t_1(x_1 - x_0) + \dots + t_n(x_n - x_{n - 1}) + t_{x - x_n})$}
   &= ∫_0^1 ∫_0^{t_1} \dots ∫_0^{t_{n - 1}}[g(t_n) - g(0)] \d t_n \dots \d t_1 \\
   &= ∫_0^1 ∫_0^{t_1} \dots ∫_0^{t_{n - 1}} ∫_0^{t_n} f^{(n + 1)}(x_0 + t_1(x_1 - x_0) + \dots + t_n(x_n - x_{n - 1}) + t(x - x_n))(x - x_n)\d t \d t_n \dots \d t_1 \\
   ⇒ f[x_0, \dots, x_n, x] &= ∫_0^1 ∫_0^{t_1} \dots ∫_0^{t_n} f^{(n + 1)}(\dots) \d t \d t_n \dots \d t_1
   \end{align*}
   #+end_proof
   Die Integraldarstellung der dividierten Differenzen gestattet ihre stetige Fortsetzung für den Fall, das Stützstellen zusammenfallen:
   \[f[x_0, \dots, x_r, x_r, \dots, x_n] = \lim_{ε \to 0} f[x_0, \dots, x_r, x_r + ε, \dots, x_n]\]
   Im Extremfall $x_0 = x_1 = \dots = x_n$ wird
   \begin{align*}
   f[x_0, \dots, x_n] &= ∫_0^1 ∫_0^{t_1} \dots ∫_0^{t_{n - 1}} f^{(n)}(x_0) \d t_n \dots \d t_1 \\
   &= ∫_0^1 ∫_0^{t_1} \dots ∫_0^{t_{n - 1}} 1 \d t_n \dots \d t_1 f^{(n)}(x_0) \\
   &= \frac{1}{n!}f^{(n)}(x_0)
   \end{align*}
   Damit geht das Newtonsche Interpolationspolynom über in das Taylorpolynom n-ten Grades von $f$ in $x_0$.
   Konstruieren wir die Fehlerdarstellung so erhalten wir für ein $ξ_x ∈ (x_0, \dots, x_n, x)$
   \begin{align*}
   \frac{f^{(n + 1)}(ξ_x)}{(n + 1)!} \prod_{j = 0}^n (x - x_j) &= f(x) - p(x) \\
   &= f[x_0, \dots, x_n, x] \prod_{j = 0}^n (x - x_j) \\
   ⇒ f[x_0, \dots, x_n, x] &= \frac{f^{(n + 1)}(ξ_x)}{(n + 1)!}
   \end{align*}
   #+ATTR_LATEX: :options [Hermite-Interpolation]
   #+begin_defn latex
   Die Hermitesche Interpolationsaufgabe lautet: \\
   Gegeben $x_i, i = 0, \dots, m$ (paarweise verschieden),
   $y_i^{(k)}, i = 0, \dots, m, k = 0, \dots, μ_i, μ \geq 0$. \\
   Gesucht: $p ∈ P_n, n = m + \sum_{i = 0}^{m} μ_i, p^{(k)}(x_j) = y_i^{(k)}, i = 0, \dots, m, k = 0, \dots, μ_i$,
   $(μ_i + 1)$ -fache Stützstellen.
   #+end_defn
   #+begin_ex latex
   $x_0 = -1, x_1 = 1, m = 1, y_0^{(0)} = 0, y^{(0)}_1 = 1, y_1^{(1)} = 2 ⇒ μ_0 = 0, μ_1 = 1 ⇒ n = 1 + 0 + 1 = 2 ⇒ p(x) = x^2$
   #+end_ex
   Analog zur Lagrange-Interpolation:
   - Existenz + Eindeutigkeit
   - Darstellung des Interpolationsfehlers
   Wiederholung: Fehlerdarstellung Lagrange-Interpolation. Sei $f∈ C^{n + 1}[a, b]$. $∃ ξ_x ∈ \overline{(x_0, \dots, x_n, x)}$
   \begin{align*}
   f(x) - p(x) &= \frac{f^{(n + 1)}(ξ_x)}{(n + 1)!} \prod_{j = 0}^{n}(x - x_j) \\
   f(x) - p(x) &= f[x_0, \dots, x_n, x] \prod_{j = 0}^n (x - x_j) \\
   f[x_0, \dots, x_n, x] &= ∫_0^1 ∫_0^{t_1} \dots ∫_0^{t_n}	f^{(n + 1)}(x_0 + t_1(x_1 - x_0) + \dots + t_n(x_n - x_{n - 1}) + t(x_n - x))\d t \d t_n \dots \d t_1
   \end{align*}
   Hermite-Interpolation: Such $p ∈ P_n, n = m + \sum_{i= 0}^{m} μ_i$
   \[p^{(k)}(x_i) = y_i^{(k)}, i = 0, \dots, m, k = 0, \dots, μ_i\]
** Richardsonsche Extrapolation zum Limes
   Gegeben: Numerischer Prozess mit Werten $a(h), h ∈ ℝ_+, h \to 0$. \\
   Gesucht: $a(0) = \lim_{h \to 0} a(h)$ \\
   Idee: Für $h_i > 0, i = 0, \dots, n$, interpoliere $(h_i, a(h_i))$ und berechne $p_n(0)$
   #+ATTR_LATEX: :options [Numerische Differentation]
   #+begin_ex latex
   Sei $f ∈ C^{∞}[a, b], x ∈ (a, b)$. Nach Taylor gilt
   \[a(h) = \frac{f(x + h) - f(x - h)}{2h} = f'(x) + \sum_{i = 1}^{∞} \frac{f^{(2i + 1)}(x)}{(2i)!} h^{2i}\]
   #+end_ex
   #+ATTR_LATEX: :options [Extrapolationsfehler]
   #+begin_thm latex
   Für $h ∈ ℝ_+$ habe $a(n)$ die Entwicklung
   \[a(h) = a_0 + \sum_{j = 1}^{n}a_j h^{jq} + a_{n + 1}(h) h^{(n + 1)q}\]
   mit $q > 0$, Koeffizienten $a_j$ und
   \[a_{n + 1}(h) = a_{n + 1} + \mathcal{o}(1)\]
   Die Folge $(h_i)_{k ∈ ℕ}$ erfülle
   \[0 \leq \frac{h_{k + 1}}{h_k} \leq ρ < 1\]
   ($⇒ h_k$ positiv, monoton fallend). Dann gilt für das Interpolationspolynom
   $p_1^{(k)} ∈ P_n$ (in $h^q$) durch
   \[(h_k^q, a(h_k)), \dots, (h_{k + n}^q, a(h_{k + n}))\]
   \[a(0) - p_n^{(k)}(0) = \mathcal{O}(h_k^{(n + 1)q})\]
   $(k \to ∞)$
   #+end_thm
   #+begin_proof latex
   Abkürzungen $z = h^q, z_k = h_k^q$. Interpoliere $(z_{k + i}, a(h_{k + i})), i = 0, \dots, n$.
   \begin{align*}
   p_n(z) &= \sum_{i = 0}^{n} a(h_{k + i})L_{k + i}^{(n)}I \\
   L_{k + 1}^{(n)}(z) &= \prod_{\substack{l = 0\\ l\ neq i}} \frac{z - z_{k + l}}{z_{k + 1} - z_{wl}}
   \end{align*}
   Übung:
   \[\sum_{i = 0}^{n} x_{k + 1}^n(0) = \begin{cases} 1 & r = 0 \\ 0 & r = 1, \dots, n \\ \string(-1\string)^n \prod_{j = 0}^n z_{k + i} & r = n + 1\end{cases}\]
   \begin{align*}
   p_n(0) &= \sum_{i = 0}^{n}(a_0 + \sum_{j = 1}^{n} a_j z_{k + i}^j + a_{n +1}(h_{k + 1})z_{k + i}^{n + 1})	L_{k + i}^{(n)}(0) \\
   &= a_0 \underbrace{\sum_{i = 0}^{n} L_{k + 1}^{(n)}}_{= 1} + \sum_{j = 1}^{n} a_j \underbrace{\sum_{i = 0}^{n}z_{k + 1}^j L_{k + i}^{(n)}(0)}_{0} \\
   &= + a_{n + 1} \underbrace{\sum_{i = 0}^{n}z_{k + 1}^{n + 1} L_{k + 1}^{(n)}}_{= (-1)^n \prod_{i = 0}^n z_{k + i}} + \sum_{i = 0}^{n} \mathcal{o}(1) z_{k + i}^{n + 1} L_{k + i}^{(n)}(0) \\
   \intertext{Da man Landau-Symbole nich ausklammern darf, schätzen wir ab:}
   \abs{L_{k + i}^{(n)}(0)} &= \prod_{\substack{l = 0\\ l \neq i}}^n \abs{\frac{z_k + l}{z_{k + i} - z_{k + y}}} \\
   &\leq \prod_{l = 0}^{i - 1} \abs{\frac{z_{n + l}}{z_{k + i} - z_{k + l}}} \prod_{l= 1 + i}^n \abs{\frac{z_{k + i}}{z_{k + i} - z_{k + l}}} \\
   &= \prod_{l = 0}^{i - 1} \frac{1}{\abs{\frac{z_{k + i}}{z_{k + y}} - 1}} \prod_{l= i + 1}^n \frac{1}{\abs{1 - \frac{z_{k + l}}{z_{k + i}}}} \\
   &\leq \frac{1}{(1 - ρ^q)^n} \\
   ⇒ p_n(0) &= a_0 + a_{n + 1} (-1)^n \prod_{i = 0}^n z_{k + i} + \mathcal{o}(z_k^{n + 1}) \\
   &= a_0 + \mathcal{O}(h_k^{(n + 1)q})
   \end{align*}
   #+end_proof
** Spline-Interpolation
   Problem: Oszillationen des Interpolationspolynoms, wenn man Stützstellen nicht geeignet wählen kann.
   Abhilfe: Stückweise polynomielle Interpolation:
   - Zerlegung: $a = x_0 < x_1 < \dots < x_n = b$
   - Teilintervalle: $I_i = [x_{i - 1}, x_{i}], i = 1, \dots, n$
   - Feinheit: $h = \max_{i = 1, \dots, n} h_i$ mit $h_i = \abs{I_i} = x_i - x_{i - 1}$
   - Vektorräume stückweise polynomieller Funktionen
     \[S^{k, r}_n [a, b] = \{p ∈ C^r[a, b] \mid p\mid_{I_i} ∈ P_k (i_i)\}, i = 1, \dots, n\]

   #+ATTR_LATEX: :options [Stückweise lineare Interpolation]
   #+begin_ex latex
   $⇒ p ∈ S_n^{(1, 0)}[a, b]$. Fehlerabschätzung:
   \[\max_{x ∈ [a, b]} \abs{f(x) - p(x)} \leq \frac{1}{2} h^2 \max_{x ∈ [a, b]} \abs{f''(x)}\]
   #+end_ex
   #+ATTR_LATEX: :options [Splines]
   #+begin_ex latex
   Zweimal stetig differenzierbare, stückweise kubische Polynome. Motivation: Biegestab. Minimiere Biegeenergie
   \[∫_{x_0}^{x_n} s''(x)^2 \d x\]
   #+end_ex
   #+ATTR_LATEX: :options [Kubischer Spline]
   #+begin_defn latex
   Eine Funktion $s_n:[a, b] \to ℝ$ heißt kubischer Spline bezüglich $a = x_0 < x_1 < \dots < x_n = b$, wenn gilt
   1. $s_n ∈ C^2[a, b]$
   2. $s_n\mid I_i ∈ P_3, i = 1, \dots, n$
   Gilt zusätzlich
   3. [@3] $s_n''(a) = s_n''(b) = 0$ so heißt $s_n$ natürlicher Spline.
   #+end_defn
   Existenz des interpolierenden kubischen Spline zu Knotenwerten $s_n(x_i) = y_i, i = 0, \dots, n$
   #+ATTR_LATEX: :options [Spline-Interpolation]
   #+begin_thm latex
   Der interpolierende kubische Spline existiert und ist eindeutig bestimmt durch zusätzliche Vorgabe von $s_n''(a), s_n''(b)$
   #+end_thm
   #+begin_proof latex
   $s$ hat die Form
   \[s(x) \mid_{I_i} = p_i(x), i = 1, \dots, n, p_i ∈ P_3(I_i)\]
   4 Koeffizienten auf jedem der $n$ Intervalle ergeben $4n$ Freiheitsgrade. Zur Bestimmung:
   | $s(x_i) = y_i, i = 0, \dots, n$          | $2n$ Gleichungen |
   | $s' ∈ C[a, b]$                           | $n - 1$          |
   | $s'' ∈ C[a, b]$                          | $n - 1$          |
   | Zusatzbedingung für $s_n''(a), s_n''(b)$ | 2                |
   |------------------------------------------+------------------|
   |                                          | $4n$             |
   $⇒$ quadratisches lineares Gleichungssystem, $4n × 4n$
   \[N = \{ω ∈ C^2[a, b] \mid ω_{x_i} = 0, i = 0, \dots, n\}\]
   Seien $s_n^{(1)}$ und $s_n^{(2)}$ interpolierende Splines $⇒ s = s_n^{(1)} - s_n^{(2)} ∈ N$. Für $ω ∈ N$ beliebig:
   \begin{align*}
   ∫_a^b s''(x) ω''(x) \d x &= \sum_{i = 0}^{n - 1} ∫_{x_i}^{x_{i + 1}} s''(x) ω''(x) \d x \\
   &= \sum_{i = 0}^{n - 1}[-∫_{x_i}^{x_{i + 1}} s^{(3)} ω' \d x + s'' ω' \mid_{x_i}^{x_{i + 1}}] \\
   &= \sum_{i = 0}^{n - 1}[-∫_{x_i}^{x_{i + 1}} s^{(4)} ω \d x - s^{(3)} ω \mid_{x_i}^{x_{i + 1}} + s'' ω' \mid_{x_i}^{x_{i + 1}}] \\
   &= \sum_{i = 0}^{n - 1} s'' ω' \mid_{x_i}^{x_{i + 1}} = s''(x) ω'(x) - s''(a) ω'(a) \\
   &= 0
   \end{align*}
   Speziell für $ω = s$
   \[∫_a^b \abs{s''(x)}^2 \d x = 0\]
   $⇒ s$ ist linear $0 = s(a) = s(b) = 0$
   #+end_proof
   Wiederhohlung: Extrapolation
   $a(h), h_i > 0, a(0) = \lim_{h \to 0}  a(h)$
   Fehler: Entwicklung
   \[a(h) = a_0 + \sum_{j = 1}^{n} a_j h^{a_j}\]
   \[0 < \frac{h_{k + 1}}{h_k} \leq ρ < 1\]
   interpolieren $(h_{k + 1}^a, a(h_{k + 1})), i = 1, \dots, n$
   \[⇒ a(0) - p_i^{(k)}(0) = \mathcal{O}(h_k^{(n + 1)})\]
   Splines: $S_h^{(k, r)}[a, b] = \{p ∈ C^r[a, b] \mid p \big|_{[x_i, x_{i + 1}]} ∈ P_k[x_i, x_{i + 1}]\}$
   Splines: $s ∈ S_k^{(n, x)}[a, b]$. Natürliche kubische Splines: $s''(a) = s''(b) = 0$.
   #+begin_thm latex
   Für den interpolierenden natürlichen Spline $S_n$ durch $x_0, \dots, x_n, y_0, \dots, y_n$ gilt
   \[∫_a^b \abs{S'(x)}^2 \d x \leq ∫_a^b \abs{g''(x)}^2 \d x\]
   bezüglich allen Funktionen $g ∈ C^2[a, b]$ mit $g(x_i) = y_i, i = 0, \dots, n$
   #+end_thm
   #+begin_proof latex
   Sei $N = \{ω ∈ C^2[a, b] \mid ω(x_i) = 0, i = 0, \dots, n\}$
   $⇒ ω = g - I_n ∈ N$.
   \begin{align*}
   ⇒ ∫_a^b \abs{g''(x)}^2 \d x &= ∫_a^b \abs{S_n''(x) + ω''(x)}^2 \d x \\
   &= ∫_a^b \abs{S_n''(x)}^2 \d x + \underbrace{2 ∫_a^b S_n''(x) ω''(x) \d x}_{0} + \underbrace{∫_a^b \abs{ω''(x)}^2 \d x}_{\geq 0}	\\
   &\geq ∫_a^b \abs{S_n''(x)}^2 \d x
   \end{align*}
   #+end_proof
   #+ATTR_LATEX: :options [Approximationsfehler]
   #+begin_thm latex
   Sei $f ∈ C^4[a, b]$. Erfüllt der interpolierende kubische Spline $S_1''(a) = f''(a) ∧ S_n(b) = f''(b)$ so gilt:
   \[\max_{x ∈ [a, b]} \abs{f(x) - S_n(x)} \leq \frac{1}{2} h^4 \max_{x ∈ [a, b]} \abs{f^{(4)}(x)}\]
   Ohne Beweis.
   #+end_thm
** Gauß Approximation
   Wir betrachten $C[a, b]$, die Menge der stetigen Funktionen auf $[a, b]$ über dem Zahlkörper $\mathbb{K} = ℝ$ oder $\mathbb{K} = ℂ$, als $\mathbb{K}$ -Vektorraum.
   Für $f, g ∈ [a, b]$ erfüllt
   \[(f, g) := ∫_a^b f(t) \overline{g(t)} \d t\]
   die Eigenschaften eines Skalarproduktes:
   1. Definitheit:
	  \[(f,f) =∫_a^b f(t) \overline{f(t)} \d t = ∫_a^b \abs{f(t)}^2 \d t \geq 0\]
	  und $(f, f) = 0 ⇒ f = 0$
   2. $α ∈ \mathbb{K}, h ∈ C[a, b]$:
	  \[(αf + g, h) = ∫_a^b(α f(t) + g(t))\overline{h(t)} \d t = α ∫_a^b f(t) \overline{h(t)} \d t + ∫_a^b g(t) \overline{h(t)} \d t = α(f, h) + (g, h)\]
   3. Symmetrie:
	  \[(f, g) = ∫_a^b f(t) \overline{g(t)} \d t = ∫_a^b \overline{\overline{f(t)} g(t)} \d t = ∫_a^b g(t) \overline{f(t)} \d t = \overline{(g, f)}\]
   Durch $\norm{f} = \sqrt{(f, f)}$ ist damit eine Norm auf $C[a, b]$ gegeben:
   1. Definitheit:
	  \[\norm{f} \geq 0, f = 0 ⇔ \norm{f} = 0\]
   2. Sublinearität: Wir benutzen die Cauchy-Schwarz-Ungleichung
	  \begin{align*}
	  \abs{(f, g)} &\leq \norm{f}\norm{g} \\
	  ⇒ \norm{f + g}^2 &= (f + g, f + g) = (f, f) + (f, g) + (g, f) + (g, g) \\
	  &= \norm{f}^2 + \underbrace{2\Re(f, g)}_{\leq \abs{(f, g)}} + \norm{g}^2 \\
	  &\leq \norm{f}^2 + 2 \norm{f} \norm{g} + \norm{g}^2 = (\norm{f} + \norm{g})^2 \\
	  ⇒ \norm{f + g} &\leq \norm{f} + \norm{g} \tag{Dreiecksungleichung}
      \end{align*}
   3. Homogenität:
	  \[\norm{α f} = \sqrt{(αf, αf)} = \sqrt{α \bar α (f, f)} = \abs{α} \norm{f}\]
   Mit diesem Skalarprodukt und dieser Norm ist also $C[a, b]$ ein Prähilbertraum.
   #+ATTR_LATEX: :options [Gauß-Approximation]
   #+begin_thm latex
   Sei $H$ ein Prähilbertraum und sei $S ⊂ H$ eine endlichdimensionaler Teilraum. Dann existiert zu jedem $f ∈ H$ eine eindeutig bestimmte "beste Approximation" $g ∈ S$
   \[\norm{f - g} = \min_{φ ∈ S} \norm{f - φ}\]
   #+end_thm
   #+begin_proof latex
   *Vorüberlegung*: Wenn $g ∈ S$ eine beste Approximation ist, so hat für $φ ∈ S$ die Hilfsfunktion
   \[F_φ(t) = \norm{f - g - tφ}^2, t ∈ ℝ\]
   bei $t = 0$ ein Minimum. Somit ist
   \begin{align*}
   0 &= \dd{}{t} F_φ(t)\big|_{t = 0} = \dd{}{t}[(f - g - tφ, f - g - tφ)]\big|_{t = 0} \\
   &= \dd{}{t}[(f - g, f - g) - t(φ, f - g) - f(f - g, φ) + t^2(φ, φ)]\big|_{t = 0} \\
   &= 2 \Re(f - g, φ) ∀ φ ∈ S
   \end{align*}
   Falls $\mathbb{K} = ℂ$ ergibt testen mit $i φ$
   \[0 = \Re(f - g, iφ) = -\Re(f - g, φ) = \Im(t - g, φ) ⇒ (f - g, φ) = 0 ∀ φ ∈ S\]
   Interpretation: Der Fehler $f - g$ ist orthogonal zum Teilraum $S$.
   Gilt umgekehrt die letzte Gleichung für ein $g ∈ S$, so gilt für $φ ∈ S$
   \begin{align*}
   \norm{f - g}^2 &= (f - g, f - g) = (f - g, f - φ) + \underbrace{(f - g, φ)}_{0} \\
   \intertext{Cauchy-Schwarz:}
   &\leq \norm{f - g}\norm{f - φ} \\
   ⇒ \norm{f - g} \leq \inf_{φ ∈ S} \norm{f - φ}
   \end{align*}
   $⇒ g$ ist Bestapproximation. \\
   *Existenz und Eindeutigkeit*: Da $n = \dim S < ∞$, besitzt $S$ eine Basis $\{φ_1, \dots, φ_n\}$. Jedes $g ∈ S$ hat eine eindeutige Darstellung
   \begin{align*}
   g &= \sum_{i = 1}^{n} α_i φ_i \\
   ⇒ (f - \sum_{i = 1}^{n} α_i φ, φ) &= (f, φ) - \sum_{i = 1}^{n} α_i (φ_i, φ) = 0 ∀ φ ∈ S \\
   ⇒ \sum_{i = 1}^{n}(φ_i, φ) α_i &= (f, φ_k), k = 1, \dots, n
   \end{align*}
   Dies ist ein lineares $n × n$ Gleichungssystem. Notation: $Aα = B$ mit $α = (α_1, \dots, α_n)^T ∈ \mathbb{K}^n, b ∈ \mathbb{K}^n, b_i = (f, φ_i), A ∈ \mathbb{K}^{n × n}, A_{ki} = (φ_i, φ_k)$.
   $A$ ist hermitisch wegen $(φ_i, φ_k) = \overline{(φ_k, φ_i)}$. Sei $α ∈ \mathbb{K}^n$ beliebig. Wegen
   \begin{align*}
   α^H A α &= \sum_{k = 1}^{n} \sum_{i = 1}^{n} \bar α_k (φ_i, φ_k) α_i \\
   &= \sum_{k = 1}^{n} \sum_{i = 1}^{n} (α_i, φ_i, α_k, φ_k) \\
   &= (\sum_{i = 1}^{n}α_i φ_i, \sum_{k = 1}^{n} α_k φ_k) = (g, g) > 0
   \end{align*}
   für $α \neq 0 (⇒ g \neq 0)$ ist $A$ also positiv definit und damit invertierbar $⇒$ mit $α = A^{-1} b$ löst das eindeutig bestimmte Gleichungssystem und $g$ ist die Bestapproximation.
   #+end_proof
   Das lineare Gleichungssystem besitzt besonders einfache Lösung, wenn die Basis $\{φ_1, \dots, φ_n\}$ eine Orthogonalbasis ist, das heist $(φ_i, φ_j) = δ_{ij}$
   \begin{align*}
   ⇒ α_i &= (f, φ_i), i = 1, \dots, n \\
   ⇒ g &= \sum_{i = 1}^{n} (f, φ_i) φ_i \quad \text{ist Bestapproximation}
   \end{align*}
   #+ATTR_LATEX: :options [Gram-Schmidt-Algorithmus]
   #+begin_lemma latex
   Zu jeder Basis $\{ψ_1, \dots, ψ_k\}$ von $S$ lässt sich eine Orthonormalbasis $\{φ_1, \dots, φ_n\}$ konstruieren.
   \[\tilde φ_1 = ψ_1, φ_1 = \frac{\tilde φ_1}{\norm{\tilde φ_1}}\]
   \[\tilde φ_k = ψ_k - \sum_{i = 1}^{k - 1}(ψ_k, φ_i), φ_k = \frac{\tilde φ_k}{\norm{\tilde φ_k}}\]
   #+end_lemma
   #+begin_proof latex
   Per Induktion nach $n$. \\
   $n = 1$: Da $ψ \neq 0$ gilt $(φ_1, φ_1) = \frac{\abs{ψ_1}^2}{\norm{ψ_1}^2} = 1$. \\
   $n > 1$: Sei $\{φ_1, \dots, φ_n\}$ eine Orthonormalbasis. Es gilt
   \[0 \neq \tilde φ_n = ψ_n - \sum_{k = 1}^{n - 1}(ψ_n, φ_k)φ_k\]
   da sonst $\{ψ_1, \dots, ψ_n\}$ linear abhängig wären. Für $i = 1,\dots, n - 1$ gilt
   \[(φ_n, φ_1) = (ψ_n, φ_i) - \sum_{k = 1}^{n - 1}(ψ_n, φ_k)\underbrace{(φ_k, φ_i)}_{δ_{ik}} = 0\]
   und $\norm{φ_n}^2 = 1$ nach Konstruktion.
   #+end_proof
   Wiederhohlung: Gauß-Approximation, Prähilbertraum $H$, Teilraum $S ⊂ H, \dim S = n < ∞$
   \[∀ f ∈ H ∃! g ∈ S: \norm{f - g} \leq \min_{φ ∈ S} \norm{f - φ}\]
   Äquivalent: $e := f - g \perp S ⇔ (f - g, φ) = 0 ∀ φ ∈ S$ \\
   Orthogonalisiere Basis $\{ψ_1, \dots, ψ_n\}$ von $S$ mit Gram-Schmidt
   \[\tilde φ_i = \begin{cases} ψ_i & i = 1 \\ ψ_i - \sum_{j = 1}^{i - 1} \frac{(ψ_i, \tilde φ_j)}{\norm{\tilde φ_j}^2} \tilde φ_j & i = n, \dots, n\end{cases}\]
   Normalisieren: $φ_k = \norm{\tilde φ_n}^{-1} \tilde φ_k$.
   $(φ_1, \dots, φ_k)$ Orthogonalbasis $⇒ (φ_i, φ_j) = δ_{ij}$
   \[⇒ g = \sum_{k = 1}^{n} (f, φ_k) φ_k\]
   Erinnerung:
   \[f[x_0, \dots, x_n] = \frac{f[x_1, \dots, x_n] - f[x_0, \dots, x_{n - 1}]}{x_n - x_0}, \quad f[g_k] = f(k)\]
* Numerische Integration
  Approximation von bestimmten Integralen reeller Funktionen $f ∈ C[a, b]$ durch Quadraturformeln
  \[I(f) = ∫_a^b f(x) \d x \approx I^{(n)}(f) = \sum_{i = 1}^{n} α_i f(x_i)\]
  mit Stützstellen $a \leq x_0 < x_1 < \dots < x_n \leq b$ und Gewichten $α_i ∈ ℝ$.
  #+ATTR_LATEX: :options [Summierte Rechteckregel]
  #+begin_ex latex
  \[∫_a^b f(x) \d x \approx \sum_{i = 0}^{n - 1}(x_{i + 1} - x_i) f(x_i)\]
  #+end_ex
  Interpolatorische Quadraturformeln. \\
  Idee: Interpoliere $f$ durch ein Interpolationspolynom auf $[a, b]$!
  \[p_n(x) = \sum_{i = 0}^{n} f(x_i) L_i^{(n)}(x)\]
  \[⇒ I^{(n)}(f) = ∫_a^b p_n(x) \d x = \sum_{i = 0}^{n} f(x_i) ∫_a^b \underbrace{L_i^{(n)}(x) \d x}_{= α_i}\]
  Quadraturgewichte hängen nur von $a, x_0, \dots, x_n, b$ ab.
  #+ATTR_LATEX: :options [Lagrange-Quadratur]
  #+begin_thm latex
  Für interpolatorische Quadraturformeln gilt
  \[I(f) - I^{(n)}(f) = ∫_a^b f[x_0, \dots, x_n, x] \prod_{i = 0}^n (x - x_i) \d x\]
  #+end_thm
  #+begin_proof latex
  Restglieddarstellung der Interpolation.
  #+end_proof
  #+begin_defn latex
  Eine Quadraturformel $I^{(n)}$ wird "von der Ordnung $m$" genannt, wenn sie alle $p ∈ P_{m - 1}$ exakt integriert. Das heißt
  \[∫_a^b p(x) \d x = I^{(n)}(p) ∀ p ∈ P_{m - 1}\]
  $⇒$ Interpolatorische Quadraturformeln zu $n + 1$ Stützstellen sind (mindestens) von der Ordnung $n + 1$.
  #+end_defn
  Spezialfall: Äquidistante Stützstellen: Newton-Cotes-Formeln:
  1. Abgeschlossene Formeln ($H = \frac{b - a}{n}, x_i = a + iH, a = x_0, b = x_n$)
	 \begin{align*}
	 I^{(1)}(f) &= \frac{b - a}{2}[f(a) + f(b)] \tag{Trapezregel} \\
	 I^{(2)}(f) &= \frac{b - a}{6}[f(a) + 4f(\frac{a + b}{2}) + f(b)] \tag{Simpsonregel, Keplersche Fassregel} \\
	 I^{(3)}(f) &= \frac{b - a}{8}[f(a) + 3f(a + H) + 3f(b - H) + f(b)] \tag{$3/8$ Regel}
     \end{align*}
  2. Offene Formeln $(H = \frac{b - a}{n + 2}, x_i = a + (i + 1)H, a < x_0, x_n < b)$
	 \begin{align*}
	 I^{(0)}(f) &= (b - a) f(\frac{a + b}{2}) \tag{Mittelpunktregel} \\
	 I^{(1)}(f) &= \frac{(b - a)}{2} (f(a + H) + f(b - H)) \\
	 I^{(1)}(f) &= \frac{(b - a)}{3} (2f(a + H) - f(\frac{a + b}{2}) + 2f(b - H)) \\
     \end{align*}
  #+ATTR_LATEX: :options [Quadraturrestglieder]
  #+begin_thm latex
  1. Trapezregel: Für jedes $f ∈ C^2[a, b]$ gibt es ein $ξ ∈ [a, b]$ mit
	 \[∫_a^b f(x) \d x - \frac{b - a}{2}[f(a) + f(b)] = -\frac{(b - a)^3}{12}f''(ξ)\]
  2. Simpson-Regel: Für jedes $f ∈ C^4[a, b] ∃ ξ ∈ [a, b]$ sodass
	 \[∫_a^b f(x) \d x - \frac{b - a}{6}[f(a) + 4f(\frac{a + b}{2}) + f(b)] = - \frac{(b - a)^5}{2880}f^{(4)}(ξ)\]
  3. Mittelpunktregel: $∀ f ∈ C^2[a, b] ∃ ξ ∈ [a, b]$ sodass
	 \[∫_a^b f(x)\d x - (b - a)f(\frac{a + b}{2}) = \frac{(b - a)^3}{24} f''(ξ)\]
  #+end_thm
  #+ATTR_LATEX: :options [Verallgemeinerter Mittelwertsatz]
  #+begin_thm latex
  Sei $f ∈ C[a, b], g \geq 0$ oder $g \leq 0$ integriebar. Dann $∃ ξ ∈ [a, b]$, sodass
  \[∫_a^b f(x) g(x) \d x = f(ξ)∫_a^b g(x) \d x\]
  #+end_thm
  #+begin_proof latex
  (Beweis der Quadraturrestglieder).
  1. Für $x ∈ [a, b]$ ist $(x - a)(x - b) \leq 0$
	 \begin{align*}
	 ⇒ I(f) - I^{(1)}(f) &= ∫_a^b f[x_0, x_1, x] \prod_{i = 1}^1 (x - x_i) \d x \\
	 \intertext{Verallgemeinerter Mitterwertsatz: $∃ ξ ∈ [a, b]$, sodass}
	 &= \frac{f''(ξ)}{2!} (-\frac{1}{6}(b - a)^3) \\
	 &= - \frac{f''(ξ)}{12}(b - a)^3
     \end{align*}
  2.
	  \begin{align*}
	  I(f) - I^{(2)}(f) &= ∫_a^b f[a, \frac{a + b}{2}, b, x](x - a)(x - \frac{a + b}{2})(x - b) \\
	  &= ∫_a^b \frac{f[a, \frac{a + b}{2}, b, x] - f[\frac{a + b}{2}, a, \frac{a + b}{2}, b]}{x - \frac{a + b}{2}}(x - a)(x - \frac{a + b}{2})^2(x - b) \d x + f[\frac{a + b}{2}, a, \frac{a + b}{2}, b]∫_a^b (x - a)(x - \frac{a + b}{2})(x - b) \d x \\
	  &= \frac{f^{(4)}(ξ)}{4!}∫_a^b (x - a)(x - \frac{a + b}{2})^2 (x - b) \d x \\
	  &= -\frac{f^{(4)}(ξ)}{2880}(b - a)^5
      \end{align*}
  3. analog zu 2.
  #+end_proof
  Probleme:
  - negative Gewichte $α_i$ ab $n = 7$ (geschlossen) und $n = 2$ (offen) $⇒$ Auslöschungsgefahr
  -	Oszillationen des Lagrange-Interpolanten für äquidistante Gitter (Runge-Phänomen), im Allgemeinen $I^{(n)}(f) \not \to I(f), n \to ∞$
  Abhilfe: Summierte Quadraturformeln
  \[I_n^{(n)}(f) = \sum_{i = 1}^{N - 1}I_{[x_i, x_i + 1]}^{(n)}(f), h = \frac{b - a}{N}, x_i = a + ih\]
  Gilt die lokale Fehlerdarstellung:
  \[I_{[x_i, x_{i + 1}]}(f) - I_{[x_i, x_{i + 1}]}^{(n)}(f) = ω_n h^{n + 2} f^{(m + 1)}(ξ_i), \quad ξ_i ∈ [a, b]\]
  für $m \geq n$ gilt:
  \begin{align*}
  I(f) - I_{n}^{(n)}(f) &= \sum_{i = 0}^{N - 1}[I_{[x_i, x_{i + 1}]}(f) - I_{[x_i, x_{i + 1}]}^{(n)}(f)] \\
  &= ω_n h^{m + 2} N \underbrace{\sum_{i = 0}^{N - 1} \frac{f^{(m + 1)}(ξ_i)}{N}}_{∈ [\min_i f^{(m + 1)}(ξ_i), \max_i f^{(m + 1)}(ξ_i)]} \\
  &= ω_n h^{m + 2} N f^{(m + 1)}(ξ) \tag{für ein $ξ ∈ [a, b]$ (Verallg. Mittelwertsatz)} \\
  &= ω_n h^{(m + 1)}(b - a)f^{(m + 1)}(ξ)
  \end{align*}
  #+begin_ex latex
  1. Summierte Trapezregel $(m = 1)$
	 \begin{align*}
	 I_h^{(1)} &= \sum_{i = 0}^{N - 1}\frac{x_{i + 1} - x_i}{2}[f(x_i) + f(x_{i + 1})] \\
	 &= \frac{h}{2} f(a) + h \sum_{i = 1}^{N - 1} f(x_i) + \frac{h}{2}f(b) \\
	 I(f) - I_{h}^{(n)}(f) &= -\frac{b - a}{12} h^2 f''(ξ), ξ ∈ [a, b]
     \end{align*}
  2. Summierte Simpson-Regel $(m = 3)$
	 \begin{align*}
	 I_h^{(2)}(f) &= \sum_{i = 0}^{N - 1} \frac{x_{i + 1} - x_i}{6}[f(x_i) + 4f(\frac{x_i + x_{i + 1}}{2}) + f(x_{i + 1})] \\
	 &= \frac{h}{6}[f(a) + 2 \sum_{i = 1}^{N - 1} f(x_i) + 4 \sum_{i = 0}^{N - 1}f(\frac{x_i + x_{i + 1}}{2}) + f(b)] \\
	 I(f) - I_h^{(2)}(f) &= -\frac{b - a}{2880}h^4 f^{(4)}(ξ), ξ ∈ [a, b]
     \end{align*}
  3. Summierte Mittelpunktsregel $(m = 1)$
	 \begin{align*}
	 I_h^{(0)}(f) &= \sum_{i = 0}^{N - 1}(x_{i + 1} - x_i)f(\frac{x_i + x_{i + 1}}{2}) = h \sum_{i = 0}^{N - 1}f(\frac{x_i + x_{i + 1}}{2}) \\
	 I(f) - I_h^{(0)}(f) &= \frac{b - a}{24} h^2 f''(ξ), \quad ξ ∈ [a, b]
     \end{align*}
  #+end_ex
  Widerholung Quadratur
  \[∫_a^b f(x) \d x \approx \sum_{i = 0}^{n} α_i f(x_i) = I^{(n) f}\]
  - Interpolatorische Quadraturregel, Äquidistante Stützstellen \\
    $\to$ Newton-Cotes Formeln (abgeschlossen, offen)
  - Summierte Formeln $x_i = a + i H, H > 0$
	\[I_H^{(n)}(f) = \sum_{i = 1}^{n} I_{[x_{i - 1}, x_i]}^{(n)}(f)\]
** Gaußsche Quadraturformeln
   Frage: Wie wählt man $x_i$ in
   \[I^{(n)}(f) = \sum_{i = 0}^{N} α_i f(x_i)\]
   optimal? Nach Konstruktion ist $I^{(n)}$ mindestens von der Ordnung $n + 1$
   #+begin_lemma latex
   Interpolatorische Quadraturformeln sind höchstens von der Ordnung $2n + 2$
   #+end_lemma
   #+begin_proof latex
   Wähle
   \begin{align*}
   p(x) &= \prod_{i = 0}^n (x - x_i)^2 ∈ P_{2 n + 2} \\
   ⇒ 0 &< ∫_a^b p(x) \d x = \sum_{i = 0}^{n} α_i \underbrace{p(x_i)}_{0} = 0 \lightning
   \end{align*}
   #+end_proof
   Gaußsche Quadraturformen erreichen die Maximalordnung $2n + 2$ (exakt für $p ∈ P_{2n + 1}$)
   Herleitung: Für $x_0, \dots, x_n, x_{n + 1}, \dots, x_{2n + 1} ∈ [a, b]$ betrachte $I^{(n)}(t)$ und I^{(2n + 1)}(f)
   \begin{align*}
   I(f) - I^{(2n + 1)}(f) &= I(f) - \sum_{i = 0}^{2n + 1} f[x_0, \dots, x_i] \big|_a^b ∫_a^b \prod_{j = 0}^{i - 1} (x - x_j) \d x \\
   &= I(f) - I^{(n)}(f) - \sum_{i= n + 1}^{2n + 1}f[x_0, \dots, x_i] ∫_a^b \prod_{j = 0}^{i - 1}(x - x_j) \d x \\
   \intertext{Für $i > n$ gilt}
   ∫_a^b \prod_{j = 0}^{i - 1}(x - x_j) \d x &= ∫_a^b \underbrace{\prod_{j = 0}^{n}(x - x_j)}_{P_{n + 1}} \underbrace{\prod_{j = n + 1}^{i - 1} (x - x_j)}_{∈ P_n} \d x \\
   \intertext{Wähle Stützstellen so, dass}
   0 &= ∫_a^b \prod_{j = 0}^n (x - x_j) q(x) \d x = (\prod_{j = 0}^n (x - x_j), q) ∀ q ∈ P_n \\
   I(f) - I^{(n)}(f) &= I(f) - I^{(2n + 1)}(f) \\
   \end{align*}
   $⇒ I^{(n)}$ ist exakt für $p ∈ P_{2n + 1}$, das heißt von Ordnung $2n + 2$. Mit einem Orthogonalsystem $\{p_0, \dots, p_{n + 1}\}$ von $P_{n + 1}$ sind die Nullstellen
   $λ_0, \dots, λ_n$ von $p_{n + 1}$ von Interesse. Frage: Sind die Nullstellen von $p_{n + 1}$ reell, einfach und in $[a, b]$?
   #+begin_thm latex
   Gegeben sei ein Skalarprodukt auf $C[a, b]$
   \[(f, g)_ω = ∫_a^b f(x) g(x) ω(x) \d x\]
   mit integrierbarer Gewichtsfunktion $ω(x) \geq 0, x ∈ (a, b)$ mit höchstens endlich vielen Nullstellen. Dann haben die mittels Gram-Schmidt aus \{1, x^1, \dots\} bezüglich $(·,·)_ω$ orthogonalisierten Polynome
   $\{p_0, p_1, \dots\}$ lauter reelle, einfache Nullstellen in $[a, b]$
   #+end_thm
   #+begin_proof latex
   Sei $N_n := \{λ ∈ (a, b) \mid λ \text{ Nullstelle ungerader Vielfachheit von } p_n\}$. Setze
   \[q(x) = \begin{cases} 1 & N_n \neq \emptyset \\ \prod_{i = 1}^m (x - λ_i) & N_n = \{λ_1, \dots, λ_m\}, m > 0\end{cases}\]
   Nach dem Fundamentalsetz der Algebra und wegen $p(x) = x^n - r(x), r ∈ P_{n - 1}$, nach Konstruktion mit Gram-Schmidt (ohne Normalisieren) gilt
   \[p_n(x) = \prod_{i = 1}^n (x - λ_i), λ_i ∈ ℂ, i = 1, \dots, n\]
   Ist $λ_I$ nicht reell, so ist $\bar λ_i$ auch eine Nullstellen von $p_N$ und
   \[(x - λ_i)x - \bar λ_i\ = (x - λ_I)(x - λ_i) ⇒ \abs{x - λ_i}^2 \geq 0\]
   $⇒ p_n q ∈ P_{n + m}$ ist reell und hat in $[a, b]$ keinen Vorzeichenwechsel.
   \[(p_n, q)_ω = ∫_a^b p_n(x) (x) ω(x) \d k \neq 0\]
   Für $m < n$ ist das ein Wiederspruch zu $p_n \perp p_{n - 1} ⇒ μ_n = \{λ_1, \dots, λ_n\}$. Für $[a, b] = [-1, 1]$ und  $ω \equiv 1$, das heißt $(·, ·)_ω = (·,·)_2$
   sind die $p_n$ mittels $p_n(x) = x^n + \dots$ mormiere Legendre-Polynome $L_n$(x). Wir wählen also die Nullstellen $ζ_0,ndots, λ_n$ von $p_{n + 1}$ beziehungsweise $L_{n + 2}$ als
   Stützstellen einer interpolatorischen Quadraturformel auf $[-1, 1]$.
   \[I^{(n)}(f) = \sum_{i = 9}^{n} α_i f(λ_i), α_i = ∫_{-1}^1 \prod_{\substack{j = 0 \\ j \neq i}} \frac{x - λ_j}{λ_i - λ_j} \d x\]
   #+end_proof
   #+ATTR_LATEX: :options [Gauß-Quadratur]
   #+begin_thm latex
   Es gibt genau eine interpolatorische Quadraturformel zu $n + 1$ paarweise verschiedenen Stützstellen auf $[-1, b]$ mit Ordnung $2n + 2$. Ihre Stützstellen sind gerade die Nullstellen.
   $λ_0, \dots, λ_n ∈ (-1, 1)$ das $(n + 1)$ - ten Legendre Polynom $L_{n + 1} ∈ P_{n + 1}$ und die Gewichte erfüllen
   \[α_i = ∫_{-1}^1 \prod_{\substack{j = 0 \\ j \neq i}} (\frac{x - λ_j}{λ_i - λ_j})^2 \d x > 0, i = 0, \dots, n\]
   Für $f ∈ C^{2 n + 2}[-1, 1]$ besitzt des Restglied die Darstellung
   \[R^{(n)} = \frac{f^{(2n + 2)}(ξ)}{(2n + 2)!} ∫_{-1}^1 \prod_{j = 0}^n (x - λ_j)^2 \d x, ξ ∈ (-1, 1)\]
   #+end_thm
   #+begin_proof latex
   *Existenz*:
   Es gilt $p_{n + 1} \perp P_n$ Für $ω = 1$ und $p_n(x) = \prod_{i = 0}^n(x - λ_i) = x^n + \dots$
   \[⇒ I^{(n)}(f) = I^{(2n + 1)}(f)\]
   $⇒ I^{(n)}$ hat Ordnung $2n + 2$. Gewichte:
   \[L_i^{(x)}(x) = \prod_{\substack{j = 0 \\ j \neq i}}^n \frac{x - λ_j}{λ_i - λ_j} ∈ P_n\]
   $⇒ (L_i^{(n)}(x))^2 ∈ P_{2n}$
   \[⇒ 0 < ∫_{-1}^1 (L_i^{(n)})^2 \d x = \sum_{j = 0}^{n} α_j \underbrace{(L_i^{(n)}(x_i))}_{δ_{ij}} = α_i\]
   *Eindeutigkeit*: Sei $\tilde I^{(n)}(f) = \sum_{i = 0}^{n} \tilde a_I f(\tilde λ_i)$ ebenfalls der Ordnung $2n + 2$. Wie oben folgt $\tilde α_i > 0$ mithilfe
   \[\tilde L_i^{(n)}(x) = \prod_{j = 0 \\ j \neq i}^n \frac{n - \tilde λ_j}{\tilde λ_i - \tilde λ_j}\]
   \begin{align*}
   0 &= ∫_{-1}^1 \frac{1}{\tilde α_i} \tilde L_i^{(n)} p_{n + 1}(x) \d x \\
   &= \sum_{j = 0}^{n} \frac{\tilde α_i}{\tilde α_i} \underbrace{\tilde L_i^{(n)} (\tilde λ_j)}_{δ_{ij}} p_{n + 1}(\tilde λ_j) = p_{n + 1}(\tilde λ_i), i = 0, \dots, n
   \end{align*}
   $⇒ \tilde λ_i = λ_i$ und $\tilde α_i = α_i, i = 1, \dots, n$. \\
   *Restglied*: Für $f ∈ C^{(2n + 2)}[-1, 1]$ hat der Hermite-Interpolant $h ∈ P_{2n + 1}$ zu den Bedingungen
   \[h(λ_i) = f(λ_i), h'(λ_i) = f'(λ_i), i = 0, \dots, n\]
   die Darstellung:
   \[f(x) - h(x) = f[λ_0, λ_0, \dots, λ_n, λ_n, x] \prod_{i = 0}^n (x - λ_i)^2\]
   \begin{align*}
   ⇒ I(f) - I^{(f)} &= I(f) - \underbrace{I^{(n)}(h)}_{= I(h)} - (I^{(n)}(f) - I^{(n)}(h)) \\
   &= I(f - h) - I^{(n)}(f - h) \\
   &= ∫_{-1}^1 f[λ_0, λ_0, \dots, λ_n, λ_n] \underbrace{\prod_{i = 0}^n (x - λ_i)^2}_{> 0} \d x - \underbrace{\sum_{i = 0}^{n} α_i [f(λ_i) - h(λ_i)]}_{0} \\
   \intertext{Mit verallgemeinertem Mittelwertsatz folgt:}
   &= \frac{f^{(2n + 2)}(ξ)}{(2n + 2)!} ∫_{-1}^1 \prod_{i = 0}^n (x - λ_i)^2 \d x
   \end{align*}
   #+end_proof
   Die $λ_i^{(n)}$ (Nullstellen von $p_{n + 1}$) und die dazugehörigen $α_i$ lassen sich tabellieren. Durch Transformation von $[a, b]$ auf $[-1, 1]$ erhält man eine allgemeine Quadraturformel.
   #+ATTR_LATEX: :options [Konvergenz der Gauß-Quadratur]
   #+begin_thm latex
   Sei $I^{(n)}(f)$ die $(n + 1)$ punktige Grauß-Formel zur Berechnung von $I(f) = ∫_{-1}^1 f(x) \d x$. Für jedes $f ∈ C[-1, 1]$ konvergiert $I^{(n)}(f) \xrightarrow{n \to ∞} I(f)$
   #+end_thm
   #+begin_proof latex
   Es gilt
   \[I^{(n)}(f) = \sum_{i = 0}^{n} α_i^{(n)} f(λ_i^{(n)}), α_i^{(n)} > 0, \sum_{i = 0}^{n} α_i^{(n)} = 2\]
   Sei $ε > 0$. Nach dem Weierstrassschem Approximationssatz gibt es $p_ε ∈ P_n$ mit
   \[\max_{x ∈ [-1, 1]} \abs{f(x) - p_ε(x)} \leq \frac{ε}{4}\]
   Für $n > \frac{1}{2}m - 1$ (das heißt $2n + 2 > m$) gilt
   \[\abs{I(f) - I^{(n)}(f)} \leq \underbrace{\abs{I(f - p_ε)}}_{\leq \frac{ε}{4}2} + \underbrace{\abs{I(p_ε) - I^{(n)}(p_ε)}}_{0} + \underbrace{\abs{I^{(n)}(f - p_ε)}}_{\leq \frac{ε}{4}2} \leq ε\]
   #+end_proof
   Wiederholung: Gauß-Quadratur
   - $n + 1$ Stützstellen, Ordnung $2n + 2$ (optimal)
   - $x_i$ Nullstellen des Legendre Polynoms $p_{n + 1}$
   - $I^{(n)}(f) \xrightarrow{n \to ∞} I(f)$ für $f$ stetig
   - Verallgemeinerung auf gewichtete Integrale
	 \[∫_a^bf(x) ω(x) \d x I(fω) I_ω(f)\]
	 $⇒$ Orthogonalisiere bezüglich
	 \[(f,g)_ω = ∫_a^b f(x) g(x) ω(x) \d x\]
** Praktische Aspekte der Quadratur
   Ziel: Möglichst hohe Genauigkeit bei möglichst wenig Funktionsauswertungen. Schwierigkeiten:
   - Fehlerschätzung: $f^{(k)}$ nur schwer zugänglich für $k > 2$ $⇒$ a-posteriori Fehlerschätzer.
     #+begin_ex latex
	 1. Vergleiche $I_n(f)$ und $I_{\frac{n}{2}}(f)$ bei summierten Quadraturformeln
	 2. Extrapolationfehler
     #+end_ex
   - Wiederbenutzung bereits berechneter Werte von $f$
	 - schwierig bei Gauß
	 - einfach bei Newton-Cotes
* Lineare Gleichungssystem
  Gegeben: $A ∈ ℝ^{m × n} = (a_{ij}), b ∈ ℝ^m$. Gesucht: $x ∈ ℝ^n$ mit $Ax = b$
  $⇒ m$ Gleichungen, $n$ Unbekannte. Das lineare Gleichungssystem $Ax = b$ heißt
  - unterbestimmt, falls $m < n$
  - überbestimmt falls $m > n$
  - quadratisch falls $m = n$
  *Störungsstheorie*:
  - Konditionierung von quadratischen linearen Gleichungssystemen
  - Fehlereinfluss von Datenfehlern und Rundungsfehlern auf Lösung $x$
  *Vektor- und Matrizennormen*: \\
  Sei $\mathbb{K} = ℝ$ oder $\mathbb{K} = ℂ$. Erinnerung: Eigenschaften einer Norm: $\norm{·}: \mathbb{K}^n \to ℝ$
  - Definitheit: $\norm{x} > 0 ∀ x ∈ \mathbb{K}^n \setminus \{0\}$
  - Positive Homogenität: $\norm{α x} = \abs{α} \norm{x} ∀ x ∈ \mathbb{K}^n, α ∈ \mathbb{K}$
  -	Subadditivität: $\norm{x + y} \leq \norm{x} + \norm{y} ∀ x, y ∈ \mathbb{K}^n$
  #+begin_ex latex
  Euklidische Norm: $(l_2)$
  \[\norm{x}_2 = (\sum_{i = 1}^{n} \abs{x_i}^2)^{1/2}\]
  Maximumsnorm $(l_∞)$
  \[\norm{x}_∞ = \max_{i = 1, \dots, n} \abs{x_i}\]
  $l_1$ -Norm:
  \[\norm{x}_1 = \sum_{i = 1}^{n} \abs{x_i}\]
  $l_p$ -Norm, $p \geq 1, p < ∞$
  \[\norm{x}_p = (\sum_{i = 1}^{n} \abs{x_i}^p)^{1/p}\]
  #+end_ex
  Betrachte Vektorraum der $n × n$ -Matrizen $A ∈ \mathbb{K}^{n × n}$
  #+begin_defn latex
  Eine Norm $\norm{·}$ auf $\mathbb{K}^{n × n}$ heißt verträglich wit einer Vektornorm $\norm{·}$ auf $\mathbb{K}^n$, wenn gilt:
  \[\norm{A x} \leq \norm{A} \norm{x} ∀ x ∈ \mathbb{K}^n, A ∈ \mathbb{K}^{n × n}\]
  Sie heißt Matrizennorm, wenn sie submultiplikativ ist
  \[\norm{AB} \leq \norm{A} \norm{B} ∀ A, B ∈ \mathbb{K}^{n × n}\]
  #+end_defn
  #+begin_ex latex
  Die Frobeniusnorm
  \[\norm{A}_{Fr} = (\sum_{i,j = 1}^{n} \abs{a_{ij}}^2)^{1/2}\]
  ist eine mit $\norm{·}_2$ verträgliche Matrizennorn. \\
  Die natürliche Matrizennorm
  \[\norm{A} = \sup_{x ∈ \mathbb{K}^n \setminus \{0\}} \frac{\norm{A x}}{\norm{x}} = \sup_{\substack{x ∈ \mathbb{K}^n\\ \norm{x} = 1}} \norm{A x}\]
  ist eine mit $\norm{·}$ verträgliche Matrizennorm (Übung!). Es gilt
  \[\norm{\mathbb{I}} = \sup_{\substack{x ∈ \mathbb{K}^n\\ \norm{x} = 1}} \norm{\mathbb{I} x} = 1\]
  #+end_ex
  #+begin_lemma latex
  Die natürlichen Matrizennormen zu $\norm{·}_∞$ und $\norm{·}_1$ sind die "maximale Zeilen-/Spaltensumme":
  \begin{align*}
  \norm{A}_∞ &= \max_{j = 1, \dots, n} \sum_{k = 1}^{n} \abs{a_{jk}} \\
  \norm{A}_1 &= \max_{k = 1, \dots, n} \sum_{j = 1}^{n} \abs{a_{jk}}
  \end{align*}
  #+end_lemma
  #+begin_proof latex
  Skript.
  #+end_proof
  Betrachte: $A x = b$ und Störung
  \[\underbrace{(A + δ A)}_{\tilde A}\underbrace{(x + δ x)}_{\tilde x} = \underbrace{b + δ b}_{\tilde b}\]
  #+ATTR_LATEX: :options [Neumann-Reihe]
  #+begin_thm latex
  Gilt $\norm{A} < 1$, so
  \[\mathbb{I - A} \sum_{k= 0}^{∞} A^k = \mathbb{I}\]
  #+end_thm
  #+begin_proof latex
  Für die Partialsummen gilt
  \[(\mathbb{I} - A) \sum_{k = 0}^{n} A^k = \mathbb{I} - A + A - A^2 + A^2 \dots - A^{n + 1} \xrightarrow{n \to ∞} \mathbb{I}\]
  wegen $\norm{A^k} \leq \norm{A}^k \xrightarrow{k \to ∞} 0$.
  #+end_proof
  Wiederholung: Kondition numerischer Aufgabe $y = f(x), y ∈ ℝ^n, x ∈ ℝ^m$.
  \[\frac{Δy_i}{y_i} \overset{·}{=} \sum_{j = 1}^{m} \pp{f_i}{x_j}(x) \frac{Δ x_j}{y_i} = \sum_{j = 1}^{m} \underbrace{\pp{f_i}{x_j} \frac{x_j}{f_i(x)}}_{=: k_{ij}(x)} \frac{Δ x_j}{x_j}\]
  Neumann-Reihe:
  \[\norm{A} < 1 ⇒ (\mathbb{1} - A_a)^{-1} = \sum_{n = 0}^{∞} A^n\]
  Natürliche Matrixnorm:
  \[\norm{A} = \sup_{\norm{x} = 1} \norm{A x}\]
  $\norm{A}_∞$: "Zeilensummennorm" \\
  $\norm{A}_1$: "Spaltensummennorm" \\
  Euklidisches Skalarprodukt auf $\mathbb{K}$
  \[(x, y)_2 = \bar y^T x\]
  #+ATTR_LATEX: :options [Spektralnorm]
  #+begin_lemma latex
  Für $A . \mathbb{K}^{n × n}$ ist
  \[\norm{A}_2 = \max\{\sqrt{\abs{λ}} \mid λ \text{ Eigenwert von } \bar A^T A\}\]
  Für hermitesche $A = \bar A^T$ gilt:
  \[\norm{A}_2= \max \{\abs{λ} \mid λ \text{ Eigenwert von } A\}\]
  #+end_lemma
  #+begin_proof latex
  $B = \bar A^T A$ ist hermitesch. $⇒ B$ hat $n$ reelle Eigenwerte $λ_1, \dots, λ_n$ und eine Orthonormalbasis von Eigenvektoren $\{w_1, \dots, w_n\} ⊂ \mathbb{K}^n$
  $B ω_i = λ_i ω_i$. Jedes $x ∈ \mathbb{K}^n$ hat eine eindeutige Darstellung
  \[x = \sum_{i = 1}^{n} α_i ω_i\]
  \begin{align*}
  ⇒ \norm{x}_2^2 &= (x, x)_2 = \sum_{i, j = 1}^{n} α_i \bar α_j \underbrace{(ω_i, ω_j)_2}_{δ_{ij}} = \sum_{i = 1}^{n} \abs{α_i}^2 \\
  \norm{A x}_2^2 &= (Bx, Bx)_2 = \sum_{i, j = 1}^{n} λ_i α_i \overline{(λ_j α_j)} \underbrace{ω_i, ω_j}_{δ_{ij}} \\
  &= \sum_{i = 1}^{n} \abs{λ_i}^2 \abs{α_i}^2 \\
  \norm{B}_2^2 &= \sup_{x ∈ \mathbb{K}^n \setminus{\{0\}}} \frac{\norm{B x}_2^2}{\norm{x}_2^2} = \sup_{x ∈ \mathbb{K}^n \setminus{\{0\}}} \frac{\sum_{i = 1}^{n} λ_i^2 \abs{α_i}^2}{\sum_{i = 1}^{n} \abs{α_i}^2} \\
  &\leq \max_{i = 1, \dots, n} \abs{λ_i}^2
  \end{align*}
  Mit
  \begin{align*}
  \abs{λ_i} &= \abs{λ_i} \norm{ω_i}_2 = \norm{λ_i ω_i}_2 = \norm{B ω_i}_2 \\
  &\leq \norm{B}_2 \norm{ω_i}_2 = \norm{B}_2, \quad i = 1, \dots, n
  \end{align*}
  #+end_proof
  Betrachte $A x = b$ und Störung
  \[\underbrace{(A + δA)}_{\tilde A} \underbrace{(x + δ x)}_{\tilde x} = \underbrace{b + δ b}_{\tilde b}\]
  #+ATTR_LATEX: :options [Störungssatz]
  #+begin_thm latex
  Die Matrix $A ∈ \mathbb{K}^{n × n}$ sei regulär uns es sei
  \[\norm{δ A} \leq \frac{1}{\norm{A^{-1}}}\]
  Dann ist die gestörte Matrix $\tilde A = A + δ A$ ebenfalls regulär. Für den relativen Fehler der Lzsung gilt mit die Konditionszahl von $A$
  \[\cond(A) = \norm{A} \norm{A^{-1}}\]
  die Ungleichung
  \[\frac{\norm{δ x}}{\norm{x}} \leq \frac{\cond(A)}{1 - \cond(A) \frac{\norm{δ A}}{\norm{A}}} \left[\frac{\norm{δ b}}{\norm{b}} + \frac{\norm{δ A}}{\norm{A}}\right]\]
  #+end_thm
  #+begin_proof latex
  \[\norm{A^{-1} δ A} \leq \norm{A^{-1}}\norm{δ A} < 1\]
  Neumann $⇒$ $A + δ A = A[\mathbb{1d} + A^{-1} δ A]$ ist regulär. \\
  $(A + δ A) \tilde x = b + δ  b, (A + δ A) x = b + δ A x$
  \[⇒ (A + δ A) δ x= δ b - δ A x\]
  \begin{align*}
  \norm{(A + δ A)^{-1}} &= \norm{[A(\mathbb{1} + A^{-1})]^{-1}} \\
  &= \norm{(\mathbb{1} + A^{-1} δ A)^{-1} A^{-1}} \leq \norm{\sum_{n = 0}^{∞}(-A^{-1} δ A)^n} \norm{A^{-1}} \\
  &\leq (\sum_{n = 0}^{∞} \norm{A^{-1} S A}) \norm{A^{-1}} = \frac{1}{1 - \norm{A^{-1} δ A}} \norm{A^{-1}}
  \end{align*}
  \begin{align*}
  \norm{b} &= \norm{A x} \leq \norm{A} \norm{x} \\
  \norm{δ x} &\leq \norm{(A + δ A)^{-1}}[\norm{ δ b} + \norm{δ A} \norm{W}] \\
  &\leq \frac{\norm{A^{-1}}}{1 - \norm{A^{-1} δ a}}[\norm{δ B} \norm{b A} \norm{x}] \\
  &\leq \frac{\norm{A^{-1}}}{1 - \norm{A^{-1}}\norm{ δ A} \norm{A} \norm{A}^{-1}} \left[\frac{\norm{ δ b}}{\norm{x}} + \frac{\norm{S A}}{\norm{A}}\right]
  \end{align*}
  \[\frac{\cond(A)}{1 - \cond (A) \frac{\norm{δ A}}{\norm{A}}}\left[\frac{\norm{ S b}}{\norm{b}} + \frac{\norm{δ A}}{A}\right] \norm{x}\]
  #+end_proof
  Ist $\cond(A) \norm{δ A} \ll \norm{A_i}$, so gilt
  \[\frac{\norm{δ x}}{\norm{x}} \overset{·}{\leq} \cond (A) \left[\frac{\norm{δ b}}{\norm{b}} + \frac{\norm{δ A}}{A}\right]\]
  Die Konditionszahl hängt von der verwendeten Norm ab.
  #+begin_ex latex
  1. $\cond_∞(A) = \norm{A}_∞ \norm{A^{-1}}_∞$
  2. Für die Spektralnorm gilt:
	 \[\cond_2 (A) = \norm{A}_2 \norm{A^{-1}} = \sqrt{\frac{\abs{μ_{max}}}{\abs{μ_{min}}}}\]
	 wobei $μ_{max}, μ_{nin}$ Betragsgößter beziehungsweise kleinster Eigenvektor von	$\bar A^T A$. Ist $A = A=$. Ist $A = \bar A^T$ so gilt:
	 \[\cond_2(A) = \frac{\abs{λ_{max}}}{\abs{λ_{min}}}\]
	 mit $λ_{max}$ und $λ_{min}$ Betrasgrößter beziehungsweise kleinster Eigenvektor von $A$. Regel: Es gelte $\cond (A) \approx 10^s$
	 \[\frac{\norm{δ A}}{\norm{A}} \approx 10^{-k}, \frac{\norm{δ b}}{\norm{b}} \approx 10^{-k}\]
	 Dann muss einn relativer Fehler von
	 \[\frac{\norm{δ x}}{\norm{x}} \approx 10^{s - k}\]
	 erwartet werden. Mit $\norm{·} = \norm{·}_∞$ verliert man $s$ Stellen Genauigkeit.
  #+end_ex
  #+begin_ex latex
  \[A = \begin{pmatrix}1 & 1 \\ 0 & ε\end{pmatrix}, ε ∈ \string(0, 1], A^{-1} = \begin{pmatrix}1 & -ε^{-1} \\ 0 & ε^{-1}\end{pmatrix}\]
  \begin{align*}
  ⇒ \norm{A}_∞ &= 2, \norm{A^{-1}}_∞ = 1 + ε^{-1} \\
  ⇒ \cond_∞ \norm{A} \norm{A^{-1}} = 2 + ε^{-1}
  \end{align*}
  für $ε = 10^{-8}$ kann man bereits 8 Stellen Genauigkeit verlieren.
  #+end_ex
  Ist die Abschätzung im Störungssatz scharf? Sei $A ∈ ℝ^{n × n}$ symmetrisch posisiv definit mit Eigenwerten $λ_1 \geq \dots \geq λ_n$.
  Wähle: $δ A = 0, b = ω_1, δ  B = ε w_k, ε \neq 0$
  \begin{align*}
  A x &= b ⇒ x = \frac{1}{λ_1} w_1 \\
  A \tilde x &= b + δ b ⇒ \tilde x = \frac{1}{λ_1} ω_1 + ε \frac{1}{λ_k} ω_k \\
  ⇒ \frac{\norm{δ x}_2}{\norm{x}_2} &= \abs{ε} \frac{λ_1}{λ_n} \frac{\norm{ω_n}_2}{\norm{ω_1}_2} \\
  &= \cond(A) \frac{\norm{δ b}_2}{\norm{b}_2}
  \end{align*}
** Eliminationsverfahren
   Direkte Methode zur Lösung von $A x = b, A ∈ ℝ^{n × n}$. Spezialfall: $A$ obere Dreichsmatrix $a_{ij} = 0, i > j$
   \[\begin{pmatrix}a_{11} & \dots & \dots & \dots & a_{1n} \\ 0 & a_{22} &   &   &  \vdots \\ \vdots & 0 & \ddots &   & \vdots \\ \vdots & \vdots & \ddots & \ddots & \vdots \\ 0 & 0 & \dots & 0 & a_{nn}\end{pmatrix} \begin{pmatrix}x_1 \\ \vdots \\ \vdots \\ \vdots \\ x_n\end{pmatrix} = \begin{pmatrix}b_1 \\ \vdots \\ \vdots \\ \vdots \\ b_n\end{pmatrix}\]
   Ist $a_{ii} \neq 0, i = 1, \dots, n$ löst man durch Rückwärtseinsetzen
   \[x_j = \begin{cases} \frac{b_n}{a_{nn}} & j = n \\ \frac{1}{a_{jj}}(b_j - \sum_{k = j + 1}^{n} a_{jk} x_k) & j= n - 1, \dots, 1 \end{cases}\]
   Arithmetische Operationen:
   \[\sum_{j = 1}^{n} j = \frac{(n + 1) n}{2} = \frac{n^2}{2} + \mathcal{O}(n)\]
   Eine	Operation: eine Division oder eine Multiplikation und eine Addition.
   Wiederholung: Konditionszahl einer Matrix
   \[A ∈ ℝ^{n × n}: \cond(A) = \norm{A} \norm{A^{-1}}\]
   Störungssatz: $(A + δA)(x + δx) = b + δb$
   \[\frac{\norm{δ x}}{x} \leq \frac{\cond(A)}{1 - \cond(A) \frac{\norm{δ A}}{\norm{A}}}[\frac{\norm{δ A}}{\norm{A}} + \frac{\norm{δ b}}{\norm{b}}]\]
   *Gaußsches Eliminationsverfahren* \\
   Umformung von $A x = b$ auf $R x = c$ mit $R$ obere Dreiecksmatrix mittels
   - Vertauschen von Gleichungen
   - Addition von Vielfachen einer Gleichung zu einer anderen
   Annahme: $A$ hat Vollrang
   0. [@0] Setze $A^{(0)} = A, b^{(0)} = b$
      \[\left[\begin{array}{ccc|c} a_{11}^{(0)} & \dots & a_{1n}^{(0)} & b_{1}^{(0)} \\ \vdots & & \vdots & \vdots \\ a_{n1}^{(0)} & \dots & a_{nn}^{(0)} & b_{n}^{(0)}\end{array}\right]\]
   1. Wähle $r ∈ \{1, \dots, n\}$ mit $a_{r1}^{(0)} \neq 0$ (Pivotelement) und vertausche 1. und r-te Zeile
      \[\left[\begin{array}{ccc|c} \tilde a_{11}^{(0)} & \dots & \tilde a_{1n}^{(0)} & \tilde b_{1}^{(0)} \\ \vdots & & \vdots & \vdots \\ \tilde a_{n1}^{(0)} & \dots & \tilde a_{nn}^{(0)} & \tilde b_{n}^{(0)}\end{array}\right] := [\tilde A^{(0)} \mid \tilde b^{(0)}]\]
   2. Für $j = 2, \dots, n$ eleminiere $\tilde a_{j1}^{(0)}$ durch Subtraction von $ \frac{\tilde a_{j1}^{(0)}}{\tilde a_{11}^{(0)}} := q_{j1}$
	  mal der ersten Zeile von Zeilen $2, \dots, n$:
      \[\left[\begin{array}{cccc|c} \tilde a_{11}^{(0)} & \tilde a_{12}^{(0)} \dots & \tilde a_{1n}^{(0)} & \tilde b_{1}^{(0)} \\ 0 & \tilde a_{22}^{(1)} \dots & \tilde a_{2n}^{(1)} & \tilde b_{2}^{(1)} \\ \vdots & \vdots & & \vdots & \vdots \\ 0 & \tilde a_{n2}^{(1)} & \dots & \tilde a_{nn}^{(1)} & \tilde b_{n}^{(1)}\end{array}\right] := [A^{(1)} \mid b^{(1)}]\]
	  Fahre fort auf kleinerem System $⇒ [A^{(0)} \mid b^{(0)}] \to [A^{(1)}\mid b^{(1)}] \to \dots \to [A^{(n - 1)} \mid b^{(n - 1)}] =: [R\mid c]$
   Wird im $k$ -ten Schritt $[A^{(k - 1)} \mid b^{(n - 1)}] \to [\tilde A^{(k - 1)} \mid \tilde b^{(n - 1)}] \to [A^{(k)} \mid b^{(k)}]$ das Pivot-Element $q_{r_k k}^{k - 1}$ gewählt, so gilt
   $[\tilde A^{(k -1 )} \mid \tilde b^{(k - 1)}] = P_k[A^{(k - 1)} \mid b^{(k - 1)}]$ mit der Permutationsmatrix
   \begin{equation*}
   P_k = \begin{pmatrix}
   1 & & & & & & & & & & \\
   & \ddots & & & & & & & & & \\
   & & 1& & & & & & & & \\
   & & & 0 & \dots & \dots & \dots & 1 & & & \\
   & & & & 1 & & & & & & \\
   & & & & & \ddots& & & & & \\
   & & & & & & 1 & & & & \\
   & & & 1 & \dots & \dots & \dots & 0 & & & \\
   & & & & & & & & 1 & & \\
   & & & & & & & & & \dots & \\
   & & & & & & & & & & 1 \\
   \end{pmatrix} \quad
   G_k = \begin{pmatrix}
   1 & & & & \\
   & \ddots & & & \\
   & -q_{k + 1, k}^{(k)} & 1& & \\
   & \vdots & & \ddots & \\
   & -q_{n,k}^{(k)} & & & 1 \\
   \end{pmatrix}
   \end{equation*}
   Mit den Fehlstellungen von $P_k$ an $k$ und $r_k$ und der Fehlspalte von $G_k$ bei $k$. Weiterhin gilt:
   $[A^{(k)} \mid b^{(k)}] = G_k [\tilde A^{(k - 1)} \mid \tilde b^{(k - 1)}]$ mit
   $q_{jk}^{(k)} = \tilde a_{jk}^{(k - 1)} / \tilde a_{kk}^{(k - 1)}$. $G_k$ heißt Frobenius Matrix.
   Wegen $P_k^{-1} = P_k$ und
   \begin{equation*}
   G_k^{-1} = \begin{pmatrix}
   1 & & & & \\
   & \ddots & & & \\
   & q_{k + 1, k}^{(k)} & 1& & \\
   & \vdots & & \ddots & \\
   & q_{n,k}^{(k)} & & & 1 \\
   \end{pmatrix}
   \end{equation*}
   haben $A x = b$ und $A^{(k)} x = b^{(k)}$ dieselbe Lösung:
   \[A x = b ⇔ A^{(k)} x = G_{n - 1} P_{n - 1} \dots G_1 P_1 A x = G_{n - 1} P_{n - 1} \dots G_1 P_1 b = b^{(k)}\]
   *Wahl des Pivot-Elementes* \\
   Zierl: Numerische Stabilität.
   1. Spaltenpivotierung:
	  \[\abs{a_{r_k, k}^{(k - 1)}} = \max_{j = k, \dots, n} \abs{a_{j k}^{(k - 1)}}\]
   2. Totalpivotierung
	  \[\abs{a_{r_k, s_k}^{(k - 1)}} = \max_{i,j = k, \dots, n} \abs{a_{ij}^{(k - 1)}}\]
	  - bessere Stabilität
	  - teurer
	  - Permutationsmatrizen $Q_k$ für $x$
		\[\underbrace{G_k P_k \dots G_1 P_1 A Q_1 \dots Q_k}_{A^{(k)}} \underbrace{Q_k \dots Q_1 x}_{Q x} = G_k P_k \dots G_1 P_1 b\]
   *Speicherausnutzung* \\
   Die $q_{jk}^{(k)}$ können an den eliminierten Stellen im unteren	Dreieck von $A$ gespeichert werden. Das obere Dreick von $A$ wird während der Rechnung ersetzt.
   Nach $k$ Eliminationsschritten
   \begin{equation*}
   \left[
   \begin{array}{ccccccc|c}
   r_{11} & r_{12} & \dots & r_{1k} & r_{1, k + 1} & \dots  & r_{1n} & c_1 \\
   λ_{21} & r_{22} & \dots & r_{2k} & r_{2, k + 1} & \dots  & r_{2n} & c_2 \\
   λ_{31} & λ_{32} & \ddots &       & \vdots & & \vdots & \vdots \\
   \vdots & \vdots & \ddots &       & \vdots & & \vdots & \vdots \\
   λ_{k1} & \dots  & λ_{kk} & r_{kk} & r_{k, k + 1} & \dots & r_{kn} & c_k \\
   λ_{k1} & \dots  & \dots & λ_{k + 1, k} & a_{k +1, k + 1}^{(k)} & \dots & a_{k + 1, n}^{(k)} & b^{(k)}_{k + 1} \\
   \vdots &        &       & \vdots       & \vdots  & & \vdots & \vdots \\
   λ_{n1} & \dots  & \dots & λ_{n, k} & a_{n, k + 1}^{(k)} & \dots & a_{n, n}^{(k)} & b^{(k)}_{n} \\
   \end{array}
   \right]
   \end{equation*}
   mit $λ_{i + 1, 1}, \dots, λ_{n i}$ Permutationen von $q_{i + 1,i}^{(k)}, \dots , q_{ni}^{(k)}$. Endresultat ($k = n - 1$)
   \begin{equation*}
   \left[
   \begin{array}{cccc|c}
   r_{11} & \dots & \dots & r_{1n} & c_1 \\
   l_{21} & r_{22} & \dots & r_{2n} & \vdots \\
   \vdots & \ddots  & \ddots & \vdots & \vdots \\
   l_{n1} & \dots & l_{n,n-1} & r_{nn} & c_n \\
   \end{array}
   \right]
   \end{equation*}
   #+ATTR_LATEX: :options [LR-Zerlegung]
   #+begin_thm latex
   Die Matrizen
   \begin{equation*}
   L =
   \left[
   \begin{array}{cccc}
   1 & & &   \\
   l_{21} & 1 & & \\
   \vdots & \ddots  & \ddots & \\
   l_{n1} & \dots & l_{n,n-1} & 1 \\
   \end{array}\right], R =
   \left[
   \begin{array}{ccc}
   r_{11} & \dots & r_{1n} \\
   & \ddots & \vdots \\
   & & r_{nn}
   \end{array}
   \right]
   \end{equation*}
   bilden eine LR-Zerlegung der Matrix $PA$. $PA = LR$, mit $P = P_{n - 1} \dots P_{1}$.
   Für $P = \mathbb{1}$ ist die Zerlegung eindeutig.
   #+end_thm
   #+begin_proof latex
   (für $P = \mathbb{1}$).
   \[R = G_{n - 1} \dots G_1 A ⇔ \underbrace{G_1^{-1} \dots G_{n - 1}^{-1}}_{L} R = A\]
   Eindeutigkeit: Übung.
   #+end_proof
   Aufwand:
   $k$ -ter Eliminationsschritt
   \begin{align*}
   a_{ij}^{(k)} &= a_{ij}^{(k - 1)} - \frac{a_{ik}^{(k - 1)}}{a_{kk}^{(k - 1)}} a_{kj}^{(k - 1)} \\
   b_i^{(k)} &= b_i^{(k - 1)} - \frac{a_{ik}^{(k - 1)}}{a_{kk}^{(k - 1)}} b_k^{(k - 1)}
   \end{align*}
   $i, j = k + 1, \dots, n$ $⇒$ $n - k$ Divisionen, $(n - k) + (n - k)^2$ Multiplikationen und Additionen
   \[⇒ N_{\text{Gauß}}(n) = \frac{1}{3} n^3 + \mathcal{n^2}\]
   Girl für Lösung von $Ax = b$ und für die Berechnung der Zerlegung $PA = LR$
   #+begin_ex latex
   \[A = \begin{pmatrix}3 & 1 & 6 \\ 2 & 1 & 3 \\ 1 & 1 & 1\end{pmatrix}, b = \cvec{2; 7; 4}\]
   Pivotierung:
   \begin{equation*}
   \left[
   \begin{array}{ccc|c}
   3 & 1 & 6 & 2 \\
   2 & 1 & 3 & 7 \\
   1 & 1 & 1 & 4
   \end{array}\right] \to
   \left[
   \begin{array}{ccc|c}
   3 & 1 & 6 & 2 \\
   2/3 & 1/3 & -1 & 17/3 \\
   1/3 & 2/3 & -1 & 10/3
   \end{array}\right] \to
   \left[
   \begin{array}{ccc|c}
   3 & 1 & 6 & 2 \\
   1/3 & 2/3 & -1 & 10/3 \\
   2/3 & 1/3 & -1 & 17/3 \\
   \end{array}\right] \to
   \left[
   \begin{array}{ccc|c}
   3 & 1 & 6 & 2 \\
   1/3 & 2/3 & -1 & 10/3 \\
   2/3 & 1/2 & -1/2 & 4 \\
   \end{array}
   \right]
   \end{equation*}
   $\to$
   \begin{align*}
   x_3 &= - 8 \\
   x_2 &= \frac{3}{2}(\frac{10}{3} + x_3) = -7 \\
   x_1 &= \frac{1}{3}(2 - x_2 - 6x_3) = 19
   \end{align*}
   LR-Zerlegung:
   \[P_1 = E_3, P_2 = \begin{pmatrix}1 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 1 & 0\end{pmatrix}\]
   \[PA = \begin{pmatrix}3 & 1 & 6 \\ 1 & 1 & 1 \\ 2 & 1 & 3\end{pmatrix} = LR= \begin{pmatrix}1 & 0 & 0 \\ 1/3 & 1 & 0 \\ 2/3 & 1/2 & 1\end{pmatrix} \begin{pmatrix}3 & 1 & 6 \\ 0 & 2/3 & -1 \\ 0 & 0 & -1/2\end{pmatrix}\]
   Für die numerische Stabilität der Gauß-Elimination ist im Allgemeinen Pivotierung sehr wichtig.
   #+end_ex
   Rückwärtsanalyse nach Wilkinson $A ∈ ℝ^{n × n}$, löse $A x = b$ mit Gauß-Elimination mit Spaltenpivotierung. Die berechnete Lösung $\tilde x$ ist
   die exakte Lösung eines gestörten Systems $(A + δ A) \tilde x = b$ mit
   \[\frac{\norm{δ A}_∞}{\norm{A}_∞} \leq 1.01 · 2^{n - 1}(n^3 + 2 n^2) eps\]
   (ohne Beweis) \\
   Störungssatzt $⇒$
   \[\frac{\norm{δ x}}{\norm{x}} \leq \frac{\cond_∞ (A)}{1 - \cond_∞ (A) \norm{δ A} / \norm{A}} · 1.01 2^{n - 1}(n^3 + 2n^2) eps\]
   Diese Abschätzung deckt pathologische Fälle ab. In der Praxis ist das Verhalet gutartig, das heißt die Gaußelimination mit Spaltenpivotierung ist ein stabiler Algorithmus.
