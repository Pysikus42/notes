#+AUTHOR: Robin Heinemann
#+TITLE: Einführung in die Numerik (Potschka)

#+INCLUDE: "../header.org" :minlevel 1
#+LATEX_HEADER:	\setcounter{section}{-1}

# #+BEGIN_SRC	python
# def fib(n):
#     if n < 2:
#         return 1
#     else:
#         return fib(n - 1) + fib(n - 2)

# for n in range(10):
#     print(fib(n))
# #+END_SRC
# Andreas Potschka: INF 205, Raum 2.418
# Keine Vorlesung an Feiertagen
# - Do 25.05 (Himmelfahrt)
# - Do 15.06 (Fronleichnam)
# Webseite: [[http:]]//goo.gl/dzaGPd
# Klausurtermin: 27.07.2017 14-16 Uhr
# Klausurtermin: 21.09.2017 ? Uhr
# Zulassung: 50% der Punkte der Übungsaufgaben, einmal vorrechnen
# Übungsblatt Donnerstag, Beginn der Übungsgruppen 24.04, Abgabe im Mathematikon

# #+begin_src python :results	file :exports both
# import matplotlib, numpy
# matplotlib.use('Agg')
# import matplotlib.pyplot as plt
# fig = plt.figure(figsize=(4,2))
# x=numpy.linspace(-15,15)
# plt.plot(numpy.sin(x)/x)
# fig.tight_layout()
# plt.savefig('/tmp/test.pgf')
# #+end_src

# #+RESULTS:
# [[file:/tmp/test.png]]

#+INCLUDE: "introduction.org" :minlevel 1
#+INCLUDE: "error_analysis.org" :minlevel 1
#+INCLUDE: "interpolation_approximation.org" :minlevel 1
#+INCLUDE: "numeric_integration.org" :minlevel 1
#+INCLUDE: "linear_systems.org" :minlevel 1
#+INCLUDE: "nonlinear_system.org" :minlevel 1
* Lineare Gleichungssysteme: Iterative Verfahren
  Problem direkter Methoden: Speicheraufwand für große $n$. Alternatives Beispiel: Fixpunktiteration für $A x = b$ ($A ∈ ℝ^{n × n}, b ∈ ℝ^n$)
  \[⇒ a_{jj} x_j + \sum_{\substack{k = 1\\ k \neq j}}^{n} a_{jk} x_k = b_j, j = 1, \dots, n\]
  Ist $a_{jj} \neq 0$
  \[⇔ x_j = \frac{1}{a_{jj}}(b_j - \sum_{\substack{k = 1 \\ k \neq j}}^{n} a_{jk} x_k), j = 1, \dots, n\]
  Gesamtschritt- /Jacobi-Verfahren:
  \begin{align*}
  x^0 &= 0 \\
  x_j^t &= \frac{1}{a_{jj}} (b_j - \sum_{\substack{k = 1\\ k \neq j}}^{n} a_{jk x_k^{t - 1}}) \\
  j &= 1, \dots, n, t = 1, 2, \dots
  \end{align*}
  Einzelschritt- /Gauß-Seidel-Verfahren
  \begin{align*}
  x_j^t &= \frac{1}{a_{jj}}(b_j - \sum_{k < j} a_{jk} x^t_k - \sum_{k > j} a_{jk} x_k^{t - 1}) \\
  j &= 1, \dots, n, t = 1, 2, \dots
  \end{align*}
  Fixpunktiterationen:
  \[A = D + L + R\]
  Jacobi:
  \begin{align*}
  x^t &= D^{-1}(b - (L + R) x^{t - 1}) \\
  &= \underbrace{-D^{-1}(L + R)}_{=: J} x^{t - 1} + D^{-1} b
  \end{align*}
  Gauß-Seidel:
  \begin{align*}
  x^t &= D^{-1}(b - L x^t - Rx^{t - 1}) \\
  ⇔ D x^t + L x^t &= b - R x^{t - 1} \\
  ⇔ x^t &= -(D + L)^{-1} R x^{t - 1} + (D +L)^{-1} b
  \end{align*}
  Gemeinsame Form $x^t = B x^{t - 1} + c$, $B$: Iterationsmatrix. Konvergiert $(x^t)$ gegen $x$, so gilt $x = B x + c$.
  Allgemein: Wähle $C ∈ ℝ^{n × n}$ invertierbar
  \begin{align*}
  A x = b &⇔ C x = C x - A x + b \\
  &⇔ x = x + C^{-1}(b - A x)
  \end{align*}
  Form der Fixpunktiteration:
  \[x^t = \underbrace{(E_n - C^{-1} A)}_{=: B} x^{t - 1} + \underbrace{C^{-1} b}_{=: c}\]
  Defektkorrekturiteration:
  \begin{align*}
  d^{t - 1} &= b - A x^{t - 1}, C δ x^{t - 1} = d^{t - 1} \\
  x^t &= x^{t - 1} + δ x^{t - 1}
  \end{align*}
  Erinnerung: Lokaler Kontraktionssatz:
  \[κ = \norm{E_n - C^{-1} A} < 1\]
  $⇒$ Konvergenz für beliebige Startwerte $(ω = 0)$. Problem: $κ$ ist Norm-abhängig. "Schärfere" Alternative
  \[\spr(B) = \max\{\abs{λ} \mid λ ∈ σ(B)\}\]
  $σ(B) ⊂ ℂ$: Menge der Eigenwerte von $B$ ($B x = λ x, λ ∈ ℂ, x ∈ ℂ^n, x \neq 0$).
  Achtung: $\spr(B)$ ist keine Norm. Betrachte
  \[\spr(\begin{pmatrix}0 & 1 \\ 0 & 0\end{pmatrix}) = 0\]
  aber dies ist nicht die Nullmatrix. Für natürliche Matrizennormen gilt
  \[\norm{B} = \sup_{x ∈ ℂ^n \setminus \{0\}} \frac{\norm{B x}}{\norm{x}} \geq \abs{λ}\]
  mit $λ$ ein Eigenwert, wählen $x$ als den zugehörigen Eigenvektor.
  \[⇒ \spr(B) \leq \norm{B}\]
  #+begin_lemma latex
  Für jede $B ∈ ℝ^{n × n}$ gibt es zu jedem $ε > 0$ eine natürliche Matrizennorm $\norm{·}_ε$, sodass
  \[\spr(B) \leq \norm{B}_ε \leq \spr(B) + ε\]
  #+end_lemma
  #+begin_proof latex
  Schnur-Zerlegung $B = T^{-1} R =, T ∈ ℂ^{n × n}$, unitär
  \[R = \begin{pmatrix}r_{11} & \dots & r_{1n} \\   & \ddots & \vdots \\ 0 &   & r_{nn}\end{pmatrix}\]
  \[⇒ \spr(B) = \spr(R) = \max_{j = 1, \dots, n} \abs{r_{jj}}\]
  Für beliebige $δ ∈ \string(0, 1]$, wähle
  \[S_δ = \diag(δ^0, δ^1, \dots, δ^{n - 1})\]
  \begin{align*}
  R_0 &= \diag{r_{11}, r_{22}, \dots, r_{nn}} "\\
  Q_δ &= \begin{pmatrix}0 & r_{12} & δr_{13} & \dots & δ^{n - 2}r_{1n} \\   & \ddots & \ddots & \ddots & \vdots \\   &   & \ddots & \ddots & δr_{n - 2, n} \\   &   &  & \ddots & r_{n - 1, n} \\ 0 &   &   &   & 0\end{pmatrix} \\
  R_δ &= S^{-1}_δ R D_δ = \begin{pmatrix}r_{11} & δ r_{12} & δ^2 r_{13} & \dots & δ^{n - 1}r_{1n} \\   & \ddots & \ddots & \ddots & \vdots \\   &   & \ddots & \ddots & δ^2 r_{n - 2, n} \\   &   &  & \ddots & δ r_{n - 1, n} \\ 0 &   &   &   & r_{n, n}\end{pmatrix} \\
  ⇒ R_δ &= R_0 + δ Q_δ
  \end{align*}
  $S^{-1}_δ T$ invertierbar
  \[⇒ \norm{x}_δ = \norm{S_δ^{-1} T x}_2\]
  ist Vektornorm auf $ℝ^n$. Mit $B = T^{-1} R T = T^{-1} S_δ R_δ S^{-1}_δ T$ und $y = S_δ^{-1} T x$ folgt
  \begin{align*}
  \norm{B x}_δ &= \norm{T^{-1} S_{δ} R_δ S_δ^{-1} T x}_δ \\
  &= \norm{R_δ y}_2 \leq \norm{R_0 y}_2 + δ\norm{Q_δ y}_2 \\
  &\leq (\max_{i = 1, \dots, n} \abs{r_{ii}} + δ μ) \norm{y}_2 \\
  &= (\spr(B) + δ μ)\norm{x}_δ \\
  \intertext{mit}
  μ &= (\sum_{i, j = 1}^{n} \abs{r_{ij}})^{1/2} \\
  \norm{B}_δ &= \sup_{x ∈ ℝ^n \setminus \{0\}} \frac{\norm{B x}_δ}{\norm{x}_δ} \\
  &\leq \spr(B) + δ μ
  \end{align*}
  Wähle $δ = ε / μ$
  #+end_proof
  #+ATTR_LATEX: :options [Fixpunktiteration]
  #+begin_thm latex
  Die durch
  \[x^t = B x^{t - 1} + c\]
  erzeugten Iterierten konvergieren genau dann für jeden Startwert $x^0 ∈ ℝ^n$ gegen die Lösung von $x = B x + c$, wenn $\spr(B) < 1$. Asymptotisches Konvergenzverhalten:
  \[\sup_{x_0 ∈ ℝ^n} \limsup_{t \to ∞} (\frac{\norm{x^t - x}}{x^0 - x})^{1/t} = \spr(B)\]
  #+end_thm
  #+begin_proof latex
  Fehler:
  \begin{align*}
  e^t &:= x^t - x = B x^{t - 1} + c - B x - c = B e^{t  1} \\
  ⇒ e^t &= B^t e^0, t ∈ ℕ
  \end{align*}
  1. $\spr(B) < 1$. Sei $ε < 1 - \spr(B)$
	 \[⇒ ∃ \norm{·}_ε: \norm{B}_ε \leq \spr(B) + ε < 1\]
	 \[\norm{e^t}_ε = \norm{B^t e^0}_ε \leq \norm{B}_ε^t \norm{e^0}_ε \xrightarrow{t \to ∞} 0\]
	 $⇒ x^t \to x$ für $x \to ∞$
  2. (Beweis für Fall $B ω = λ ω, \abs{λ} = \spr(B), ω ∈ ℝ^n \setminus \{0\}$). Konvergenz für jeden Startwert. Wähle $x^0 = x + w$
	 \[λ^t ω = B^t ω = B^t e^ = e^t \to 0\]
	 $⇒ \abs{λ} < 1 ⇒ \spr(B) < 1$. Weiterhin:
	 \[(\frac{\norm{e^t}}{\norm{e^0}})^{1 / t} = \abs{λ}\]
  3. Norm Äquivalenz: $∃m, M > 0$, sodass
	 \begin{align*}
	 m \norm{x} &\leq \norm{x}_ε \leq M \norm{x}\quad x ∈ ℝ^n \\
	 ⇒ \norm{e^t} &\leq \frac{1}{m} \norm{e^t}_ε \leq \frac{1}{m} \norm{B}_ε^t \norm{e^0}_ε \\
	 &\leq \frac{M}{m}(\spr(B) + ε)^t \norm{e^0}
   	 \end{align*}
	 Wegen
	 \[(\frac{M}{m})^{1 / t} \xrightarrow{t \to ∞} 1\]
	 \[\limsup_{t \to ∞} (\frac{\norm{e^t}}{\norm{e^0}})^{1/t} \leq \sup(B) + ε \xrightarrow{ε \to 0} \spr(B)\]
  #+end_proof
  Wiederholung: $A x = b, A ∈ ℝ^{n × n}, b ∈ ℝ^n$
  - Fixpunktiteration: $x^{t + 1} = B x^t + c$ konvergiert genau dann $∀ x^0$, wenn $\spr(B) < 1$
  - Jacobi: $B = J = -D^{-1}(L + R)$ wobei $A = A + L + R$
  - Gauß-Seidel $B = H_1 = -(D + L)^{-1} R$
  -	Asymptotische Konvergenzrate:
	\[\sup_{x^0 ∈ ℝ^n} \lim_{t \to ∞} (\frac{\norm{e^t}}{\norm{e^0}})^{1/t} = \spr B\]
  Interpretation: Gewinn von $k$ Dezimalstellen (für große $t$) $ρ = \spr(B)$. Bestimme $t$ so, dass
  \[ρ^t \leq 0,1^t ⇒ t \log_{10} ρ \leq - k ⇒ t \geq - \frac{k}{\log_{10}ρ}\]
  #+ATTR_LATEX: :options [$ρ = 0.99, k = 1$]
  #+begin_ex latex
  $t = 230$,
  #+end_ex
  Konstruktion von Iterationsverfahren: Zwei Ziele (Gegenspieler)
  1. $\spr(\underbrace{E_n - C^{-1}A}_{B})$ klein
  2. $C δ x^{t - 1} = d^{t_ 1}$ leicht lösbar
  Jacobi- und Gauß-Seidel_verfahren
  #+ATTR_LATEX: :options [Starke Zeilensummenkriterium]
  #+begin_thm latex
  Ist $A ∈ ℝ^{n × n}$ strikt diagonaldominant
  \[\sum_{\substack{k = 1\\ k \neq j}}^{n} \abs{a_{j k}} < \abs{a_{jj}}, j = 1, \dots, n\]
  so ist $\spr(J) < 1$ und $\spr(H_1) < 1$ das heißt Jacobi- und Gauß-Seidel-Verfahren konvergieren.
  #+end_thm
  #+begin_proof latex
  $0 < \abs{a_{jj}}$. Sei $λ ∈ σ(J)$ und $μ ∈ σ(H_1)$ mit Eigenvektoren $v, w ∈ ℂ^n$
  \[\norm{v}_∞ = \norm{w}_∞ = 1\]
  das heißt
  \[λ v = J v = - D^{-1}(L + R) v\]
  und
  \begin{align*}
  μ w &= H_1 w = -(D + L)^{-1} R w \\
  ⇔ μ w &= - D^{-1}(μ L + R) w \\
  ⇒	\abs{λ} &\leq \norm{D^{-1}(L+ R)}_∞ \\
  &= \max_{j = 1, \dots, n} \{\frac{1}{\abs{a_{jj}}} \sum_{\substack{k = 1\\ k \neq j}}^{n} \abs{a_{jk}}\} < 1 \\
  \abs{μ} &\leq \norm{D^{-1}(μ L + R)}_∞ \\
  &\leq \max_{j = 1, \dots, n} \{\frac{1}{\abs{a_{jj}}} (\sum_{k < j} \abs{μ} \abs{a_{jk}} + \sum_{k > j} \abs{a_{jk}})\}
  \end{align*}
  wäre $\abs{μ} > 1$, so würde
  \[\abs{μ} \leq \abs{μ} \norm{D^{-1}(L + R)}_∞ < \abs{μ} \lightning\]
  #+end_proof
  Die Voraussetzungen können abgeschwächt werden (siehe Skript). \\
  SOR-Verfahren (Successive Overrelaxation) \\
  \begin{align*}
  \tilde x^t &= \frac{1}{a_{jj}}(b_j - \sum_{k < j} a_{jk} \tilde x_k^t - \sum_{k > j} a_{jk} x_k^{t - 1}), j = 1, \dots, n \\
  x^t &= ω \tilde x^t + (1 - ω) x^{t - 1}, ω \geq 1
  \end{align*}
  Für $ω = 1$ ist SOR gleich Gauß-Seidel ($ω < 1$: Unterrelaxation)
  \begin{align*}
  x^t &= -ω(D + L)^{-1} R x^{t - 1} + (1 - ω)x^{t - 1} + ω(D + L)^{-1} b \\
  H_ω &= (D + ω L)^{-1}((1 - ω)D - ω R)
  \end{align*}
  #+begin_lemma latex
  Für $A ∈ ℝ^{n × n}$ mit $D$ regulär gilt
  \[\spr(H_ω) \geq \abs{ω - 1}, ω ∈ ℝ\]
  #+end_lemma
  #+begin_proof latex
  \begin{align*}
  H_ω &= (E_n - ω \underbrace{D^{-1} L}_{L'})^{-1} B^{-1} D D((1 - ω) E_n - ω \underbrace{D^{-1} R}_{R'}) \\
  \det(H_ω) &= \det(E_n - ω L') · \det((1 - ω) E_n - ω R') = (1 - ω)^2
  \end{align*}
  Wegen
  \[\det(H_ω) = \prod_{λ ∈ σ(H_ω)} λ\]
  folgt
  \begin{align*}
  \spr(H_ω) &= \max_{λ ∈ σ(H_ω)} \abs{λ} \geq (\prod_{λ ∈ σ(H_ω)} \abs{λ})^{1/n} \\
  &= \abs{1 - ω}
  \end{align*}
  #+end_proof
  #+ATTR_LATEX: :options [SOR]
  #+begin_thm latex
  Sei $A ∈ ℝ^{n × n}$ symmetrisch positiv definit. Dann gilt
  \[\spr(H_ω) < 1 ∀ ω ∈ (0, 2)\]
  Insbesondere konvergiert Gauß-Seidel.
  #+end_thm
  #+begin_proof latex
  $A$ symmetrisch $⇒ R = L^T$. $A = D + L + L^T$. Sei $λ ∈ σ(H_ω)$, $ω ∈ (0, 2)$ mit Eigenvektor $v ∈ ℝ^n \setminus \{0\}$, das heißt
  $H_ω v = λ v$
  \begin{align*}
  ⇒((1 - ω)D - ω L^T) v &= λ(D + ω L) v \\
  ⇒ ω(D + L^T)v &= (1 - λ) D v + λ ω L v \\
  ⇒ ω A v &= λ ω(D + L^T) v + ω L v
  \end{align*}
  und
  \begin{align*}
  λ ω A v &= λ ω (D + L^T) v + λ ω L v \\
  &= λ ω (D + L^T) v + (1 - λ) D v - ω(D + L^T) v \\
  &= (λ - 1) ω(D + L^T) v + (1 - λ)D v \\
  &= (1 - λ)(1 - ω) D v - (1 - λ)ω L^T v
  \end{align*}
  Wegen $vωT L v = v^T L^T v$ folgt
  1. $ω v^T A v = (1 - λ)v^T D v + ω(1 - λ)v^T L v$
  2. $λ ω v^T A v = (1 - λ)(1 - ω) v^T D v - (1 - λ)ω v^T L v$
  \[⇒ (1 + λ) ω v^T A v = (1 - λ)\underbrace{(2 - ω)}_{> 0} v^T D v\]
  $A$ positiv definit $⇒$ $D$ positiv definit. Also: $v^T A v > 0, v^T D v > 0$. $⇒ λ \neq \pm 1$ und
  \[μ := \frac{1 + λ}{1 - λ} = \frac{2 - ω}{w} \frac{v^T D v}{v^T A v} > 0\]
  \[⇒(1 - λ)μ = (1 + λ)\]
  \[(1 + μ)λ = -(1 - μ)\]
  \[⇒ \abs{λ} = \abs{\frac{μ - 1}{μ + 1}} < 1\]
  #+end_proof
  Wiederholung: SOR $A x = b, A = D + L + R$
  \[x_j^t = (1 - ω)x_j^{t - 1} + \frac{ω}{a_{jj}}(b_j - \sum_{k < j} a_{jk} x_k^t - \sum_{k > j} a_{jk} x_k^{t - 1})\quad j = 1, \dots, n\]
  \begin{align*}
  ⇒ x^t &= (1 - ω)x^{t - 1} + ω D^{-1}(b - L x^t - R x^{t - 1}) \\
  ⇒ (D + ω L) x^t &= ((1 - ω)D - ω R)x^{t - 1} + ωb \\
  ⇒ x^t &= \underbrace{(D + ω L)^{-1}((1 - ω)D - ω R)}_{H_{ω}}x^{t - 1} + ω(D + ω L)^{-1} b
  \end{align*}
  - SOR konvergiert für $A$ symmetrisch positiv definit $ω ∈ (0, 2)$
  - $ω = 1$: Gauß-Seidel
  -	$ω$ optimal ist schwer zu finden
  *Abstiegsverfahren* \\
  Vorraussetzung: $A$ symmetrisch, positiv definit.
  \begin{align*}
  ⇒ (A x, y)_2 &= (x, A y)_2 ∀ x, y ∈ ℝ^n \\
  \string(A x, x\string)_2 &> 0 ∀ x ∈ ℝ^{n} \setminus \{0\}
  \end{align*}
  #+ATTR_LATEX: :options [\(A\)-Skalarprodukt, \(A\)-Norm]
  #+begin_defn latex
  \[(x, y)_A = (A x, y), \norm{x}_A = \sqrt{(A x, x)}\]
  #+end_defn
  Erinnerung: $A$ hat nur reelle Eigenwerte
  \[0 < λ := λ_1 \leq \dots \leq λ_n =: Λ\]
  und die Eigenvektoren $\{ω_1, \dots, ω_n\} ⊂ ℝ^n$
  sind eine Orthonormalbasis von $ℝ^n$
  \[⇒ \spr(A) = Λ, \cond_2(A) = \frac{Λ}{λ}\]
  #+begin_thm latex
  Sei $A ∈ ℝ^{n × n}$ symmetrisch, positiv definit. Dann gilt $A x = b$ genau dann, wenn
  \[Q(x) \leq Q(y) ∀ y ∈ ℝ^{n} \setminus \{x\}\]
  mit
  \[Q(y) = \frac{1}{2}(Ay, y) - (b, y)\]
  #+end_thm
  #+begin_proof latex
  1. Sei $A x = b$ für $x \neq y$ folgt
	 \begin{align*}
	 Q(y) - Q(x) &= \frac{1}{2}((A y, y) - 2(b, y) - (Ax, x) + 2(b, x)) \\
	 &= \frac{1}{2}((A y, y) - 2(A x, y) + (A x, x)) \\
	 &= \frac{1}{2}(A(x - y), x - y) > 0
   	 \end{align*}
  2. $x$ ist Minimum von $Q ⇒ \grad Q(x) = 0$
	 \begin{align*}
	 \pp{Q}{x_i}(x) &= \frac{1}{2}\pp{}{p_i} \sum_{j,k = 1}^{n} a_{jk} x_j x_k - \pp{}{x_i} \sum_{k = 1}^{n} b_k x_k \\
	 &= \sum_{k = 1}^{n} a_{ik} x_k - b_i = 0\quad i = 1, \dots, n \\
  	 ⇒ A x &= b \\
	 ⇒ \grad Q(y) &= \frac{1}{2}(A + A^T) y - b = A y - b \tag{negativer Defekt}
     \end{align*}
  #+end_proof
  Iteration:
  \[x^{t + 1} = x^t + α_t r^t\]
  mit Abstiegsrichtung $r^t ∈ ℝ^n$ \\
  und Schrittweite $α_t ∈ ℝ$. Schrittweitenbestimmung: zum Beispiel Liniensuche
  \begin{align*}
  Q(x^{t + 1}) &= \min_{α ∈ ℝ} Q(x^t + α r^t) \\
  ⇒ 0 &\overset{!}{=} \dd{}{α}Q(x^t + α r^t) \\
  &= \grad Q(x^t + α r^t) r^t \\
  &= (A(x^t + α r^t) - b, r^t) \\
  &= (A x^t - B, r^t) + α (A r^t, r^t) \\
  ⇒ αt &= - \frac{(^t, r^t)_2}{(r^t, r^t)_A} \\
  g^t &:= \grad Q(x^t) = A x^t - b
  \end{align*}
  #+ATTR_LATEX: :options [Allgemeines Abstiegsverfahren]
  #+begin_defn latex
  Gegeben $x^0 ∈ ℝ^n$
  - Gradint $g^t = A x^t - b$, Abstiegsrichtung $r^t$
  - Schrittweite
	\[α_t = - \frac{(g^t, r^t)}{(A r^t, r^t)}\]
  - Iteration: $x^{t + 1} = x^t + α_l r^t$
  Ökonomischer:
  \begin{align*}
  g^0 &= A x^0 - b \\
  t \geq 0 : α_t &= \frac{(g^t, r^t)}{(A r^t r^t)} \\
  x^{t + 1} &= x^t + α_t r^t \\
  g^{t + 1} &= g^t + α_l A r^t
  \end{align*}
  #+end_defn
  Beobachtung:
  \begin{align*}
  \norm{y - x}_A^2 - \norm{x}_A^2 = (A(y - x), y - x) - (A x, x) \\
  &= (A(y - x), A^{-1}A(y - x)) - (A x, A^{-1} A x) \\
  &= \norm{A y - b}_{A^{-1}}^2 - \norm{b}_{A^{-1}}^2 \\
  &= (A y, y) - (A y, x) - (A x,y) \\
  &= (A y, y) - 2(b, y) = 2 Q(y)
  \end{align*}
  $⇒$ Minimierung von $Q$ minimiert Defektnorm $\norm{A y - b}_{A^{-1}}$ und Fehlernorm $\norm{y - x}_A$. \\
  Gradientenverfahren: Richtung des	steilsten Abstiegs
  \[r^t = -\grad Q(x^t) = - g^t\]
  Iteration: $x^0 ∈ ℝ^n, g^0 = A x^0 - b$. $t \geq 0$:
  \begin{align*}
  α_t &= \frac{\norm{g^t}^2}{(A g^t, g^t)} \\
  x^{t + 1} &= x^t - α_t g^t \\
  g^{t + 1} &= g^t - α_t A g^t
  \end{align*}
  Ist $(A g^t, g^t) = 0$ folgt $g^t = 0 ⇒ A x^t = b$.
  #+ATTR_LATEX: :options [Gradientenverfahren]
  #+begin_thm latex
  Ist $A ∈ ℝ^{n × n}$ symmetrisch, positiv definit, so konvergiert das Gradientenverfahren für alle $x^0 ∈ ℝ^n$ gegen die Lösung von $A x = b$
  #+end_thm
  #+begin_proof latex
  Fehlerfunktional
  \[E(y) = \norm{y - x}_A^2 = (y - x, A[y - x]), y ∈ ℝ^n\]
  Fehler $e^t = x^t - x$
  \begin{align*}
  ⇒ \frac{E(x^t) - E(x^{t + 1})}{E(x^t)} &= \frac{(e^t, A e^t) - (e^{t + 1}, A e^{t + 1})}{(e^t, A e^t)} \\
  &= \frac{(e^t, A e^t) - (e^t - α_t g^t, A[e^t - α_t g^t])}{(e^t, A e^t)} \\
  &= \frac{2 α_t(e^t, A g^t) - α_t^2(g^t, A g^t)}{(e^t, A e^t)} \\
  &= \frac{2 α_t \norm{g^t}^2 - α_t^2(g^t, A g^t)}{(g^t, A^{-1} g^t)} \\
  &= \frac{2 \frac{\norm{g^t}^2}{(A g^t, g^t)} \norm{g^t}^2 - \frac{\norm{g^t}^4}{(A g^t, g^t)}}{(g^t, A^{-1} g^t)} \\
  &= \frac{\norm{g^t}^4}{(g^t, A g^t)(g^t, A^{-1} g^t)}
  \end{align*}
  $A$ symmetrisch, positiv definit $⇒ λ \norm{y}^2 \leq (y, A y) \leq Λ \norm{y}^2$
  \[Λ^{-1} \norm{y}^2 \leq (y, A^{-1} y) \leq λ^{-1} \norm{y}^2\]
  Ist $x^t \neq x$, das heißt $E(x^t) \neq 0$ und $g^t \neq 0$ folgt:
  \[\frac{\norm{g^t}^4}{(g^t, A g^t)(g^t, A^{-1} g^t)} \geq \frac{\norm{g^t}^4}{Λ \norm{g^t}^2 λ^{-1} \norm{g^t}^2} = \frac{λ}{Λ}\]
  $⇒ E(x^{t + 1}) \leq [1 - κ^{-1}] E(x^t), κ := \cond_2(A)$. Wegen $0 < 1 - κ^{-1} < 1$ konvergiert $E(x^t) \xrightarrow{t \to ∞} 0$ für alle
  $x_0 ∈ ℝ^n ⇒ x^t \xrightarrow{t \to ∞} x$
  #+end_proof
  #+ATTR_LATEX: :options [Lemma vovn Kantorovich]
  #+begin_lemma latex
  Sei $A ∈ ℝ^{n × n}$ symmetrisch positiv definit mit $λ, Λ > 0$ kleinster / gröhter  eigenwert. Dann
  \[4 \frac{λ Λ}{(λ + Λ)^2} \leq \frac{\norm{y^4}}{(y, A y)(y, A^{-1} y)}\]
  #+end_lemma
  #+begin_proof latex
  Skript.
  #+end_proof
  #+ATTR_LATEX: :options [Fehlerabschätzung]
  #+begin_thm latex
  Für das Gradientenverfahren gilt die Fehlerabschätzung
  \[\norm{x^t  - x}_A \leq (\frac{1 - κ^{-1}}{1 + κ^{-1}})^t \norm{x^0 - x}_A, t ∈ ℕ\]
  #+end_thm
  #+begin_proof latex
  \[E(x^{t + 1}) = (1 - \frac{\norm{g^t}^4}{(g^t, A g^t)(g^t, Λ^{-1} g^t)}) E(x^t)\]
  \begin{align*}
  ⇒ E(x^{t + 1}) &\leq (1 - 4 \frac{λ Λ}{(λ + Λ)^2})E(x^t) \\
  &= \frac{λ^2 + 2λ Λ + Λ^2 - 4λ Λ}{(λ + Λ)^2}E(x^t) = (\frac{λ - Λ}{λ + Λ})^2 E(x^t) \\
  ⇒ \norm{x^t - x}_A^2 &\leq (\frac{Λ - λ}{Λ + λ})^{2t} \norm{x^0 - x}_A^2
  \end{align*}
  #+end_proof
  $A x = b ⇔ \underbrace{K^{-1} A K^{-1T}}_{\tilde A} \underbrace{K^T x}_{\tilde x} = \underbrace{K^{-1} b}_{\tilde b}$
  Wiederholung: Allgemeines Abstiegsverfahren für $A x = b, A ∈ ℝ^{n × n}$ symmetrisch positiv definit.
  \[⇔ \min Q(y) = \frac{1}{2}(y, Ay) - (b, y)\]
  \[g^t = A x^t - b, α_t = -\frac{(g^t, r^t)}{r^t A r^t}\]
  \[x^{t + 1} = x^= + α_t r^t\]
  Gradientenverfharen: $r^t := - g^t$. Fehlerabschätzung:
  \[\norm{x^t - x}_A \leq (\frac{1 - κ^{-1}}{1 + κ^{-1}}) \norm{x^0 - x}_A, κ = Λ/λ = \cond_2(A)\]
  Beobachtung:
  \[(g^{t + 1}, g^t) = (g^t - α_t A g^t, g^t) = \norm{g^t}^2 - \underbrace{α_t (A g^t, g^t)}_{\norm{g^t}^2} = 0\]
  $⇒$ $g^{t + 1} \perp g^t$. $⇒$ Langsame Konvergenz für $\cond_2 A \gg 1$. \\
  *Conjugate-Gradients-Verfahren (CG)* \\
  Idee: Wähle Abstiegsrichtung $d^t$ mit $(d^i, d^j)_A = 0 ∀ i \neq j$. (\(A\)-orthogonal).
  Ansatz: $B_t := \Lin \{d^0, \dots, d^{t - 1}\}$.
  \[x^t = x^0 + \sum_{i = 0}^{t - 1} α_i d^i α x^0 + B_t\]
  bestimmt durch
  \begin{align*}
  Q(x^t) &= \min_{y ∈ x^0 + B_t} Q(y) \\
  ⇔	\norm{A x^t - b}_{A^{-1}} &= \min_{x^0 + B_j} ! \\
  ⇔	\norm{x^t - x}_{A} &= \min_{x^0 + B_j} ! \\
  ⇔ 0 &= \dd{Q}{α_i}(x^t) = (\grad Q(x^t), d^i) \\
  ⇔ (A x^t - b, d^i) &= 0, \quad i = 0, \dots, t - 1 \\
  ⇔ g^t &\perp B_t
  \end{align*}
  Wir legen	zuerst $B_t$ fest:
  \[B_t := K_t(d^0; A), d^0 = b - A x^0\]
  mit dem Krylov-Raum
  \[K_t(v,; A)= \Lin \{v, A v, A^2 v, \dots, A^{t - 1} v\}\]
  Motivation (Lucky breakdown). Wird $K_t(d^0; A)$ stationär, das heißt gilt
  \[A^t d^0 ∈ K_t(d^0; A)\]
  für ein $t ∈ ℕ$, so folgt
  \[-g^t = b - A x^t = d^0 + A(x^0 - x^t) ∈ d^0 + A K_t(d^0; A) ⊂ K_t(d^0; A) ⊂ K_t(d^0 ; A)\]
  und wegen $g^t \perp K_t(d^0; A)$ $⇒$ $g^t = 0$, das heißt $A x^t = b$. \\
  Konstruktion der Richtungen $d^t ∈ K_{t + 1}(d^0; A)$: Ansatz:
  \[d^t = \underbrace{-g^t}_{\not ∈ K_t(d^0; A)} + \underbrace{\sum_{j = 0}^{t - 1} β_j^{t - 1} d^j}_{∈ K_t(d^0; A)} ∈ K_{t + 1}(d^0; A)\]
  \(A\)-Orthogonalität:
  \begin{align*}
  0 &\overset{!}{=} (d^t, A d^i) = (-g^t, A d^i) + \sum_{j = 0}^{t - 1} β_j^{t - 1} \underbrace{(d^j, A d^i)}_{= 0, i \neq j} \\
  &= (-g^t + β_j^{t - 1} d^i, A d^i), i = 0, \dots, t - 1
  \end{align*}
  Wegen $(g^t, d^i) = 0, i = 0, \dots, t - 1$ folgt
  \[(g^t, A d^i) = 0, \quad i = 0, \dots, t - 2\]
  $⇒$ $β_i^{t - 1} = 0, i = 0, \dots, t - 2$. $i = t - 1$:
  \[0 = (-g^t, A d^{t - 1}) + β_{t - 1}^{t - 1}(d^{t - 1}, A d^{t - 1})\]
  \[⇒ β_{t_ 1} := β_{t - 1}^{t - 1} = \frac{(g^t, A d^{t - 1})}{(d^{t - 1}, A d^{t - 1})}\]
  \begin{align*}
  ⇒ d^t &= - g^t + β_{t - 1} d^{t - 1} \\
  ⇒ g^{t + 1} &= A x^{t + 1} - b = A x^t - b + α_t A d^t \\
  &= g^t + α_t A d^t \\
  ⇒ α_t &= - \frac{(g^t, d^t)}{(d^t, A d^t)} \tag{klassische Form} \\
  ⇒ x^{t + 1} &= x^t + α_t d^t \\
  \intertext{Vereinfachung:}
  α_t = \frac{\norm{x^t}^2}{(d^t, A d^t)}, β_t = \frac{\norm{y^{t + 1}}^2}{\norm{g^t}^2}
  \end{align*}
  Beobachtung: $g^t \neq 0, t = 0, \dots, n - 1$
  \[⇒ \Lin \{d^0, \dots, d^{n -1}\} = ℝ^n ⇒ x^n = x\]
  (Gilt nur in exakter Arithmetik)
  #+begin_lemma latex
  Für ein Polynom $p ∈ P_t, p(0) = 1$ gelte auf einer Menge $S ⊂ ℝ$ mit $σ(A) ⊂ S$
  \[\sup_{μ ∈ S} \abs{p(μ)} \leq M\]
  Dann gilt $\norm{x^t - x}_A \leq M\norm{x^0 - x}_A$
  #+end_lemma
  #+begin_proof latex
  \[\norm{x^t - x}_A = \min_{y ∈ x^0 + B_t} \norm{y - x}_A\]
  Wegen $B_t = \Lin \{d^0,\dots, d^{t - 1}\} = \Lin \{A^0 g^0, \dots, A^{t - 1}, g^0\}$
  \begin{align*}
  ⇒ \norm{x^t - x}_A &= \min_{p ∈ P_{t - 1}} \norm{x_0 - x + p(A) g^0}_A \\
  &= \min_{p ∈ P_{t - 1}} \norm{[E_n + A p(A)](x^0 - x)}_A \\
  &\leq \min_{\substack{p ∈ P_t \\ p(0) = 1}} \norm{p(A)}_A \norm{x^0 - x}_A
  \end{align*}
  Orthonormalbasis $\{ω_1,\dots, ω_n\}$ aus Eigenvektoren mit Eigenwerten $λ_i$
  \begin{align*}
  y &= \sum_{i = 1}^{n} γ_i ω_i, \quad γ_i = (y, ω_i) \\
  \norm{p(A) y}_A^2 &= \sum_{i = 1}^{n} λ_i p(λ_i)^2 γ_i^2 \\
  &\leq M^2 \sum_{i = 1}^{n} λ_i γ_i^2 = M^2 \norm{y}_A^2 \\
  ⇒ \norm{p(A)}_A &= \sup_{y ∈ ℝ^n \setminus \{0\}} \frac{\norm{p(A)y}_A}{\norm{y}_A} \leq M
  \end{align*}
  #+end_proof
  #+ATTR_LATEX: :options [CG-Konvergenz]
  #+begin_thm latex
  Mit $κ = Λ / λ$ gilt
  \[\norm{x^t - x}_A \leq (\frac{1 - 1/\sqrt{κ}}{1 + 1/\sqrt{κ}})^t \norm{x^0 - x}_A\]
  #+end_thm
  #+begin_proof latex
  $S = [λ, Λ]$. Lemma $⇒$
  \[\norm{x^t - x}_A \leq \min_{\substack{p ∈ P_t \\ p(0) = 1}} \sup_{μ ∈ [λ, Λ]} \abs{p(μ)}\norm{x^0 - x}_A\]
  zu zeigen:
  \[\min_{\substack{p ∈ P_t \\ p(0) = 1}} \sup_{μ ∈ [λ, Λ]} \abs{p(μ)}\norm{x^0 - x}_A \leq 2 (\frac{1 - \sqrt{λ/Λ}}{1 + \sqrt{λ/Λ}})^t\]
  Problem der Bestapproximation mit Polynomen bezüglich Max-Norm!
  Lösung: $T_t$: \(t\)-tes Tschebyscheff-Polynom auf $[-1,1]$
  \[\bar p(μ) = T_t(\frac{Λ + λ - 2μ}{Λ - λ})T(\frac{Λ + λ}{Λ - λ})^{-1}\]
  \[⇒ \sup_{μ ∈ [λ, Λ]} \abs{\bar p(μ)} = T(\frac{Λ + λ}{Λ - λ})^{-1}\]
  Darstellung für $\abs{μ} > 1$
  \begin{align*}
  T_t(μ) &= \frac{1}{2}((μ + \sqrt{μ^2 - 1})^t + (μ - \sqrt{μ^2 - 1})^t) \\
  T_t(μ) &= \frac{1}{2}((\frac{\sqrt{κ + 1}}{\sqrt{κ} - 1})^t + (\frac{\sqrt{κ} - 1}{\sqrt{κ} + 1})^t) \geq \frac{1}{2}(\frac{\sqrt{κ} + 1}{\sqrt{κ} - 1})^t
  \end{align*}
  #+end_proof
  Wiederholung: CG, Abstiegsverfahren. $(d^i, A d^j) = 0, i \neq j$.
  - 2 Parameter
	\begin{align*}
	α_t &= \frac{\norm{g^t}^2}{(d^t, A d^t)} \\
	β_t &= \frac{\norm{g^{t + 1}}^2}{\norm{g^t}^2} \\
	x^{t + 1} &= x^t + α_t d^t \\
	g^{t + 1} &= g^t + α_t A d^t \\
	d^{t + 1} &= - g^{t + 1} + β_t d^t \\
	d^0 &= - g^0 = b - A x^0
   	\end{align*}
  - Axakte Arithmetik: Lösung nach spätestens $n + 1$ Schritten.
  -	\[\norm{x^t - x}_A \leq \min_{\substack{p ∈ P_t \\ p(0) = 1}} \max_{λ ∈ σ(A)} \abs{p(λ)} · \norm{x^0 - x}_A \leq 2 (\frac{1 - 1/\sqrt{κ}}{1 + 1 / \sqrt{κ}})^t \norm{x^0 - x}_A\]
* Matrizeneigenwertaufgaben
  $A ∈ \mathbb{K}^{n × n}, \mathbb{K} = ℝ$ oder $ℂ$. Suche $(λ, ω) ∈ \mathbb{K} × (\mathbb{K}^n \setminus \{0\})$, sodass
  \[A ω = λ ω\]
** Konditionierung des Eigenwert-Problems.
   #+ATTR_LATEX: :options [Stabilität]
   #+begin_lemma latex
   Sei $A, B ∈ \mathbb{K}^{n × n}, \norm{·}$ die natürliche Matrizennorm, $λ ∈ σ(A) \setminus σ(B)$. Dann gilt
   \[\norm{(λ E_n - B)^{-1} (A - B)} \geq 1\]
   #+end_lemma
   #+begin_proof latex
   \begin{align*}
   A ω &= λ ω \\
   ⇒ (A - B)ω &= (λE_n - B)ω \\
   ⇒ (λ E_n - B)^{-1} (A - B) ω &= ω \\
   ⇒ 1 &= \frac{\norm{(λ E_n - B)^{-1}(A - B) ω}}{\norm{ω}} \\
   &\leq \sup_{x ∈ \mathbb{K}^n \setminus \{0\}}	\frac{\norm{(λ E_n - B)^{-1}(A - B) x}}{\norm{x}} \\
   &= \norm{(λ E_n - B)^{-1} (A - B)}
   \end{align*}
   #+end_proof
   #+ATTR_LATEX: :options [Satz von Gerschgorin]
   #+begin_thm latex
   Alle Eigenwerte von $A ∈ \mathbb{K}^{n × n}$ liegen in der Vereinigung der sogenannten Gerschgorin-Kreise $(j = 1, \dots, n)$
   \[K_j = \{z ∈ ℂ \mid \abs{z - a_{jj}} \leq \sum_{\substack{k = 1\\ k \neq j}}^{n} \abs{a_{jk}}\}\]
   Sind $U = ∪_{i = 1}^m K_{ji}$ und $V = ∪_{j = 1}^n K_j \setminus U$ disjunkt, so liegen in $U$ genau $m$ und in $V$ genau $n - m$ Eigenwerte.
   #+end_thm
   #+begin_proof latex
   1. $B = D = \diag(a_{11}, \dots, a_{nn})$. Maximale Zeilensummennorm:
	  \begin{align*}
	  1 &\leq \norm{(λ E_n - D)^{-1} (A - D)}_∞ \\
	  &= \max_{j = 1, \dots, n} \frac{1}{\abs{λ - a_{jj}}} \sum_{\substack{k = 1\\ k \neq j}}^{n} \abs{a_{jk}}
   	  \end{align*}
	  für $λ \neq a_{jj}, j = 1, \dots, n ⇒ λ ∈ K_{j^{\ast}}$ mit $j^{\ast} = \argmax \{\dots\}$.
   2. Für $t ∈ [0, 1]$ setze $A_t = (1 - t)D + t A$. $m$ Eigenwerte von $A_0$ liegen in $U$ und $n - m$ Eigenwerte in $V$. Wegen Stetigkeit der Eigenwerte von $A_t$ bezüglich $t$ folgt
	  die Behauptung für $A_1 = A$
   #+end_proof
   #+ATTR_LATEX: :options [Stabilitätssatz]
   #+begin_thm latex
   Sei $A ∈ \mathbb{K}^{n × n}$ mit $n$ linear unabhängigen Eigenvektoren $\{ω_1, \dots, ω_n\}$ und $B ∈ \mathbb{K}^{n × n}$. Dann
   gibt es zu jedem Eigenvert $λ(B)$ von $B$ einen Eigenwert $λ(A)$ von $A$ mit
   \[\abs{λ(A) - λ(B)} \leq \cond_2(W) \norm{A - B}_2\]
   wobei $W = (ω_1, \dots, ω_n) ∈ \mathbb{K}^{n × n}$
   #+end_thm
   #+begin_proof latex
   $A W = W Λ, Λ = \diag(λ_1, \dots, λ_n) ⇒ A = W Λ W^{-1}$. Sei $λ ∈ σ(B) \setminus σ(A)$
   \begin{align*}
   ⇒ \norm{(λ E_n - A)^{-1}}_2 &= \norm{W(λ E_n - Λ)^{-1} W^{-1}}_2 \\
   &\leq \norm{W}_2 \norm{W^{-1}}_2 \norm{(λ E_n - Λ)^{-1}}_2 \\
   &= \cond_2(W) \max_{i = 1, \dots, n} \abs{λ - λ_i}^{-1} \\
   ⇒ 1 &\leq \norm{(λ E_n - A)^{-1}}_2 \norm{A - B}_2 \\
   &\leq \cond_2(W) \norm{A - B}_2 \max_{i = 1, \dots, n} \abs{λ - λ_i}^{-1} \\
   ⇒ \mid_{i = 1, \dots, n} \abs{λ - λ_i} &\leq \cond_2(W) \norm{A - B}_2
   \end{align*}
   #+end_proof
   $A$ hermitesch: $W$ orthonormal $⇒ \cond_2(W) = 1$. Regel: Das Eigenwert-Problem hermitescher Matrizen ist gut konditioniert, während das allgemeine ja nach $\cond_2(W)$
   beliebig schlecht konditioniert ist.
** Iterative Methoden
   Verfahren um einen (nicht alle) Eigenwert zu finden.
   Potenzmethode (von Mises)
   $z^0 ∈ ℂ^n, \norm{z^0} = 0$. $t \geq 1$:
   \begin{align*}
   \tilde z^t &= A z^{t - 1}, z^t = \frac{\tilde z^t}{\norm{\tilde z}^t} \\
   λ^{(t)} &:= \frac{\tilde z_k^t}{z_k^t}, z_k^t \neq 0
   \end{align*}
   #+begin_thm latex
   Sei $A$ diagonalisierbar mit $\abs{λ_n} > \abs{λ_i}, i = 1, \dots, n$. $z^0$ habe eine nichtverschwindende Komponente bezüglich Eigenvektor $ω_n$.
   Dann gibt es $σ_t ∈ ℂ, \abs{σ_t} = 1$, sodass
   \[\norm{z^t - σ_t ω_n} \xrightarrow{t \to ∞} 0\]
   und
   \[λ^{(t)} - λ_n = \mathcal{O}(\abs{\frac{λ_{n - 1}}{λ_n}}^t)\]
   #+end_thm
   #+begin_proof latex
   Skript.
   #+end_proof
   Hermitesches $A$:
   \[λ^{(t)} = \frac{(z^t, A z^t)_2}{(z^t, z^t)}_2\]
   (Rayleigh-Quotient)
   \[\to λ^{(t)} = λ_n + \mathcal{O}(\abs{\frac{λ_{n + 1}}{λ_n}}^{2t})\]
   Inverse Iteration. \\
   Annahme: Gute Näherung $\tilde λ$ für $λ_k$ verfügbar. Beobachtung: Ist $\tilde λ \not ∈ σ(A)$ so hat
   $(A - \tilde λ E_n)^{-1}$ die Eigenwerte $μ_i = (λ_i - \tilde λ)^{-1}$. Idee:
   Potenzmethode für $(A - \tilde λ E_n)^{-1}$. Löse $(A - \tilde λ E_n) \tilde z^t = z^{t - 1}$. Normiere
   \[z^t = \frac{\tilde z^t}{\norm{\tilde z^t}}\]
   Wiederholung: $A x = λ x$ \\
   - Potenzmethode: $z^0 ∈ ℂ^n$
	 \begin{align*}
	 \tilde z^{(t)} &= A z^{(t)} \\
     \quad z^{(t + 1)} &= \frac{\tilde z^{(t)}}{\norm{\tilde z^{(t)}}} \\
	 λ^{(t)} &= \frac{\tilde z^{(t)}_t}{z^{(t)}_k} \\
	 λ^{(t)} &= λ_n + \mathcal{O}(\abs{\frac{λ_{n - 1}}{λ_n}}^t)
     \end{align*}
   - Inverse Iteration = Potentzmethode auf $(A - \tilde λ E_n)^{-1}$
** Reduktionsmethoden
   #+begin_defn latex
   $A, B ∈ ℂ^{n × n}$ heißen ähnlich $(A \sim B)$, wenn $∃ T ∈ C^{n × n}$ invertierbar mit $A = T^{-1} B T$.
   #+end_defn
   Beobachtungen:
   \begin{align*}
   \det(A - z E_n) = \det(T^{-1}(B - z E_n) T) \\
   &= \det(T^{-1}) \det(B - z E_n) \det(T) \\
   &= \det(B - z E_n) \\
   ⇒ σ(A) &= σ(B)
   \end{align*}
   \[A ω = λ ω ⇒ T^{-1} B T ω = λ W ⇒ B \underbrace{T ω}_{\tilde ω} = λ \underbrace{T ω}_{\tilde ω}\]
   Reduktionsmethode: Benutze Ähnlichkeitstransformationen, um $A$ auf eine Gestalt zu bringen, in der men die Eigenwerte leicht
   ablesen kann:
   \[A =: A^{(0)} = T_1^{-1} A^{(1)} T_1 = \dots = T_i^{-1} A^{-1} T_i = \dots\]
   #+ATTR_LATEX: :options [Jordansche Normalform]
   #+begin_defn latex
   Jede	Matrix $A ∈ ℂ^{n × n}$ ist ähnlich zu einer Blockmatrix der Form ($λ_i \neq λ_j, i \neq j$)
   \[\diag(C_{r_1^{(1)}}(λ_1),\dots, C_{r_{ρ_1}^{(1)}}(λ_1), \dots, C_{r_1^{(m)}}(λ_m)), \dots, C_{r_{ρ_m}^{(m)}}(λ_m)\]
   mit Jordan-Blöcken
   \[C_r(λ) = \begin{pmatrix}λ & 1 &   & 0 \\   & \ddots & \ddots &   \\   &   & \ddots & 1 \\   &   &   & λ\end{pmatrix} ∈ C^{r × r}\]
   #+end_defn
   Beobachtung:
   - $σ(C_r(λ)) = \{λ\}$
   - $\ker(C_r(λ) - λ E_n) = \Lin \{e_1\}$
   $⇒$
   - Algebraische Vielfachkeit von $λ_i$:
	 \[\sum_{j = 1}^{ρ_i} r_j^{(i)} = σ_i\]
   - Geometrische Vielfachheit von $λ_i: ρ_i$
   Achtung: Jordan-Zerlegung numerisch nicht sinnvoll ($\cond(T) \gg 1 / eps$)
   #+begin_lemma latex
   Für $A ∈ ℂ^{n × n}$ ist äquivalent:
   1. $A$ ist diagonalisierbar
   2. $∃$ Basis von $ℂ^n$ aus Eigenvektoren von $A$
   3. $σ_i = ρ_i, i = 1, \dots, m$
   #+end_lemma
   *Schursche Normalform* \\
   Sei $A ∈ ℂ^{n × n}$. Dann $∃ U ∈ ℂ^{n × n}$ unitär, sodass
   \[\bar U^T A U = \begin{pmatrix}λ_1 &   &   \\   & \ddots & \ast \\ 0 &   & λ_n\end{pmatrix}\]
   Folgerung: Wenn $A = \bar A^T$, dann ist $\bar U^T A U$ hermitesch. $⇒ A$ diagonalisierbar.
   Fehlefortpflanzung bei Reduktionsmethoden. Sei $B \sim A$, das heißt $B = T^{-1} A T$
   \begin{align*}
   B + δ B &= T^{-1}(A + δ A) T \\
   ⇒ δ A &= T δ B T^{-1} \\
   ⇒ \norm{B} &\leq \cond{T}\norm{A} \\
   \norm{δA} &\leq \cond(T) \norm{δB} \\
   ⇒ \frac{\norm{δA}}{\norm{A}} &\leq \cond(T)^2 \frac{\norm{δB}}{\norm{B}}
   \end{align*}
   Wegen $\cond(T) = \cond(T_1 \dots T_m) \leq \cond(T_1) · \dots · \cond(T_m)$ muss $\cond(T_i)$ klein gewährleistet sei.
   $T$ unitär $⇒ \cond_2(T_i) = 1$.
   Reeller Fall
   #+begin_defn latex
   $A ∈ ℝ^{n × n}$ heißt Hessenberg-Matrix, wenn $a_{ij} = 0 ∀ i > j + 1$
   #+end_defn
   #+ATTR_LATEX: :options [Hessenbergsche Normalform]
   #+begin_thm latex
   Für jede Matrix $A ∈ ℝ^{n × n}$ existieren Hauseholdertransformationen $T_1, \dots, T_{n - 2}$ so, dass mit $T = T_{n - 2} · \dots · T_1$ die Matriz
   $T A T^T$ Hessenberg ist. Für $A = A^T$ ist $T A T^T$ tridiagonal.
   #+end_thm
   #+begin_proof latex
   $A = [a_1, \dots, a_n]$. Wähle $a_1 = (0, u_{12}, \dots, 2_{1n})^T ∈ ℝ^n, \norm{u_1}_2 = 1$ sodass
   \begin{align*}
   T_1 a_1 &= (E_n - 2 u_1 u_1^T) a_1 ∈ \Lin \{e_1, e_2\} \\
   ⇒ A^{(1)} &= T_1 A T_1^T = (\begin{array}{c|ccc} a_{11} & a_{12} & \dots & a_{1n} \\ \hline  \ast & & & \\ 0 & & & \\ \vdots & &\ast & \\ 0 & & & \end{array}) · (\begin{array}{c|ccc}1 & & 0 & \\ \hline & & & \\ 0 & & \ast & \\ & & & \\\end{array}) \\
   &= (\begin{array}{c|ccc} a_{11} & \ast & \dots & \ast \\ \hline  \ast & & & \\ 0 & & & \\ \vdots & &\tilde A^{(1)} & \\ 0 & & & \end{array})
   \end{align*}
   Fahre fort auf $\tilde A^{(1)}$ für $n - 2$ Schritte $⇒ A^{(n - 2)}$ Hessenberg.
   #+end_proof
   Verfahren für Hessenberg-/Tridiagonalmatrizen:
   QR-Verfahren:
   \begin{align*}
   A^{(t)} &=: Q^{(t)} R^{(t)}, A^{(t + 1)} = R^{(t)} Q^{(t)} \\
   A^{(t + 1)} &= \underbrace{(Q^{(t)})^T (Q^{(t)})}_{E_n} R^{(t)} Q^{(t)} = (Q^{(t)})^T A^{(t)} Q^{(t)} \\
   ⇒ A^{(t + 1)} &\sim A^{(t)}
   \end{align*}
   #+begin_thm latex
   Für die Eigenwerte $λ_i$ von $A$ gelte $\abs{λ_1} > \abs{λ_2} > \dots > \abs{λ_n}$.
   Dann gilt für $A^{(t)} = (a_{jk}^{(t)})$ aus dem QR-Verfahren
   \[\lim_{t \to ∞} a_{jj}^{(t)} = λ_j, \quad j = 1, \dots, n\]
   #+end_thm
