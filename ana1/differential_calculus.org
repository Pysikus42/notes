* Differentiation
  #+ATTR_LATEX: :options [4.1 Differenzquotienten]
  #+begin_defn latex
  Für eine Funktion $f:D\to \mathbb{R}, D\subseteq \mathbb{R}$, definieren wir in einem Punkt $x_0 \in D$ einen
  *Differenzquotienten* durch $D_n f(x_0):= \frac{f(x_0 + h) - f(x_0)}{h}$, wobei $x_0 + h \in D$
  #+end_defn
  #+ATTR_LATEX: :options [4.2 Ableitung]
  #+begin_defn latex
  $f:D\to\mathbb{R}$ heißt *differenzierbar* im Punkt $x_0 \in D$ mit *Ableitung* $f'(x_0)$, wenn
  für jede Nullfolge $(h_n)_{n\in\mathbb{N}}$ mit $x_0 + h_n \in D$, die Folge $(D_{h_n} f(x_0))_{n\in\mathbb{N}}$
  konvergiert zu $f'(x_0)$
  #+end_defn
  #+begin_remark latex
  $f'(x)$ ist eindeutig.
  #+end_remark
  #+begin_proof latex
  Für zwei Nulfolge $h_n, \tilde h_n$, sodass:
  \[\lim_{n\to\infty} D_{h_n} f(x_0) = a, \lim_{n\to\infty} D_{\tilde h_n} f(x_0) = \tilde a\]
  fassen wir eine Nullfolge $\{h_1, \tilde h_1, h_2, \tilde h_2, \ldots\}$ zusammen. Der
  zugeörige Differenzquotient konvergiert $\implies a = \tilde a$
  #+end_proof
  Notation:
  \begin{align*}
  f'(x_0) &=: \dd{f}{x}(x_0) \\
  f'(x_0) &= \lim_{\substack{x\in D \\ x\to x_0}}  \frac{f(x) - f(x_0)}{x - x_0}
  \end{align*}
  #+ATTR_LATEX: :options [4.3]
  #+begin_defn latex
  Eine Funktion $f:D\to\mathbb{R}$ heißt differenzierbar auf $D$, wenn sie in edem Punkt $x_0 \in D$ differenzierbar ist. Sie heißt stetig differenzierbar, wenn
  die Ableitung $f'$ auf $D$ eine stetige Funktion ist.
  #+end_defn
  #+begin_remark latex
  Im Falle eines Randpunktes behalten wir einseite Stetigkeit. $D = [a,b]:$
  - für $x_0 = a, x\downarrow a :\iff x > a \wedge x \to a$
  - für $x_0 = b, x\uparrow b :\iff x < b \wedge x \to b$
  #+end_remark
  #+ATTR_LATEX: :options [4.4]
  #+begin_thm latex
  Eine Funktion $f:D\to\mathbb{R}$ ist in einem $x_0 \in D$ genau dann differenzierbar mit Ableitung $f'(x_0)$, wenn
  \[\Forall \eps > 0 \Exists \delta_\eps > 0: x_0 + h \in D, \abs{h} < \delta_\eps \implies \abs{\frac{f(x_0 + h) - f(x_0)}{h} - f'(x_0)} <  \eps\]
  #+end_thm
  #+begin_proof latex
  Beweis aus der Definition des Grenzwerts.
  #+end_proof
  #+ATTR_LATEX: :options [4.5]
  #+begin_thm latex
  Eine Funktion $f:D\to\mathbb{R}$ ist genau dann in einem Punkt $x_0\in D$ differenzierbar, wenn es eine Konstante gibt, $x\in\mathbb{R}$, sodass
  \[f(x) = f(x_0) + c(x - x_0) + \omega(x), x\in D\]
  mit einer Funktion $\omega:D\to\mathbb{R}$, sodass
  \[\lim_{\substack{x\in D\\ x\to x_0}} = 0\]
  Diese Konstante $c = f'(x_0)$
  #+end_thm
  #+begin_proof latex
  Sei $f$ in $x$ differenzierbar und $\omega(x) := f(x) - f(x_0)(x - x_0)$. Dann aus differenzierbarkeit von $f$
  \[\frac{\omega(x)}{x - x_0} = \frac{f(x) - f(x_0)}{x - x_0} - f'(x_0) \xrightarrow{x\to x_0} 0\]
  Sei umgekehrt $f(x) = f(x_0) + c(x - x_0) + \omega(x)$ mit $\lim_{x\to x_0} \frac{\omega(x)}{x\to x_0} = 0$
  Dann gilt:
  \[\frac{f(x) - f(x_0)}{x - x_0} - c = \frac{\omega(x)}{x - x_0} \xrightarrow{x\to x_0} 0\]
  das heißt $f$ ist in $x_0$ differenzierbar mit Ableitung $f'(x_0)$
  #+end_proof
  #+begin_remark latex
  Der Satz besagt, dass affin-lineare Funktion (Gerade) $g(x) = f(x_0) + f'(x_0)(x - x_0)$
  approximiert die differenzierbare Funktion in $x_0 \in D$. Der Graph von $g$ ist
  die tangenta an dem Graphen von $f$ in $(x_0, f(x_0))$
  #+end_remark
  #+ATTR_LATEX: :options [4.6]
  #+begin_lemma latex
  Eine Funktion $f:D\to\mathbb{R}$ in $x_0 \in D$ differenzierbar ist dort stetig.
  #+end_lemma
  #+begin_proof latex
  \[f(x) = f(x_0) + f'(x_0)(x - x_0) + \omega(x) \Rightarrow f(x) \xrightarrow{x\to x_0} f(x_0)\]
  #+end_proof
  #+begin_remark latex
  Man kann die n-te Ableitung rekursiv definieren.
  \begin{align*}
  \frac{\d^n f}{\d x^n}(x) = f^{(n)}(x), n\geq 3 \\
  \frac{\d^2 f}{\d x^2} (x) = f^{(2)}(x) = f''(x)
  \end{align*}
  #+end_remark
  #+ATTR_LATEX: :options [4.7]
  #+begin_ex latex
  $f(x) = \abs{x}$ ist nicht in $x_0 = 0$ differenzierbar. Um dies zu sehen, betrachten wir eine Nullfolge
  \[h_n = (-1)^n \frac{1}{n}, n\in \mathbb{N}\]
  und
  \[\frac{f(h_n) - f(0)}{h_n} = \frac{\abs{h_n}}{h_n} = (-1)^n\]
  nicht konvergent. $in x_0 \neq 0$ ist $f(x) = \abs{x}$ differenzierbar
  #+end_ex
  #+ATTR_LATEX: :options [4.8]
  #+begin_lemma latex
  Für $f,g: D\to\mathbb{R}$ differenzierbar gelten die folgenden Rechenregeln:
  1. Linearkombination ist differenzierbar $(\alpha f + \beta g)' = \alpha f' + \beta g', \alpha,\beta\in\mathbb{R}$
  2. $(f\cdot g)'(x) = f'(x)g(x) + f(x) g'(x)$
  3. $g(x) \neq 0$
	 \[(\frac{f}{g})'(x) = \frac{f'(x)g(x) - f(x)g'(x)}{g^2(x)}\]
  #+end_lemma
  #+begin_proof latex
  1. Aus den Eigenschaften von konvergenten Zahlenfolgen
  2. Aus Definition:
     \begin{align*}
     \string(f\cdot g\string)'\string(x_0\string) &= \lim_{x\to x_0}  \frac{f(x)(g(x) - g(x_0)) + (g(x) - f(x_0))g(x)}{x - x_0} \\
     &= f(x_0)g'(x_0) + f'(x)g(x)
     \end{align*}
  3. Erst $f \equiv 1$
     \begin{align*}
     \string(\frac{1}{g}\string)'\string(x\string) &= \lim_{x\to x_0} (\frac{1}{g(x)} - \frac{1}{g(x_0)}) \frac{1}{x - x_0} \\
     &= \lim_{x\to x_0} \frac{g(x_0) - g(x)}{g(x)g(x_0)} \frac{1}{x - x_0} \\
     &= \lim - \frac{g'(x_0)}{g^2(x_0)} \\
     \string(\frac{f}{g}\string)'\string(x_0\string) = (f \frac{1}{g})'(x_0) = \frac{f' g - f g'}{g^2}(x)
     \end{align*}
  #+end_proof
  #+ATTR_LATEX: :options [4.9]
  #+begin_lemma latex
  Sei $f:D\to B\subseteq\mathbb{B}$ eine auf einem abgeschlossenen Definitionsbereich stetige und invertierbare Funktion mit Inverse $f^{-1}: B\to D$.
  Ist $f$ in einem $x_0 \in D$ differenzierbar mit $f'(x_0) \neq 0$, so ist auch $F^{-1}$ in einem $y_0 = f(x_0)$ differenzierbar
  und es gilt
  \[(f^{-1})'(y_0 = \frac{1}{f'(x_0)}, y_0 = f(x_0))\]
  #+end_lemma
  #+begin_proof latex
  Für $y_n = f(x_n), y_0 = f(x_0)$ mit $y_n \neq y_0$ und $y_n \xrightarrow{n\to\infty} y_0$.
  Aus Stetigkeit von $f^{-1}$ gilt auch $x_n \xrightarrow{n\to\infty} x_0$ und $x_n \neq x_0$.
  Aus der Differenzierbarkeit von $f$ in einem $x_0$ folgt:
  \[\frac{f^{-1}(y_n) - f^{-1}(y_0)}{y_n - y_0} = \frac{x_n - x_0}{f(x_n) - f(x_0)} = (\frac{f(x_n) - f(x_0)}{x_n - x_0})^{-1} \xrightarrow{n\to\infty} (f'(x_0))^{-1}\]
  Dies impliziert, dass $f^{-1}$ im Punkt $y_0 = f(x_0)$ differenzierbar ist mit der Ableitung $\frac{1}{f'/x_0}$
  #+end_proof
  #+ATTR_LATEX: :options [4.10]
  #+begin_ex latex
  \mbox{}
  1. $\displaystyle \ln'(y): f^{-1}(y) = \ln{y}, f(x) = e^x \implies \ln'(y) = \frac{1}{(e^x)'} = \frac{1}{e^x} = \frac{1}{y}$
  2. Umkehrfunktion des Sinus
	 \begin{align*}
	 y &= \sin{x}, x\in (-\frac{\pi}{2}, \frac{\pi}{2}) \\
	 x = \arcsin{y}, y\in (-1,1) = D \\
	 \arcsin'(y) = \frac{1}{\sin'(x)} = \frac{1}{\cos{x}} = \frac{1}{\sqrt{1 - \sin^2{x}}} = \frac{1}{\sqrt{1 - y^2}}
	 \end{align*}
  #+end_ex
  #+ATTR_LATEX: :options [4.11 Kettenregel]
  #+begin_lemma latex
  Seien $g:D_g \to \mathbb{R}, f:D_f \to D_g \subseteq \mathbb{R}$ stetige Funktionen. Die Funktion $f$ sei in $x_0 \in D_f$ differenzierbar
  und $g in y_0 = f(x_0)$ differenzierbar. Dann ist die zusammengesetzte Funktion $g(f(x_0)) =: (g\circ f)(x_0)$ in $x_0$ differenzierbar und
  es gilt
  \[(g\circ f)'(x_0) = g'(f(x_0))f'(x) \tag{Kettenregel}\]
  #+end_lemma
  #+begin_proof latex
  \begin{align*}
  \intertext{Wir definieren eine Funktion $\Delta g: D_g \to \mathbb{R}$ durch}
  \Delta g\string(y\string) &:= \begin{cases} \frac{g(y) - f(y_0)}{y - y_0} & y \neq y_0 \\ g'(y) & y = y_0\end{cases} \\
  \intertext{Da $g$ in $y_0$ differenzierbar ist gilt}
  \lim_{y \to y_0} \Delta g(y) &= g'(y_0) \\
  \intertext{Ferner gilt für $y\in D_g$:}
  g\string(y\string) - g(y_0) &= \Delta g(y)(y - y_0) \\
  \intertext{Damit erhalten wir}
  \string(g\circ f\string)'\string(x_0\string) &= \lim_{x\to x_0} \frac{g(f(x)) - g(f(x_0))}{x - x_0} \\
  &= \lim_{x \to x_0} \frac{\Delta g(f(x))(f(x) - f(x_0))}{x - x_0} \\
  &= \lim_{x\to x_0} \Delta g(f(x)) \lim_{x - x_0} \frac{f(x) - f(x_0)}{x - x_0} = g'(f(x_0))f'(x_0) \tag*{\qedhere}
  \end{align*}
  #+end_proof
  #+ATTR_LATEX: :options [4.12]
  #+begin_ex latex
  1. $g(x) = f(a x + b), a, b \in\mathbb{R} \implies g'(x) = a f'(a x + b)$
  2. $x^{\alpha} = e^{\alpha \ln{x}} = f(g(x)) = f(g(x)), f(y) := e^y, g(x) := \alpha \ln(x)$ \\
	 \[(x^{\alpha})' = f'(g(x))g'(x) = e^{\alpha\ln{x}}\alpha x^{-1} = \alpha x^{\alpha - 1}\]
  #+end_ex
** Mittelwertsätze und Extremalbedingungen
   #+ATTR_LATEX: :options [4.13]
   #+begin_defn latex
   Die Funktion $f:D\to\mathbb{R}$ hat in einem Punkt $x_0 \in D$ ein *globales Extremum* (Minimum oder Maximum), wenn gilt
   \[f(x_0) \leq f(x), x\in D \vee f(x_0) \geq f(x) \Forall x \in D\]
   Es handelt sich um ein *lokales Extremeum* (Minimum oder Moaximum), wenn auf einer $\delta$-Umgebung von $x_0$ (das heißt $U_\delta(x_0) = \{x\in D\mid\abs{x - x_0} < \delta\}$)
   gilt $f(x_0) \geq f(x) \Forall x\in U_\delta(x_0) \vee f(x_0) \leq f(x) \Forall x\in U_\delta(x_0)$
   Ein Extremum (globales oder lokales) heißt strikt, wenn es das isolierteste PUnkt in $D$ beziehungsweise in $U_\delta(x_0)$ ist, as heißt $f(x_0) > f(x) \vee f(x_0) < f(x)$
   #+end_defn
   #+ATTR_LATEX: :options [4.14 Satz von Extremum]
   #+begin_thm latex
   Besitz eine auf einem Intervall $I = (a,b)$ differenzierbare Funktion ein lokales Extremum $x_0 \in I$, so gilt dort notwendig $f'(x_0) = 0$
   #+end_thm
   #+begin_proof latex
   Habe $f$ in $x_0$ ein Minimum. Dann gilt für eine $(h_n)_{n\in\mathbb{N}}$ mit $h_n > 0, x_0 + h_n \in U_\delta(x_0)$
   \[\frac{f(x_0 + h_n) - f(x_0)}{h_n} \geq 0\]
   für eine Nullfolge $(h_n)_n\in\mathbb{N}$ mit $h_n < 0, x_0 + h_n \in U_\delta(x_0)$
   \[\frac{f(x_0 + h_n) - f(x_0)}{h_n} \leq 0\]
   Im Limes $h_n \to 0$ bekommen wir
   \[f'(x_0) \leq 0 \leq f'(x_0) \implies f'(x_0) = 0\]
   (Analog für Maximum)
   #+end_proof
   #+begin_remark latex
   Eine stetige Funktion besitzt auf einem abgeschlossenem Interball $[a,b]$ ein Minimum. Dieses kann in einem Randpunkt ($x_0 = a vee x_0 = b$) liegen,
   das heißt es ist nicht notwendig, das $f'(x_0) = 0$
   #+end_remark
   #+ATTR_LATEX: :options [4.15 Satz von Rolle]
   #+begin_thm latex
   Wenn eine im Interball $[a,b]$ stetige Funktion, in $(a,b)$ differenzierbar ist und $f(a) = f(b)$, so existiert ein $c\in (a,b)$, sodass $f'(c) = 0$
   #+end_thm
   #+begin_proof latex
   - Stetige Funktion auf $[a,b]$ nimmt ihr Maximum und Minimum
   - Wenn $f$ ist konstant $\implies f'(x) = 0$
   - Wen $f$ nicht konstant $\implies \Exists x_0 \in (a,b): f(x_0) > f(a) = f(b) \vee f(x_0) < f(a) = f(b)$
   $\implies$ das Maximum oder Minimum ist in einem $x_0 \in (a,b)$ angenommen $\implies f'(x_0) = 0$
   #+end_proof
   #+ATTR_LATEX: :options [4.16 1. Mittelwertsatz]
   #+begin_thm latex
   Ist $f$ stetig in $[a,b]$ und differenzierbar in $(a,b)$, so $\Exists c \in (a,b): f'(c) = \frac{f(b) - f(a)}{b - a}$
   #+end_thm
   #+begin_proof latex
   Wir definieren Funktion
   \[g(x) := f(x) - \frac{f(b) - f(a)}{b - a}(x - a)\]
   - $g$ ist stetig in $[a,b]$, differenzierbar in $(a,b)$
   - $g(a) = f(a) = g(b)$, Satz von Rolle liefert, dass $\Exists c\in(a,b): g'(c) = 0$
   \[0 = g'(c) = f'(c) - \frac{f(b) - f(a)}{b - a} \implies f'(c) = \frac{f(b) - f(a)}{b - a}\tag*{\qedhere}\]
   #+end_proof
   #+ATTR_LATEX: :options [4.17]
   #+begin_korollar latex
   Sei $f:(a,b) \to \mathbb{R}$ mindestens zweimal differenzierbar mit $f'(x_0) = 0$für ein $x_0 \in (a,b)$.
   Dann hat $f$ im Fall $f''(x_0) > 0$ in $x_0$ ein striktes lokales Minimum und im Fall $f''(x_0) < 0$ ein striktes lokales Maximum.
   #+end_korollar
   #+begin_proof latex
   Sei $f$ zweimal differenzierbar mit $f''(x_0) > 0$ Wegen
   \[f''(x_0) = \lim_{x\to x_0} \frac{f'(x) - f'(x_0)}{x - x_0} > 0\]
   gibt es ein $\eps \in \mathbb{R}_+$, sodass f+r $0 < \abs{x - x_0} < \eps$ gilt
   \[\frac{f'(x) - f'(x_0)}{x - x_0} > 0\]
   mit $f'(x_0) = 0$ folgt damit
   \begin{align*}
   f'(x) < 0 &\quad x\in (x_0 - \eps, x_0) \\
   f'(x) < 0 &\quad x\in(x_0, x_0 + \eps)
   \end{align*}
   $\implies f$ ist streng monoton fallend in $x\in (x_0 - \eps, x_0)$ und streng monoton wachsend in $(x_0, x_0 + \eps)$,
   das heißt $f$ hat in $x_0$ ein striktes lokales Maximum (Analog im Fall $f''(x_0) < 0$)
   #+end_proof
   #+begin_remark latex
   Es ist keine notwendige Bedingung zum Beispiel $f(x) = x^4$ hat lokales Minimum $x_0 = 0$, aber $f''(x_0) = 0$
   #+end_remark
   #+ATTR_LATEX: :options [4.18]
   #+begin_defn latex
   Sei $I$ ein offenes Intervall $f:I \to \mathbb{R}$ heißt
   - (streng) konvex $\iff \Forall \lambda \in (0,1), x, y \in I: f(\lambda x + (1 - \lambda)y) \leq \string(\underarrow[<]{streng}\string) \lambda f(x) + (1 - \lambda) f(y)$
   - (streng) konkav $\iff \Forall \lambda \in (0,1), x, y \in I: f(\lambda x + (1 - \lambda)y) \geq \string(\underarrow[>]{streng}\string) \lambda f(x) + (1 - \lambda) f(y)$
   #+end_defv
   #+ATTR_LATEX: :options [4.19]
   #+begin_ex latex
   $\exp$ ist eine (streng) konvexe Funktion
   Für $\lambda \in (0,1), x < y$ gilt:
   \begin{align*}
   \exp(\lambda x + (1 - \lambda)y) &= \exp(x + (1 - \lambda)(y - x)) = \exp(x)\exp((1 - \lambda)(y - x)) \\
   &= \exp(x)(\underbrace{\lambda + 1 - \lambda}_{= 1} + \sum_{j = 1}^{\infty}(1 - \lambda)^j \frac{(y - x)^j}{j'}) \\
   &= \lambda \exp(x) + (1 - \lambda)\exp(x)(1 + \sum_{j = 1}^{\infty}\underbrace{(1 - \lambda)^{j - 1}}_{< 1}) \frac{(y - x)^j}{j'} \\
   &< \lambda\exp(x) + (1 -  \lambda)\exp(x)\exp(y - x) = \lambda\exp(x) + (1 - \lambda)\exp(y)
   \end{align*}
   #+end_ex
   #+ATTR_LATEX: :options [4.20]
   #+begin_korollar latex
   Sei $I$ offen, $f: I\to\mathbb{R}$ zweimal differenzierbar. Falls $f''(x) \geq 0 \Forall x\in I$, so ist $f$ konvex.
   #+end_korollar
   #+begin_proof latex
   $f'' > 0 \implies f'$ monoton ist wachsend. Für $x = y$ ist $f(\lambda x + (1 - \lambda)y) = \lambda f(x) + (1 - \lambda)f(y)$
   Ohne Beschränkung der Allgemeinheit nehmen wir $x < y, x,y\in I, \lambda \in (0,1)$. Wir setzen $x_\lambda := \lambda x + (1 - \lambda)y$
   Nach dem Mittelwertsatz $\Exists \xi \in (x, x_\lambda)$ und $\eta \in (x_\lambda, y)$ mit
   \[\frac{f(x_\lambda) - f(x)}{x_\lambda - x} \underarrow[=]{Mittelwertsatz} = f'(\xi) \overarrow[\leq]{aus Monotonität} f''(\eta) \underarrow[=]{Mittelwertsatz} \frac{f(y) - f(x_\lambda)}{y - x_\lambda}\]
   \begin{align*}
   \intertext{Es gilt:}
   x_\lambda - x &= \lambda x + (1 -  \lambda)y - x = (1 - \lambda)(y - x) \\
   y - x_\lambda &= y - \lambda x - (1 - \lambda) y = \lambda(y - x) \\
   \intertext{Damit erhält man:}
   \frac{f(x_\lambda) - f(x)}{1 - \lambda} &\leq \frac{(f(y) - f(x_\lambda))}{(y - x_\lambda)} \frac{(x_\lambda - x)}{1 - \lambda} = (f(y) - f(x_\lambda))\frac{(1 - \lambda)(y - x)}{\lambda(y - x)(1 - \lambda)} = \frac{f(y) - f(x_\lambda)}{\lambda} \\
   &\implies f(x_\lambda) \leq \lambda f(x) + (1 - \lambda)f(y) \\
   &\implies f(\lambda x + (1 -\lambda) y) \leq \lambda f(x) + (1 - \lambda)f(y)  \implies f ~\text{ist konvex} \tag*{\qedhere} \\
   \end{align*}
   #+end_proof
   #+ATTR_LATEX: :options [4.21 2. Mittelwertsatz (verallgemeinert)]
   #+begin_thm latex
   Sind die Funktion $f$ und $g$ in $[a, b]$ stetig und in $(a, b)$ differenzierbar und $g'(x) \neq 0$ für $x\in (a, b)$,
   so gibt es ein $c\in (a, b)$ sodass
   \[\frac{f'(c)}{g'(c)} = \frac{f(b) - f(a)}{g(b) - g(a)}\]
   #+end_thm
   #+begin_proof latex
   Wegen $g'(x) \neq 0$ bekommen wir $g(a) \neq g(b)$ (wegen Satz von Rolle). Weiter
   \[\Exists c\in (a,b): \frac{g(b) - g(a)}{b - a} = g'(c) \neq 0\]
   Wir definieren auf $[a, b]$ die Funktion
   \[F(x) := f(x) - \frac{f(b) - f(a)}{g(b) - g(a)} g'(c)\]
   Wir verifizieren $\underline{F(a)} = f(a) = \underline{F(b)}$.
   Nach dem Setz von Rolle gibt es ein $c\in (a, b)$ mit $F'(c) = 0$, das heißt
   \begin{align*}
   0 &= F'(c) = f'(c) = \frac{f(b) - f(a)}{g(b) - g(a)} g'(c) \\
   \intertext{wegen $g'(c) \neq 0$:}
   \frac{f'(c)}{g'(c)} = \frac{f(b) - f(a)}{g(b) - g(a)} \tag*{\qedhere}
   \end{align*}
   #+end_proof
*** Anwendung von MW Satz 2
	#+ATTR_LATEX: :options [4.22 Regeln von L'Hospital]
	#+begin_thm latex
	Es seien $f,g: I \to \mathbb{R}, I = (a, b)$ sodass $g'(x) \neq 0 \Forall x\i I$ und \[\lim_{x \downarrow a} \frac{f'(x)}{g'(x)} =: c \in \mathbb{R}\]
	Dann gelten die Folgenden Regeln:
	1. Im Fall
	   \[\lim_{x\downarrow a} f(x) = \lim_{x\downarrow a} g(x) = 0\]
	   ist $g(x) \neq 0$ in $I$ und es gilt
	   \[\lim_{x\downarrow a} \frac{f(x)}{g(x)} = c\]
	2. Im Fall $f(x) \to \pm \infty, g(x) \to\pm \infty$ für $x\downarrow a$ ist $g(x) \neq 0$ für $a < x y x_\ast \leq b$ und
	   \[\lim x\downarrow a \frac{f(x)}{g(x)} = c\]
	#+end_thm
	#+begin_proof latex
	1. Wir fassen $f$ und $g$ als Funktion auf, die in $a$ stetigs sind $f(a) = g(a) = 0$. Wegen $g'(x) \neq 0$ kann $g$ keine weitere
	   Nullstelle von $g$ in $I$ geben, das heißt $g(x) \neq 0$ in $I$. Satz 4.21 $\implies$
	   \[\Forall x \in I \Exists \xi \in (a, x): \frac{f(x)}{g(x)} = \frac{f'(\xi)}{g'(\xi)}\]
	   $\implies$ für $x \to a$ auch $\xi \to a$ und
	   \[\lim_{x\downarrow a} \frac{f(x)}{g(x)} = \lim_{\xi \downarrow a} \frac{f'(\xi)}{g'(\xi)}\]
	2. Sei $\eps > 0$ beliebig. Nach Vorraussetzung ist $g'(x) \neq 0$ in $(a, b)$.
	   \begin{gather*}
	   \intertext{Wir wählen ein $\delta > 0$ mit $a + \delta \leq x_\ast$, sodass}
	   \Forall x \in (a, a + \delta): f(x) \neq 0 \wedge g(x) \neq 0 \wedge \abs{\frac{f'(x)}{g'(x)} - c} < \eps \\
	   \intertext{Für beliebigs $x, y \in (a, a + \delta)$ mit $f(x) \neq f(y)$}
	   \frac{f(x)}{g(x)} = \frac{f(x) - f(y)}{g(x) - g(y)} \frac{g(x) - g(y)}{f(x) - f(y)} \frac{f(x)}{g(x)} = \frac{f(x) - f(y)}{g(x) - g(y)} \frac{(1 - \frac{g(y)}{g(x)})g(x)}{\underbrace{(1 - \frac{f(y)}{f(y)})}_{x\downarrow a \to 1}f(x)}  \frac{f(x)}{g(x)} \\
	   \implies \Exists \delta_\ast > 0: \Forall x \i n(a, a + \delta_\ast): \abs{\frac{f(x)}{g(x)} - \frac{f(x) - f(y)}{g(x) - g(y)}} < \eps
	   \intertext{Für ein $x$ sodass $a < x < \underbrace{a + \min\{\delta, \delta_\ast\}}_{x_\ast}$ bekommen wir}
	   \abs{\frac{f(x)}{g(x)} - c} < 2\eps \tag*{\qedhere}
	   \end{gather*}
	#+end_proof
	#+ATTR_LATEX: :options [4.23]
	#+begin_ex latex
	$I = (0, 1), f(x) = \ln(x), g(x) = x - 1, f'(x) = \frac{1}{x}, g'(x) = 1$
	\[\lim_{x\to 1} \frac{f'(x)}{g'(x)} = 1, \lim_{x\uparrow 1} \frac{\ln x}{x - 1} = \lim_{x\uparrow 1} \frac{\frac{1}{x}}{1} = 1\]
	#+end_ex
	#+begin_remark latex
	Analoge Aussagen gelten auch für $x \to \pm \infty$. Wir nehmen $y := \frac{1}{x} \to 0$ und
	\[\lim_{x\to\pm\infty} \frac{f(x)}{g(x)} = \lim_{y \to 0_{\pm}} \frac{f(\frac{1}{x})}{g(\frac{1}{x})} = \lim_{\lambda \to \pm \infty} \frac{f'(\lambda)}{g'(\lambda)}\]
	#+end_remark
	#+begin_remark latex
	Bei der Anwendung der Regeln von L'Hospital ist zunächst zu prüfen, ob die Limes von $\frac{f'(x)}{g'(x)}$ überhaupt existiert.
	zum Beispiel
	\[\lim_{x\downarrow 0} \frac{x^2 \sin(\frac{1}{x})}{\sin{x}} = \lim_{x \downarrow} \frac{x}{\sin{x}} x \sin{\frac{1}{x}}  = 0\]
	aber
	\[\lim_{x\downarrow 0} \frac{2x \sin{\frac{1}{x}} + x^2 \cos{\frac{1}{x}}(- \frac{1}{x^2})}{\cos{x}} = \lim_{x\downarrow 0} \frac{2x \sin{\frac{1}{x}} - \cos{\frac{1}{x}}}{\cos{x}} = -\lim_{x\downarrow 0} \cos{\frac{1}{x}}\]
	der existiert nicht
	#+end_remark
	#+begin_remark latex
	Die L'Hospital Regeln kann man auch anwenden in dem Fall
	\[f(x) \to 0, g(x) \to  \infty ~\text{für}~ \lim_{x\downarrow a} f(x) g(x) = \lim_{x\downarrow} \frac{f(x)}{\frac{1}{g(x)}}\]
	Auch für $0^0, \infty^0, 0^\infty$
	#+end_remark
	#+ATTR_LATEX: :options [4.24]
	#+begin_ex latex
	1. $\lim_{x\downarrow 0} x^x$ Wir logarithmieren und erhalten
	   \[\lim_{x\downarrow 0} x\ln x = \lim_{x\downarrow 0} \frac{\ln x}{\frac{1}{x}} = \lim_{x\downarrow 0} \frac{\frac{1}{x}}{-\frac{1}{x^2}} = 0\]
	   und
	   \[\lim_{x\downarrow 0} x^x = \lim_{x\downarrow 0} e^{x\ln x} = e^0 = 1\]
	2. $\lim_{x\to 1} x^{\frac{1}{x - 1}} = \lim_{e^{\frac{1}{x - 1} \ln x}}, \lim_{x \to 1} \frac{1}{x -1} \ln x = \lim_{x \to 1} \frac{\frac{1}{x}}{1} = 1 \implies \lim_{x \to 1} x^{\frac{1}{x - 1}} = e^1 = e$

	#+end_ex
** Taylor Entwicklung
   Wir kennen
   \[e^x = \sum_{k = 0}^{\infty} \frac{x^k}{k!} \quad e^{x - x_0} = \sum_{k = 0}^{\infty} \frac{1}{k!}(x - x_0)^k\]
   Wir wollen untersuchen unter welchen Bedingugen solche Potenzreihe für eine Funktion möglich ist
   und wie man diese aus der Funktion bestimmen kann. Wir haben schon in Übung für die Darstellung für Polynome gezeicht:
   \[p(x) = \sum_{k = 0}^{n} \frac{p^{(k)}(x_0)}{k!}(x - x_0)^k\]
   $p$ -Polynom. Wie ist das bei allgemeinen Funktionen
   #+ATTR_LATEX: :options [4.25]
   #+begin_defn latex
   Für $f:(a, b) \to\mathbb{R}, f$ n-mal stetig differenzierbar definieren wir das n-te Tylor Polynom
   für ein $x_0 \in (x,b)$
   \[f_n(x_0, x):= \sum_{k = 0}^{\infty} \frac{f^{(n)}(x_0)}{k!}(x - x_0)^k\]
   Wir studieren dann den Fehler der Approximation
   #+end_defn
   #+ATTR_LATEX: :options [4.26]
   #+begin_thm latex
   Sei $f: (a, b) \to \mathbb{R}~(n + 1)$ -mal stetig differenzierbar und $t_n(x_0, \cdot)$  ihr
   n-tes Taylor Polynom um ein $x_0\i n(a,b)$. Dann gibt es zu jeden $x\in (a,b)$ ein $\xi$ zwischen
   $x$ und $x_0$, so dass gilt
   \[f(x) = t_n(x_0, x) + \frac{f^{(n + 1)}(\xi)}{(n + 1)!}(x - x_0)^{n + 1}\]
   mit dem sogenannten Lagrangschen Restglied.
   #+end_thm
   #+begin_proof latex
   Wir bemerken, dass $t(x_0, x_0) = f(x_0)$ und definieren das Restglied
   \[R_{n + 1}(y, x):= f(x) - t_n(y, x)\]
   Für festes $x$ ist die Funktion von $y$.
   Weil $f(n + 1)$ stetig differenzierbar ist, ist $R_{n + 1}(y, x)$ mindestens einmal nach $y$ differenzierbar.
   \[\dd{{}}{y}R_{n + 1}(x, y) = \dd{{}}{y}(f(x) - t_n(y, x)) = - \sum_{k = 0}^{\infty}\frac{f^{(n + 1)}(y)}{k!}(x - y)^k + \sum_{k = 1}^{\infty}\frac{f^{(k)}(y)}{(k - 1)!}(x - y)^{k - 1} = -\frac{f^{(n + 1)}(g)}{n!}(x - y)^n\]
   Wir wenden jetzt 2. Mittelwertsatz für $f(y) := R_{n + 1}(y,x), g(y):=(x - y)^{n + 1}$ an.
   \begin{align*}
   R_{n + 1}(x , x) &= f(x) -t_n(x, x) = 0 \\
   \frac{R_{n + 1}(y, x)}{(x - y)^{n + 1}} &= \frac{R_{n + 1}(x, y) - R_{n + 1}(y,x)}{(x - x)^{n + 1} - (x - y)^{n + 1}} = \frac{\dd{}{y}R_{n + 1}(\xi, x)}{-(n + 1)(x - \xi)^n}\\
   \end{align*}
   mit $\xi \in (a, b)$ zwischen $x$ und $y$.
   Mit der obigen Identität für $y = \xi$ ergibt sich
   \[\frac{R_{n + 1}(y, x)}{(x - y)^{n + 1}} = \frac{f^{(n + 1)}(\xi)}{(n + 1)!}\]
   und folgt die Aussage.
   #+end_proof
   #+ATTR_LATEX: :options [4.27]
   #+begin_defn latex
   1. $f:(a,b)\to\mathbb{R}$ heißt glatt (oder $C^\infty$ -Funktion) wenn sie beliebig oft
	  differenzierbar ist, das heißt $\Forall k\in\mathbb{N}$ ihre k-te Ableitung $f^{(k)}$ existiert
   2. Die Taylorreihe von $f$ und ein $x_0 \in (a,b)$ ist dann definiert durch
	  \[t_\infty(x_0, x):= \sum_{k = 0}^{\infty} \frac{f^{(k)}(x_0)}{k!}(x - x_0)^k\]
   3. Konvergiert die Taylorreihe von $f$ und $x_0 \Forall x$ in einer Umgebung von $x_0$
	  und gilt $f(x) = t_\infty(x_0, x)$, so heißt $f$ (reell) analytisch in $x_0$
   #+end_defn
   #+ATTR_LATEX: :options [4.28 Taylor-Entwicklung]
   #+begin_thm latex
   Sei $f:(a, b)\to\mathbb{R}$ eine glatte Funktion mit gleichmäßig beschränker Ableitung und
   \[\sup_{x\in (a, b)}\abs{f^{(n)}(x)} \leq M < \infty \Forall n\in\mathbb{N}\]
   Dann ist $f$ auf $(a, b)$ analytisch, also $\Forall x,y_0 \in (a, b)$ konvergiert die Tylorreihe von $f$
   und es gilt
   \[f(x) = \sum_{k = 0}^{\infty} \frac{f^{(k)}(x_0)}{k!}(x - x_0)^k\]
   #+end_thm
   #+begin_proof latex
   Aus der Restglieddarstellung folt mit Hilfe der Vorraussetzung
   \[\abs{f(x) - t_{x_0, x}} \leq \frac{\abs{f^{(n + 1)}(\xi)}}{(n + 1)!}\abs{x - x_0}^{n + 1} \leq \frac{M}{(n + 1)!}\abs{b - a}^{n + 1}\]
   Zu beliebigen $\eps > 0$ gibt es ein $n_\eps \in \mathbb{N}$, sodass $\Forall n\geq n_\eps$ gilt:
   \[\frac{M}{(n + 1)!}(b - a)^{n + 1} < \eps\]
   was die Behauptung impliziert.
   #+end_proof
   #+begin_remark latex
   Eine glatte Funktion muss nicht analytisch sein. Zum Beispiel:
   \[f(x) = \begin{cases} \exp(-x^{-x^2}) & x \neq 0 \\ 0 & x = 0\end{cases}\]
   \[\lim_{x\to 0}\exp(- \frac{1}{x^2}) = 0\]
   $\implies f$ stetig auf $\mathbb{R}$
   \begin{align*}
   f'(x) &= 2x^{-3}\exp(-\frac{1}{x^2}) \\
   f''(x) &= 4x^{-6}\exp(-\frac{1}{x^2}) + 6x^{-4}\exp(-\frac{1}{x^2}) \\
   &\vdots \\
   f^{(n)}(x) = p_n(x^{-1}) \exp(\frac{1}{x^2}), n\geq 1
   \end{align*}
   wobein $p_n$ ein Polynom ist. Die Ableitungen sind stetig in $x\neq 0$. Wir setzen
   \[y = x^{-2}\]
   Es gilt
   \[\frac{y^k}{e^y} \xrightarrow{y \to 0} 0, k \in \mathbb{N}\]
   $\implies$ die Ableitung sich stetig in $x_0 = 0$ durch die Null vorstehen lassen, weil
   \[f^{(n)}(x) \xrightarrow{x \to 0} 0 n\in \mathbb{N} \implies f\in C^\infty(\mathbb{R})\]
   aber
   \[\sum_{k = 0}^{\infty} f^{(k)}(0) (x - 0)^k = 0\]
   konvergiert $\Forall x$, aber stellt die Funktion $f$ außer $x = 0$ nicht dar.
   #+end_remark
   #+begin_remark latex
   Es gibt auch Funktionen, deren Taylorreihen außer in $x = x_0$ nicht konvergieren
   #+end_remark
** Bemerkung zu Stetigkeit
   stetige Funktion $\supset$ gleichmäßig stetig Funktion $\supset$ Libschitz stetige Funktionen $\supset$ differenzierbare Funktion mit gleichmäßig beschränkten Ableitung (auf beschränkter Menge) $\supset$ stetig differenzierbare Funktionen
   - $f(x) = \frac{1}{x}$ ist stetig, aber nicht gleichmäßig stetig in $(0, 1)$
   - $\sqrt{x}$ ist gleichmäßig stetig, aber nicht Lipschitz stetig
   - $f(x) = x^2, f'(x) = 2x, x\in\mathbb{R} \not\Exists M:\abs{f'(x)} < M \Forall x\in\mathbb{R}$
   - $\abs{f(x) - f(y)} \leq \abs{(x - y)(x + y)} \leq K\abs{x - y}$
   - $f(x) = x^2 x\in (a, b), f'(x) = x, \abs{f'(x)} < \max\abs{2a, 2b}$
   - es gibt Funktionen, die differenzierbar und eine gleichmäßig beschränkte Ableitung haben, aber nicht stetig differenzierbar sind (Beispiel: UB10)
** Anwendung von Taylor-Entwicklung
   #+ATTR_LATEX: :options [4.29]
   #+begin_korollar latex
   Sei $f:(a, b)\to \mathbb{R}$ eine $n$-mal $(n \geq 2)$ stetig differenzierbare Funktion mit
   \[f'(x_0) = f''(x_0) = \ldots = f^{n - 1}(x_0)= 0, f^{(n)}(x_0) \neq 0\]
   für ein $x_0 \in (a, b)$
   Ist $n$ gerae, so hat $f$ in $x_0$ ein striktes lokales Minimum oder Maximum (je nachdem ob $f^{(n)}(x_0) > 0$ oder $f^{(n)}(x_0) < 0$ ist).
   Ist $n$ ungerade, so hat $f$ in $x_0$ einen Wendepunkt.
   #+end_korollar
   #+begin_proof latex
   Ist $f$ $n$-mal differenzierbar in $(a, b)$ und gilt die Bedingung, so folgt mit der Taylor-Entwicklung
   \[f(x) = \sum_{k = 0}^{n - 1} \frac{f^{(k)}(x_0)}{k!}(x - x_0)^k + \frac{f^{(n)}(\xi)}{n!}(x - \xi)^n = f(x_0) + \frac{f^{(n)}(\xi)}{n!}(x - \xi)^n\]
   mit einem $\xi \in (a, b)$ zwichen $x$ und $x_0$. Die Funktion
   \[\Delta_n(x) := \frac{f(x) - f(x_0)}{(x - x_0)^n}, x \neq x_0\]
   konvergiert für $x \to x_0$ nach
   \[\Delta_n(x) \to \frac{f^{(n)}(x_0)}{n!}\]
   Folglich kann die Funktion $\Delta_n$ zu einer auf $(a, b)$ stetigen Funktion fortgesetzt werden. Für diese gilt
   \[f(x) - f(x_0) = \Delta_n(x)(x - x_0)^n, \Delta_n(x_0) = \frac{f^{(n)}(x_0)}{n!}\]
   Falls $\Delta_n(x_0) > 0$, existiert wegen der Stetigkeit von $\Delta_n$ eine $\eps$-Umgebung, $\eps > 0$,
   von $x_0$, in der $\Delta_n > 0$. Auf dieser $\eps$-Umgebung git daher für gerades $n$:
   \[f(x) - f(x_0) = \underbrace{\Delta_n(x)}_{> 0}(x - x_0)^n \implies f(x) > f(x_0)\]
   $\implies f(x_0)$ ist ein lokales Minimum. Analog argumentieren wir für $\Delta_n(x_0) < 0$.
   Für ungerades $n$ dagegen \[f(x) - f(x_0) = \underbrace{\Delta_n(x)}_{> 0}(x - x_0)^n \begin{cases} > 0 & x > x_0 \\ < 0 & x < x_0 \end{cases}\]
   $\implies f(x_0)$ ist kein lokales Extremum
   #+end_proof
   #+ATTR_LATEX: :options [4.30]
   #+begin_ex latex
   1. Exponentialfunktion vesitzt um $x_0$ die Taylor-Entwicklung
	  \[e^x = \sum_{k = 0}^{n} \frac{x^k}{k!} + R_{n + 1}(x), R_{n + 1}(x) = \frac{e^\xi}{(n + 1)!}x^{n + 1}\]
	  stimmt mit der Potenzreihe Darstellung überein.
   2. $\displaystyle \sin(x) = \sum_{k = 0}^{n} \frac{(-1)^k}{(2k + 1)!}x^{2k + 1} + R_{2n + 3}(x), R_{2n + 3}(x) = \frac{\sin^{(2n + 3)}(\xi)}{(2n + 3)!}x^{2n + 3} = \frac{(-1)^{2n + 1}\cos(\xi)}{(2n + 3)!}x^{2n + 3}$
   3. $f(x) = \ln(1 + x), x_0 = 0$
	  \[\ln^{(k)}(1 + x)\mid_{x = 0} = (-1)^{k - 1} \frac{(k - 1)!}{(1 + x)^k}\mid_{x = 0} = (-1)^{k - 1}(k - 1)!\]
	  Dann für $-1 < x \leq 1$
	  \[\ln(1 + x) = \sum_{k = 1}^{n} \frac{\ln^{(k)}(1)}{k!}x^k + r_{n + 1} = \sum_{k = 1}^{n} \frac{(-1)^{k - 1}}{k}x^k + R_{n + 1}, R_{n + 1} = \frac{\ln^{(n)}(1 + \xi)}{(n + 1)!}x^{n + 1} = \frac{(-1)^k}{(n + 1)(2 + \xi)^{n + 1}}x^{n + 1}\]
	  Für festes $x, \xi \in (-1, 1)$ ist $\abs{R_{n + 1}(x)} \leq \frac{c(x)}{n} \xrightarrow{n \to \infty} 0$.
	  Die Reihe konvergiert für $x\in \string(-1, \string]$ (absolut nur für $x\in(-1, 1)$). Zum Beispiel:
	  \[\ln(2) = \sum_{k = 1}^{\infty} \frac{(-1)^{k - 1}}{k}\]
	  und wir bekommen Limes der alternierenden harmonischen Reihe.
   #+end_ex
** Differentation und Grenzprozesse
   \[\sum_{k = 1}^{\infty} a(x) = \lim_{n \to \infty} \sum_{k = 1}^{\infty}a(x)\]
   Motivation: Frage:
   \[\dd{}{x}(\sum_{k = 1}^{\infty}a(x)) \overset{?}{=} \sum_{k = 1}^{\infty}\dd{}{x}a(x)\]
   #+ATTR_LATEX: :options [Patalogische Beispiele]
   #+begin_remark latex
   1. Eine gleichmäßig konvergente Folge differenzierbarer Funktionen mit dicht differenzierbarem Limes
	  \[f_n(x) := \begin{cases} \frac{n}{2}x^2 + \frac{1}{2n} & \abs{x} \leq \frac{1}{n} \\ \abs{x} & \abs{x} > \frac{1}{n} \end{cases}\]
	  - $f_n$ sind differenzierbar
	  - $f_n \xrightarrow{x \to \infty} \abs{x}$ auf $\mathbb{R}$ gleichmäßig und nicht differenzierbar
   2. $f_n(x) := \frac{\sin(n^2 x)}{n}$
	  - $f_n$ differenzierbar
	  - $f_n \xrightarrow{n\to\infty} 0$ gleichmäßig auf $\mathbb{R}$, Limes Funktion $f(x) \equiv 0$
	  - $f'_n(x) = n\cos{n^2}x$ ist in $x = m\pi, m\in \mathbb{R}$ divergent $\implies f'_n(x) \not\to f'(x)$
   3. $f_n(x) := x - \frac{x^n}{n}$
	  - $f_n$ differenzierbar
	  - $f_n \to f, f(x) = x$ gleichmäßig auf $[0, 1]$, aber $f'_n(x) = 1 - x^{n - 1}$ in $x = 1, f'_n(1) = 0$ nicht gegen $f'(x) = 1$ konvergent
   #+end_remark
   #+ATTR_LATEX: :options [Stabilität der Differenzierbarkeit]
   #+begin_thm latex
   Sei $(f_n)_{n\in\mathbb{N}}$ eine Folge stetig differenzierbarer Funktionen auf einem beschränktem Intervall, welche Punktweise gegen ein Funktion $f$ konvergiert.
   Ist die Folge der Ableitungen $(f_n)'_{n\in\mathbb{N}}$ gleichmäßig konvergent gegen einen $f^\ast$, so ist auch $f$ differenzierbar und es gilt
   \[f' = f^\ast \iff \dd{}{x}(\lim_{n\to\infty} f_n(x)) = \lim_{n \to\infty} f_n'(x)\]
   #+end_thm
   #+begin_proof latex
   Sei $x_0 \in I$ und
   \[\Delta(x) := \begin{cases} \frac{f(x) - f(x_0)}{x - x_0} & x\neq x_0 \\ f^{\ast}(x_0) & x = x_0 \end{cases}\]
   Falls $f_{(x_0)} = f^\ast(x_0) \iff \Delta(x)$ ist stetig in $x = x_0$. Für $x \in i\setminus\{x_0\}$ konvergiert
   \[\Delta_n(x) := \frac{f_n(x) - f_n(x_0)}{x - x_0}\xrightarrow{n\to\infty} \Delta(x)\]
   Nach dem Mittelwertsatz gibt es ein $\xi_n \in I$, zwischen $x$ und $x_0$, sodass
   \[f'_n(\xi_n) = \frac{f_n(x) - f_n(x_0)}{x - x_0} = \Delta_n(x)\]
   Folglich ist
   \[\Delta(x) - \Delta(x_0) = \Delta{x} - \Delta_n(x) + f_n'(\xi_n) - f^\ast(x)\]
   Sei $\eps > 0$ gegeben. Wir wählen ein $n_0 \in \mathbb{N}$ und ein $\delta > 0$, sodass $\Forall n\geq n_0$ und $x \in U_\sigma(x_0)$ gilt
   \[\abs{f'_n(x) - f^\ast(x_0)} \leq \abs{f'_n(x) - f^\ast(x)} + \abs{f^\ast(x) - f^\ast(x_0)} < \frac{\eps}{2}\]
   Mit $x\in U_\sigma(x_0)$ gilt
   \[\xi_n \in U_\sigma(x_0), \Forall x \in U_\sigma(x_0)\setminus\{x_0\} \Exists n_1(x) \geq n_0: \Forall n\geq n_1(x): \abs{\Delta(x) - \Delta_n(x)} < \frac{\eps}{2}\]
   Für beliebige $x\in U_\sigma(x_0)\setminus\{x_0\}$ folgt dann, dass für $x \geq n_1(x)$
   \[\Delta(x) -\Delta(x_0) \leq \abs{\Delta(x) - \Delta_n(x)} + \abs{f'_n(\xi_n) - f} < \eps \implies \Delta(x) \xrightarrow{x\to x_0} \Delta(x_0)\]
   #+end_proof
