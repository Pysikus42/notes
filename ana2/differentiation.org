* Stetigkeit und Differenzierbarkeit im $ℝ^n$
  #+begin_defn latex
  Eine Funktion $f: D ⊂ \mathbb{K}^n \to \mathbb{K}^m, m, n ∈ ℕ \setminus\{0\}, D \neq \emptyset$, ist stetig in einem $a ∈ D$, wenn
  \[∀ ε > 0 ∃ δ > 0 ∀ x ∈ D: \norm{x - a} < δ ⇒ \norm{f(x) - f(a)} < ε\]
  #+end_defn
  #+begin_remark latex
  Es gelten auch im Mehrdimensionalen die Permanenzeigenschaften, das heißt $f, g$ stetig $⇒ f + g, f \circ g$ sind stetig.
  #+end_remark
  #+begin_thm latex
  Eine stetige Funktion $f: D ⊂ \mathbb{K}^n \to \mathbb{K}^m$ ist auf einer kompakten Menge $K ⊂ D$ beschränkt, das heißt für jede kompakte Menge $K$ existiert eine Konstante $M_k$, sodass
  \[∀ x ∈ K \norm{f(x)} < M_k\]
  #+end_thm
  #+begin_proof latex
  Angenommen $f$ wäre auf $K$ unbeschränkt, dann gäbe es zu jedem $k ∈ ℕ$ ein $x_k ∈ K$ mit $\norm{f(x_k)} > K$.
  Da $K$ kompakt hat die Folge $(x_k)_{k ∈ ℕ}$ eine konvergente Teilfolge $(x_{k_j})_{j ∈ ℕ}$ für die gilt $x_{k_j} \xrightarrow{j \to ∞} x ∈ K$.
  Da $f$ stetig $f(x_{k_j}) \to f(x)$ und $\norm{f(x)} < ∞$, was im Widerspruch steht zu $\norm{f(x_k)} \xrightarrow{k \to ∞} ∞$.
  #+end_proof
  #+begin_thm latex
  Eine stetige Funktion $f: D ⊂ \mathbb{K}^n \to ℝ$ nimmt auf jeder (nicht leeren) kompakten Menge $K ⊂ D$ ihr Minimum und Maximum an.
  #+end_thm
  #+begin_proof latex
  Nach Satz 2.2 besitzt $f$ eine obere Schranke auf $K$
  \[\mathcal{K} := \sup_{x ∈ K} f(x)\]
  Dazu $(x_k)_{k ∈ ℕ} ⊆ K$, sodass $f(x_k) \xrightarrow{k \to ∞} \mathcal{K}$. Da $K$ kompakt existiert eine konvergente Teilfolge $(x_{k_j})_{j ∈ ℕ}$
  und ein $x_{max}$, sodass $x_{k_j} \xrightarrow{j \to ∞} x_{max}$. Da $f$ stetig, gilt $f(x_{k_j}) \to f(x_{max})$.
  #+end_proof
  #+begin_remark latex
  Auf diese Weise lassen sich die Ergebnisse der Stetigkeit aus dem Eindimensionalen ins Mehrdimensionale verallgemeinern.
  #+end_remark
  Im folgenden Teil sei $D ⊆ ℝ^n$ offen, $\mathbb{K} = ℝ$
  #+begin_defn latex
  Eine Funktion $f: D \to ℝ$ heißt in einem Punkt $x ∈ D$ partiell differenzierbar bezüglich der i-ten Koordinatenrichtung, falls der Limes
  \[\lim_{h \to 0}  \frac{f(x + h e_i) - f(x)}{h} =: \frac{\partial f}{\partial x_i}(x) =: \partial_i f(x)\]
  existiert. Existieren in allen Punkten $x ∈ D$ *alle* partiellen Ableitungen, so heißt $f$ partiell differenziebar. Sind alle partiellen Ableitungen stetig auf $D$, so heißt $f$ stetig
  partiell differenziebar. Eine Funktion $f: D \to ℝ^m$ heißt (stetig) partiell differenzierbar, wenn $f_i, i = 1, \dots, m$ (stetig) partiell differenziebar.
  #+end_defn
  #+begin_remark latex
  Die Ableitungsregeln aus dem Eindimesionalen übertragen sich auf partielle Ableitungen.
  #+end_remark
  #+begin_ex* latex
  1. Polynome sind stetig partiell differenziebar. Sei $p: D ⊂ ℝ^2 \to ℝ, (x_1, x_2) ↦ a_{01} x_2 + a_{11}x_1 x_2 + a_{02} x_2^2 + a_{21} x_1^2 x_2$. Dann ist
	 \[\frac{\partial p}{\partial x_1}(x_1, x_2) = a_{11} x_2 + 2 a_{21} x_1 x_2 \quad \frac{\partial p}{\partial x_2} = a_{01} + a_{11} x_1 + 2 a_{02} x_2 + a_{21} x_1^2\]
  2. $\norm{·}_2: ℝ^k \setminus \{0\} \to ℝ$ ist stetig partiell differenzierbar, da
	 \[\frac{\partial \norm{·}}{x_i} = \frac{1}{2} \frac{2 x_i}{(x_1^2 + \dots + x_k^2)^{\frac{1}{2}}} = \frac{x_i}{\norm{x}_2}\]
  3. $f: ℝ^2 \to ℝ, (x_1, x_2) ↦ \frac{x_1 x_2}{(x_1^2 + x_2^2)^2}$ für $x \neq 0, f(0) = 0$
	 \[\frac{\partial f}{\partial x_1}(x) = \frac{x_2}{(x_1^2 + x_2^2)^2} - 4 \frac{x_1^2 x_2}{(x_1^2 + x_2^2)^3}, x \neq 0\]
	 Für $x = 0$ ist $f(0) = 0$
	 \[⇒ \lim_{h \to 0} \frac{f(x e_i) - f(0)}{h} = 0\]
	 Sei $x_ε (ε, ε)$ und damit gilt $\norm{x_ε}_2 \xrightarrow{ε \to 0} 0$
     \[f(x_ε) = \frac{ε^2}{4 ε^4} = \frac{1}{4 ε_2} \xrightarrow{ε \to 0} ∞\]
  #+end_ex*
  #+begin_thm latex
  Die Funktion $f: D \to ℝ$ habe in einer Kugelumgebung $K_r(x) ⊂ D$ eines Punktes $x ∈ D$ beschränkte partielle Ableitungen, das heißt
  \[\sup_{y ∈ K_r(x)} \abs{\frac{\partial f}{\partial x_i}} \leq M, i = 1, \dots, n\]
  dann ist $f$ stetig in $x$.
  #+end_thm
  #+begin_proof latex
  Es genügt $n = 2$. Für $(y_1, y_2) ∈ K_r(x)$
  \begin{align*}
  f(y_1, y_2) - f(x_1, x_2) &= f(y_1, y_2) - f(x_1, y_2) + f(x_1, y_2) - f(x_1, x_2) \\
  \intertext{Nach dem 1-D Mittelwertsatz existieren $ξ, η ∈ K_r(x)$, sodass}
  \abs{f(y_1, y_2) - f(x_1, x_2)} &= \frac{\partial f}{\partial x_1}(ξ, y_2)(y_1 - x_1) + \frac{\partial f}{\partial x_2}(x_1, η)(y_2 - x_2) \\
  &\leq M(\abs{y_1 - x_1} + \abs{y_2 - x_2})
  \end{align*}
  #+end_proof
  Höhere partielle Ableitungen definieren sich durch sukzessives Ableiten, das heißt
  \[\frac{\partial}{\partial x_1} \dots \frac{\partial}{\partial x_k} f(x) = \frac{\partial^k f}{\partial x_{i_1} \dots \partial x_{i_k}}\]
  #+begin_ex* latex
  \[\frac{x_1}{x_2} := \frac{x_1^3 x_2 - x_1 x_2^3}{x_1^2 + x_2^2}\]
  für $(x_1, x_2) \neq (0, 0), f(0, 0) = 0$. $f$ zweimal partiell diffbar, aber
  \[\frac{\partial^2}{\partial x_1 \partial x_2} f(0, 0) \neq \frac{\partial^2}{\partial x_2 \partial x_1} f(0, 0)\]
  #+end_ex*
  #+begin_thm latex
  Eine Funktion $f: D \to ℝ$ sei in einer Umgebung $K_r(x) ⊂ D$ eines Punktes $x ∈ D$ zweimal stetig partiell differenzierbar, dann gilt
  \[\frac{\partial^2}{\partial x_i \partial x_j}f(x) = \frac{\partial^2}{\partial x_j \partial x_i} f(x), i, j = 1, \dots, n\]
  #+end_thm
  #+begin_proof latex
  $n = 2$. Sei $A:= f(x_1 - h_1, x_2 + h_2) - f(x_1 + h_1, x_2) - f(x_1, x_2 + h_2) + f(x_1, x_2)$.
  \[φ(x_1) := f(x_1, x_2 + h_2) - f(x_1, x_2) ⇒ A = φ(x_1 + h_1) - φ{x_1}\]
  Mit dem Muttelwertsatz erhalten wir $A = h_1 φ'(x_1 + θ_1 h_1), θ_1 ∈ (0, 1)$.
  \[φ'(x_1) = \frac{\partial}{\partial x_1} f(x_1, x_2 + h_2) - \frac{\partial}{\partial x_1} f(x_1, x_2) = h_2 \frac{\partial^2}{\partial x_2 x_1} f(x_1, x_2 + θ_1' h_2), θ_1' ∈ (0, 2)\]
  Analog verfahre man mit $x_2$ und erhalte für $ψ(x_2) := f(x_1 + h_1, x_2) - f(x_1, x_2)$
  \[A = ψ(x_2 - h_2) - ψ(x_2) = h_2 ψ'(x_2 + θ_2 h_2) = h_1 h_2 \frac{\partial^2}{\partial x_1 \partial x_2} f(x_1 + θ_2 h_1, x_2 θ'_2 h_2)\]
  \[⇒ \frac{\partial^2}{\partial x_2 \partial x_1} f(x_1 + θ_1 h_1, x_2 + θ_1'h_2) = \frac{\partial^2}{\partial x_1 \partial x_2} f(x_1 + θ_2 h_1, x_2 + θ_2' h_2)\]
  \[\xRightarrow{h_1, h_2 \to 0} \frac{\partial^2}{\partial x_2 \partial x_1} f(x_1, x_2) = \frac{\partial^2}{\partial x_1 \partial x_2}f(x_1, x_2)\]
  #+end_proof
  #+begin_defn latex
  $f: D \to ℝ$ partiell differenzierbar.
  \[\grad f(x) := (\pp{}{x_1}f, \dots, \pp{}{x_n}f)^T ∈ ℝ^n\]
  heißt *Gradient* von $f$ in $x ∈ D$. Man schreibt $∇f(x) := \grad f$. $f: D \to ℝ^n$ partiell differenzierbar.
  \[\Div f(x) := \pp{}{x_1}f_1(x) + \dots + \pp{}{x_n}f_n\]
  Es gilt:
  \[\div \grad f(x) := \sum_{i = 1}^{n}\frac{\partial^2}{\partial x_i^2} f_i =: Δf(x)\]
  #+end_defn
  #+begin_defn latex
  $f: D \to ℝ^m$ partiell differenzierbar. Die Matrix der ersten partiellen Ableitungen
  \[J_f := (\pp{f_i}{x_j})_{\substack{i = 1, \dots, w \\ j = 1, \dots, n}} ∈ ℝ^{n×w}\]
  heißt die *Jacobi-Matrix* (manchmal auch F*undametalmatrix*) von $f$ in $x$.
  Im Fall $n = m$ bezeichnet man $\det(J_f)$ als *Jacobideterminante*.
  #+end_defn
  #+begin_defn latex
  $f:D \to ℝ$ zweimal partiell differenzierbar. Die Matrix der zweiten Ableitungen
  \[H_f(x) := (\frac{\partial^2}{\partial x_i \partial x_j}f)_{\substack{i = 1,\dots, n\\ j = 1, \dots, w}} ∈ ℝ^{n×m}\]
  heißt *Hesse-Matrix*.
  #+end_defn
  #+begin_defn latex
  Sei $f: D \to ℝ^m$, dann nennen wri $f$ in einem Punkt $x ∈ D$ (total differenziebar), wenn die Funktion $f$ in $x$
  sich linear approximieren lässt, das heißt es gibt eine lineare Abbildung $Df(x): ℝ^n \to ℝ^m$ (Differential) sodass in einer kleinen Umgebung von $x$ gilt:
  \[f(x + h) = f(x) + Df(x)h + w(h), h ∈ ℝ^n, x + h ∈ D\]
  mit einer Funktion $w: D \to ℝ^m$, die die Eigenschaft hat
  \[\lim_{\substack{x + h ∈ D\\ \norm{h}_2 \to 0}} \frac{\norm{w{h}}_2}{\norm{h}_2} = 0\]
  alternativ: $w(h) = \mathcal{o}(\norm{h}_2)$
  #+end_defn
  #+begin_thm latex
  Für Funktionen $f: D \to ℝ^m$ gilt:
  1. Ist $f$ in $x ∈ D$ differenziebar, so ist $f$ auch in $x$ partiell differenzierbar und das Differential von $f$ ist gegeben durch die Jacobi-Matrix.
  2. Ist $f$ partiell differenziebar in einer Umgebung von $x$ und sind zusätzlich die partiellen Ableitungen stetig in	$x$, so ist $f$ in $x$ differenzierbar.
  #+end_thm
  #+begin_proof latex
  1. Für differenzierbares $f$ gilt für $i = 1,2$:
	 \[\lim_{h \to 0} \frac{f(x + he_i) - f(x)}{h} = \lim_{h \to 0} (Df(x)e_i + \frac{w(h)}{h}) = Df(x)e_i\]
  2. Für ein stetig partiell differenzierbares $f$ gilt mit $h = (h_1, h_2)$:
	 \begin{align*}
	 f(x + h) - f(x) &= f(x_1 + h_1, x_2 + h_2) - f(x_1 + h_1, x_2) + f(x_1 + h_1, x_2) - f(x_1, x_2) \\
	 \intertext{Mittelwertsatz}
  	 &= h_2 \pp{f}{x_2}(x_1 + h_1, x_2 + θ_2 h_2) + h_1 \pp{f}{x_1}(x_1 + θ_1 h_1, x_2) \tag*{$θ_1, θ_2 ∈ (0, 1)$} \\
	 &= h_2(\pp{f}{x_2}(x_1, x_2 ) + ω_2(h_1, h_2)) + h_1(\pp{f}{x_1}(x_1, x_2) + ω_1(h_1, h_2)) \\
	 ω_1(h_1, h_2) &:= \pp{f}{x_1}(x_1 + θ_1 h_1, x_2) - \pp{f}{x_1}(x_1, x_2) \xrightarrow{h_1, h_2 \to 0} 0 \\
	 ω_2(h_1, h_2) &:= \pp{f}{x_2}(x_1 + h_1, x_2 + θ_2 h_2) - \pp{f}{x_2}(x_1, x_2) \xrightarrow{h_1, h_2 \to 0} 0\\
     \end{align*}
	 Also ist $f$ differenzierbar mit Ableitungen $Df(x) = ∇f(x)$.
  #+end_proof
  #+begin_remark latex
  Es gelten folgende Implikationen:
  stetig partiell differenzierbar $⇒$ (total) differenzierbar $⇒$ partiell differenzierbar.
  #+end_remark
  #+begin_thm latex
  Seien $D_f ⊂ ℝ^n, Dg ⊆ ℝ^m$ offen und $g: D_g \to ℝ^n, f: D_f \to ℝ^r$. Ist $g$ im Punkt $x ∈ D_g$ differenzierbar und $f$ in $y = g(x) ∈ D_f$ differenzierbar,
  so ist die Komposition $h = f \circ g$ im Punkt $x$ differenzierbar. Es gilt $D_x h(x) = D_y f(g(x)) · D_x g(x)$. Hierbei ist $·$ die Matrixmultiplikation.
  #+end_thm
  #+begin_proof latex
  Nach Vorraussetzung $x ∈ D_g$ sodass $g(x) = y ∈ D_f$. Da sowohl $f$ als auch $g$ differenzierbar
  \begin{align*}
  g(x + h_1) &= g(x) + D_x g(x)h_1 + ω_g(h_1) \\
  f(y + h_2) &= f(y) + D_y f(y)h_2 + ω_f(h_2) \\
  \lim_{\substack{x + h_1 ∈ D_y \\ \norm{h_1} \to 0}} \frac{\norm{ω_g(h_1)}}{\norm{h_1}} &= 0 \\
  \lim_{\substack{y + h_2 ∈ D_y \\ \norm{h_2} \to 0}} \frac{\norm{ω_f(h_2)}}{\norm{h_2}} &= 0 \\
  \string(f \circ g\string)(x + h_1) &= f(g(x + h_1)) = f(y + η), \quad η := D_x g(x)h_1 + ω_g(h_1) \\
  &= f(y) + D_y f(y) η + ω_f(η) \\
  &= f(y) + D_y f(y) D_x g(x) h_1 + D_y f(y) ω_g(h_1) + ω_f(D_x g(x)h_1 + ω_g(h_1)) \\
  &= (f\circ g)(x) + D_y f(y) D_x g(x) h_1 + ω_{f \circ g}(h_1) \\
  ω_{f\circ g}(h_1) &:= D_y f(y)ω_g(h_1) + ω_f(D_x g(x)h_1 + ω_g(h_1)) \\
  \intertext{Es bleibt zu zeigen $ω_{f \circ g} = \mathcal{o}(h_1)$. Nach Vorraussetzung gilt $ω_{f \circ g} \xrightarrow{h_1 \to 0} 0$}
  \end{align*}
  #+end_proof
  #+begin_lemma latex
  Sei $A:[a, b] \to ℝ^{n × m}$ stetig, dann gilt
  \[\norm{∫ 0^1 A(s) \d s}_M \leq ∫_0^1 \norm{A(s)_M \d s}, \norm{A}_M := \max\{\abs{λ} \mid λ ∈ σ(A)\}\]
  $∫A = (∫a_{ij})_{ij}, σ(A) :=$ Menge der Eigenwerte von $A$
  #+end_lemma
  #+begin_thm latex
  Sei $f: D \to ℝ^m$ stetig differenzierbar mit $J_f$ als Jacobi-Matrix, so gilt
  \[f(x + h) - f(x) = (∫_0^1 J_f (x + sh)\d s)h\]
  #+end_thm
  #+begin_proof latex
  Definiere $g_j(s) := f_j(x + sh)$, dann ist $g_{j_1}:[0, 1] \to ℝ$, also gilt
  \[f_j(x + sh) - f_j(x) = g_j(1) - g_j(0) = ∫_0^1 g_j'(s) \d s = ∫_0^1 \sum_{i = 1}^{n} \pp{f_j}{x_i}(x + sh)h_i \d s\]
  #+end_proof
  #+begin_remark latex
  Im Fall $m = 1$ kann man aus dem Mittelwertsatz für Integrale schließen, dass
  \[f(x + h) - f(x) = ∫_0^1 J_f(x + sh) h \d s = J_f(x + τh) h\]
  $x_1 + h = x_2 ⇒ h = x_2 - x_1$
  #+end_remark
  #+begin_korollar latex
   Sei $f: D \to ℝ^m$ stetig differenzierbar. Ferner sei $x ∈ D$ mit $K_r(x) ⊂ D, r > 0$, dann gilt
   \[\norm{f(x) - f(y)}_2 \leq M\norm{x - y}_2, y ∈ K_r(x), M := \sup_{z ∈ K_r(x)}\norm{J_f(z)}_M\]
  #+end_korollar
