* Stetigkeit und Differenzierbarkeit im $ℝ^n$
  #+begin_defn latex
  Eine Funktion $f: D ⊂ \mathbb{K}^n \to \mathbb{K}^m, m, n ∈ ℕ \setminus\{0\}, D \neq \emptyset$, ist stetig in einem $a ∈ D$, wenn
  \[∀ ε > 0 ∃ δ > 0 ∀ x ∈ D: \norm{x - a} < δ ⇒ \norm{f(x) - f(a)} < ε\]
  #+end_defn
  #+begin_remark latex
  Es gelten auch im Mehrdimensionalen die Permanenzeigenschaften, das heißt $f, g$ stetig $⇒ f + g, f \circ g$ sind stetig.
  #+end_remark
  #+begin_thm latex
  Eine stetige Funktion $f: D ⊂ \mathbb{K}^n \to \mathbb{K}^m$ ist auf einer kompakten Menge $K ⊂ D$ beschränkt, das heißt für jede kompakte Menge $K$ existiert eine Konstante $M_k$, sodass
  \[∀ x ∈ K \norm{f(x)} < M_k\]
  #+end_thm
  #+begin_proof latex
  Angenommen $f$ wäre auf $K$ unbeschränkt, dann gäbe es zu jedem $k ∈ ℕ$ ein $x_k ∈ K$ mit $\norm{f(x_k)} > K$.
  Da $K$ kompakt hat die Folge $(x_k)_{k ∈ ℕ}$ eine konvergente Teilfolge $(x_{k_j})_{j ∈ ℕ}$ für die gilt $x_{k_j} \xrightarrow{j \to ∞} x ∈ K$.
  Da $f$ stetig $f(x_{k_j}) \to f(x)$ und $\norm{f(x)} < ∞$, was im Widerspruch steht zu $\norm{f(x_k)} \xrightarrow{k \to ∞} ∞$.
  #+end_proof
  #+begin_thm latex
  Eine stetige Funktion $f: D ⊂ \mathbb{K}^n \to ℝ$ nimmt auf jeder (nicht leeren) kompakten Menge $K ⊂ D$ ihr Minimum und Maximum an.
  #+end_thm
  #+begin_proof latex
  Nach Satz 2.2 besitzt $f$ eine obere Schranke auf $K$
  \[\mathcal{K} := \sup_{x ∈ K} f(x)\]
  Dazu $(x_k)_{k ∈ ℕ} ⊆ K$, sodass $f(x_k) \xrightarrow{k \to ∞} \mathcal{K}$. Da $K$ kompakt existiert eine konvergente Teilfolge $(x_{k_j})_{j ∈ ℕ}$
  und ein $x_{max}$, sodass $x_{k_j} \xrightarrow{j \to ∞} x_{max}$. Da $f$ stetig, gilt $f(x_{k_j}) \to f(x_{max})$.
  #+end_proof
  #+begin_remark latex
  Auf diese Weise lassen sich die Ergebnisse der Stetigkeit aus dem Eindimensionalen ins Mehrdimensionale verallgemeinern.
  #+end_remark
  Im folgenden Teil sei $D ⊆ ℝ^n$ offen, $\mathbb{K} = ℝ$
  #+begin_defn latex
  Eine Funktion $f: D \to ℝ$ heißt in einem Punkt $x ∈ D$ partiell differenzierbar bezüglich der i-ten Koordinatenrichtung, falls der Limes
  \[\lim_{h \to 0}  \frac{f(x + h e_i) - f(x)}{h} =: \frac{\partial f}{\partial x_i}(x) =: \partial_i f(x)\]
  existiert. Existieren in allen Punkten $x ∈ D$ *alle* partiellen Ableitungen, so heißt $f$ partiell differenzierbar. Sind alle partiellen Ableitungen stetig auf $D$, so heißt $f$ stetig
  partiell differenzierbar. Eine Funktion $f: D \to ℝ^m$ heißt (stetig) partiell differenzierbar, wenn $f_i, i = 1, \dots, m$ (stetig) partiell differenzierbar.
  #+end_defn
  #+begin_remark latex
  Die Ableitungsregeln aus dem Eindimensionalen übertragen sich auf partielle Ableitungen.
  #+end_remark
  #+begin_ex* latex
  1. Polynome sind stetig partiell differenzierbar. Sei $p: D ⊂ ℝ^2 \to ℝ, (x_1, x_2) ↦ a_{01} x_2 + a_{11}x_1 x_2 + a_{02} x_2^2 + a_{21} x_1^2 x_2$. Dann ist
	 \[\frac{\partial p}{\partial x_1}(x_1, x_2) = a_{11} x_2 + 2 a_{21} x_1 x_2 \quad \frac{\partial p}{\partial x_2} = a_{01} + a_{11} x_1 + 2 a_{02} x_2 + a_{21} x_1^2\]
  2. $\norm{·}_2: ℝ^k \setminus \{0\} \to ℝ$ ist stetig partiell differenzierbar, da
	 \[\frac{\partial \norm{·}}{x_i} = \frac{1}{2} \frac{2 x_i}{(x_1^2 + \dots + x_k^2)^{\frac{1}{2}}} = \frac{x_i}{\norm{x}_2}\]
  3. $f: ℝ^2 \to ℝ, (x_1, x_2) ↦ \frac{x_1 x_2}{(x_1^2 + x_2^2)^2}$ für $x \neq 0, f(0) = 0$
	 \[\frac{\partial f}{\partial x_1}(x) = \frac{x_2}{(x_1^2 + x_2^2)^2} - 4 \frac{x_1^2 x_2}{(x_1^2 + x_2^2)^3}, x \neq 0\]
	 Für $x = 0$ ist $f(0) = 0$
	 \[⇒ \lim_{h \to 0} \frac{f(x e_i) - f(0)}{h} = 0\]
	 Sei $x_ε (ε, ε)$ und damit gilt $\norm{x_ε}_2 \xrightarrow{ε \to 0} 0$
     \[f(x_ε) = \frac{ε^2}{4 ε^4} = \frac{1}{4 ε^2} \xrightarrow{ε \to 0} ∞\]
  #+end_ex*
  #+begin_thm latex
  Die Funktion $f: D \to ℝ$ habe in einer Kugelumgebung $K_r(x) ⊂ D$ eines Punktes $x ∈ D$ beschränkte partielle Ableitungen, das heißt
  \[\sup_{y ∈ K_r(x)} \abs{\frac{\partial f}{\partial x_i}} \leq M, i = 1, \dots, n\]
  dann ist $f$ stetig in $x$.
  #+end_thm
  #+begin_proof latex
  Es genügt $n = 2$. Für $(y_1, y_2) ∈ K_r(x)$
  \begin{align*}
  f(y_1, y_2) - f(x_1, x_2) &= f(y_1, y_2) - f(x_1, y_2) + f(x_1, y_2) - f(x_1, x_2) \\
  \intertext{Nach dem 1-D Mittelwertsatz existieren $ξ, η ∈ K_r(x)$, sodass}
  \abs{f(y_1, y_2) - f(x_1, x_2)} &= \frac{\partial f}{\partial x_1}(ξ, y_2)(y_1 - x_1) + \frac{\partial f}{\partial x_2}(x_1, η)(y_2 - x_2) \\
  &\leq M(\abs{y_1 - x_1} + \abs{y_2 - x_2})
  \end{align*}
  #+end_proof
  Höhere partielle Ableitungen definieren sich durch sukzessives Ableiten, das heißt
  \[\frac{\partial}{\partial x_1} \dots \frac{\partial}{\partial x_k} f(x) = \frac{\partial^k f}{\partial x_{i_1} \dots \partial x_{i_k}}\]
  #+begin_ex* latex
  \[\frac{x_1}{x_2} := \frac{x_1^3 x_2 - x_1 x_2^3}{x_1^2 + x_2^2}\]
  für $(x_1, x_2) \neq (0, 0), f(0, 0) = 0$. $f$ zweimal partiell differenzierbar, aber
  \[\frac{\partial^2}{\partial x_1 \partial x_2} f(0, 0) \neq \frac{\partial^2}{\partial x_2 \partial x_1} f(0, 0)\]
  #+end_ex*
  #+begin_thm latex
  Eine Funktion $f: D \to ℝ$ sei in einer Umgebung $K_r(x) ⊂ D$ eines Punktes $x ∈ D$ zweimal stetig partiell differenzierbar, dann gilt
  \[\frac{\partial^2}{\partial x_i \partial x_j}f(x) = \frac{\partial^2}{\partial x_j \partial x_i} f(x), i, j = 1, \dots, n\]
  #+end_thm
  #+begin_proof latex
  $n = 2$. Sei $A:= f(x_1 - h_1, x_2 + h_2) - f(x_1 + h_1, x_2) - f(x_1, x_2 + h_2) + f(x_1, x_2)$.
  \[φ(x_1) := f(x_1, x_2 + h_2) - f(x_1, x_2) ⇒ A = φ(x_1 + h_1) - φ{x_1}\]
  Mit dem Mittelwertsatz erhalten wir $A = h_1 φ'(x_1 + θ_1 h_1), θ_1 ∈ (0, 1)$.
  \[φ'(x_1) = \frac{\partial}{\partial x_1} f(x_1, x_2 + h_2) - \frac{\partial}{\partial x_1} f(x_1, x_2) = h_2 \frac{\partial^2}{\partial x_2 x_1} f(x_1, x_2 + θ_1' h_2), θ_1' ∈ (0, 2)\]
  Analog verfahre man mit $x_2$ und erhalte für $ψ(x_2) := f(x_1 + h_1, x_2) - f(x_1, x_2)$
  \[A = ψ(x_2 - h_2) - ψ(x_2) = h_2 ψ'(x_2 + θ_2 h_2) = h_1 h_2 \frac{\partial^2}{\partial x_1 \partial x_2} f(x_1 + θ_2 h_1, x_2 θ'_2 h_2)\]
  \[⇒ \frac{\partial^2}{\partial x_2 \partial x_1} f(x_1 + θ_1 h_1, x_2 + θ_1'h_2) = \frac{\partial^2}{\partial x_1 \partial x_2} f(x_1 + θ_2 h_1, x_2 + θ_2' h_2)\]
  \[\xRightarrow{h_1, h_2 \to 0} \frac{\partial^2}{\partial x_2 \partial x_1} f(x_1, x_2) = \frac{\partial^2}{\partial x_1 \partial x_2}f(x_1, x_2)\]
  #+end_proof
  #+begin_defn latex
  $f: D \to ℝ$ partiell differenzierbar.
  \[\grad f(x) := (\pp{}{x_1}f, \dots, \pp{}{x_n}f)^T ∈ ℝ^n\]
  heißt *Gradient* von $f$ in $x ∈ D$. Man schreibt $∇f(x) := \grad f$. $f: D \to ℝ^n$ partiell differenzierbar.
  \[\Div f(x) := \pp{}{x_1}f_1(x) + \dots + \pp{}{x_n}f_n(x)\]
  Es gilt:
  \[\Div \grad f(x) := \sum_{i = 1}^{n}\frac{\partial^2}{\partial x_i^2} f_i =: Δf(x)\]
  #+end_defn
  #+begin_defn latex
  $f: D \to ℝ^m$ partiell differenzierbar. Die Matrix der ersten partiellen Ableitungen
  \[J_f := (\pp{f_i}{x_j})_{\substack{i = 1, \dots, w \\ j = 1, \dots, n}} ∈ ℝ^{n×w}\]
  heißt die *Jacobi-Matrix* (manchmal auch *Fundamentalmatrix* ) von $f$ in $x$.
  Im Fall $n = m$ bezeichnet man $\det(J_f)$ als *Jacobideterminante*.
  #+end_defn
  #+begin_defn latex
  $f:D \to ℝ$ zweimal partiell differenzierbar. Die Matrix der zweiten Ableitungen
  \[H_f(x) := (\frac{\partial^2}{\partial x_i \partial x_j}f)_{\substack{i = 1,\dots, n\\ j = 1, \dots, w}} ∈ ℝ^{n×m}\]
  heißt *Hesse-Matrix*.
  #+end_defn
  #+begin_defn latex
  Sei $f: D \to ℝ^m$, dann nennen wir $f$ in einem Punkt $x ∈ D$ (total differenzierbar), wenn die Funktion $f$ in $x$
  sich linear approximieren lässt, das heißt es gibt eine lineare Abbildung $Df(x): ℝ^n \to ℝ^m$ (Differential) sodass in einer kleinen Umgebung von $x$ gilt:
  \[f(x + h) = f(x) + Df(x)h + w(h), h ∈ ℝ^n, x + h ∈ D\]
  mit einer Funktion $w: D \to ℝ^m$, die die Eigenschaft hat
  \[\lim_{\substack{x + h ∈ D\\ \norm{h}_2 \to 0}} \frac{\norm{w{h}}_2}{\norm{h}_2} = 0\]
  alternativ: $w(h) = \mathcal{o}(\norm{h}_2)$
  #+end_defn
  #+begin_thm latex
  Für Funktionen $f: D \to ℝ^m$ gilt:
  1. Ist $f$ in $x ∈ D$ differenzierbar, so ist $f$ auch in $x$ partiell differenzierbar und das Differential von $f$ ist gegeben durch die Jacobi-Matrix.
  2. Ist $f$ partiell differenzierbar in einer Umgebung von $x$ und sind zusätzlich die partiellen Ableitungen stetig in $x$, so ist $f$ in $x$ differenzierbar.
  #+end_thm
  #+begin_proof latex
  1. Für differenzierbares $f$ gilt für $i = 1,2$:
	 \[\lim_{h \to 0} \frac{f(x + he_i) - f(x)}{h} = \lim_{h \to 0} (Df(x)e_i + \frac{w(h)}{h}) = Df(x)e_i\]
  2. Für ein stetig partiell differenzierbares $f$ gilt mit $h = (h_1, h_2)$:
	 \begin{align*}
	 f(x + h) - f(x) &= f(x_1 + h_1, x_2 + h_2) - f(x_1 + h_1, x_2) + f(x_1 + h_1, x_2) - f(x_1, x_2) \\
	 \intertext{Mittelwertsatz}
  	 &= h_2 \pp{f}{x_2}(x_1 + h_1, x_2 + θ_2 h_2) + h_1 \pp{f}{x_1}(x_1 + θ_1 h_1, x_2) \tag*{$θ_1, θ_2 ∈ (0, 1)$} \\
	 &= h_2(\pp{f}{x_2}(x_1, x_2 ) + ω_2(h_1, h_2)) + h_1(\pp{f}{x_1}(x_1, x_2) + ω_1(h_1, h_2)) \\
	 ω_1(h_1, h_2) &:= \pp{f}{x_1}(x_1 + θ_1 h_1, x_2) - \pp{f}{x_1}(x_1, x_2) \xrightarrow{h_1, h_2 \to 0} 0 \\
	 ω_2(h_1, h_2) &:= \pp{f}{x_2}(x_1 + h_1, x_2 + θ_2 h_2) - \pp{f}{x_2}(x_1, x_2) \xrightarrow{h_1, h_2 \to 0} 0\\
     \end{align*}
	 Also ist $f$ differenzierbar mit Ableitungen $Df(x) = ∇f(x)$.
  #+end_proof
  #+begin_remark latex
  Es gelten folgende Implikationen:
  stetig partiell differenzierbar $⇒$ (total) differenzierbar $⇒$ partiell differenzierbar.
  #+end_remark
  #+begin_thm latex
  Seien $D_f ⊂ ℝ^n, Dg ⊆ ℝ^m$ offen und $g: D_g \to ℝ^n, f: D_f \to ℝ^r$. Ist $g$ im Punkt $x ∈ D_g$ differenzierbar und $f$ in $y = g(x) ∈ D_f$ differenzierbar,
  so ist die Komposition $h = f \circ g$ im Punkt $x$ differenzierbar. Es gilt $D_x h(x) = D_y f(g(x)) · D_x g(x)$. Hierbei ist $·$ die Matrixmultiplikation.
  #+end_thm
  #+begin_proof latex
  Nach Voraussetzung $x ∈ D_g$ sodass $g(x) = y ∈ D_f$. Da sowohl $f$ als auch $g$ differenzierbar
  \begin{align*}
  g(x + h_1) &= g(x) + D_x g(x)h_1 + ω_g(h_1) \\
  f(y + h_2) &= f(y) + D_y f(y)h_2 + ω_f(h_2) \\
  \lim_{\substack{x + h_1 ∈ D_y \\ \norm{h_1} \to 0}} \frac{\norm{ω_g(h_1)}}{\norm{h_1}} &= 0 \\
  \lim_{\substack{y + h_2 ∈ D_y \\ \norm{h_2} \to 0}} \frac{\norm{ω_f(h_2)}}{\norm{h_2}} &= 0 \\
  \string(f \circ g\string)(x + h_1) &= f(g(x + h_1)) = f(y + η), \quad η := D_x g(x)h_1 + ω_g(h_1) \\
  &= f(y) + D_y f(y) η + ω_f(η) \\
  &= f(y) + D_y f(y) D_x g(x) h_1 + D_y f(y) ω_g(h_1) + ω_f(D_x g(x)h_1 + ω_g(h_1)) \\
  &= (f\circ g)(x) + D_y f(y) D_x g(x) h_1 + ω_{f \circ g}(h_1) \\
  ω_{f\circ g}(h_1) &:= D_y f(y)ω_g(h_1) + ω_f(D_x g(x)h_1 + ω_g(h_1)) \\
  \intertext{Es bleibt zu zeigen $ω_{f \circ g} = \mathcal{o}(h_1)$. Nach Voraussetzung gilt $ω_{f \circ g} \xrightarrow{h_1 \to 0} 0$}
  \end{align*}
  #+end_proof
  #+begin_lemma latex
  Sei $A:[a, b] \to ℝ^{n × m}$ stetig, dann gilt
  \[\norm{∫ 0^1 A(s) \d s}_M \leq ∫_0^1 \norm{A(s)_M \d s}, \norm{A}_M := \max\{\abs{λ} \mid λ ∈ σ(A)\}\]
  $∫A = (∫a_{ij})_{ij}, σ(A) :=$ Menge der Eigenwerte von $A$
  #+end_lemma
  #+begin_thm latex
  Sei $f: D \to ℝ^m$ stetig differenzierbar mit $J_f$ als Jacobi-Matrix, so gilt
  \[f(x + h) - f(x) = (∫_0^1 J_f (x + sh)\d s)h\]
  #+end_thm
  #+begin_proof latex
  Definiere $g_j(s) := f_j(x + sh)$, dann ist $g_{j_1}:[0, 1] \to ℝ$, also gilt
  \[f_j(x + sh) - f_j(x) = g_j(1) - g_j(0) = ∫_0^1 g_j'(s) \d s = ∫_0^1 \sum_{i = 1}^{n} \pp{f_j}{x_i}(x + sh)h_i \d s\]
  #+end_proof
  #+begin_remark latex
  Im Fall $m = 1$ kann man aus dem Mittelwertsatz für Integrale schließen, dass
  \[f(x + h) - f(x) = ∫_0^1 J_f(x + sh) h \d s = J_f(x + τh) h\]
  $x_1 + h = x_2 ⇒ h = x_2 - x_1$
  #+end_remark
  #+begin_korollar latex
   Sei $f: D \to ℝ^m$ stetig differenzierbar. Ferner sei $x ∈ D$ mit $K_r(x) ⊂ D, r > 0$, dann gilt
   \[\norm{f(x) - f(y)}_2 \leq M\norm{x - y}_2, y ∈ K_r(x), M := \sup_{z ∈ K_r(x)}\norm{J_f(z)}_M\]
   das heißt die Abbildung ist in $D$ lokal Lipschitz-stetig.
  #+end_korollar
  #+begin_proof latex
  Nach Satz 2.14 gilt mit $h = y - x$
  \begin{align*}
  \norm{f(y) - f(x)}_2 &= \norm{f(x + h) - f(x)_2} = \norm{∫_0^1 J_f(x + sh) h \d s}_2 \\
  &\leq ∫_0^1 \norm{J_f(x + sh) h}_2 \d s \leq ∫_0^1\norm{J_f(x + sh)}_m \norm{h}_2 \d s \\
  &\leq \underbrace{\sup_{0 < s < 1} \norm{J_f(x + sh)}_2}_{M} \underbrace{\norm{h}_2}_{\norm{y - x}_2}
  \end{align*}
  #+end_proof
  #+begin_remark latex
  Korollar 2.16 gilt mit beliebigen von Vektor-Matrix-norm induzierter Norm, siehe Übung 2.1.
  #+end_remark
  *Taylor-Entwicklung und Extremwerte in $ℝ^n$*
  #+ATTR_LATEX: :options [Multiindex Notation]
  #+begin_defn latex
  Ein n-dimensionaler *Multiindex* ist ein Tupel $α= (α_1, \dots, α_n)$ mit $α_i ∈ ℕ$. Für Multiindizes sind die *Ordnung* $\abs{α}$ und die Fakultät $α!$
  definiert durch
  \begin{align*}
  \abs{α} &:= α_1 + α_2 + \dots + α_n \\
  α! &:= α_1! · \dots · α_n!
  \end{align*}
  Für $x = (x_1, \dots, x_n) ∈ ℝ^n$ wird gesetzt
  \[x^α := x_1^{α_1} · \dots · x_n^{α_n}\]
  Für eine \(\abs{α}\)-mal stetig differenzierbare Funktion wird gesetzt
  \[\partial^α f := \partial_1^{α_1} \dots \partial_n^{α_n} f := \frac{\partial^{\abs{α}} f}{\partial_{x_1}^{α_1} \dots \partial_{x_n}^{α_n}}\]
  #+end_defn
  #+begin_remark latex
  Wegen der Stetigkeit der Ableitung ist dieser Ausdruck unabhängig von der Reihenfolge der partiellen Ableitungen.
  Wir definieren
  \[\sum_{\abs{α} = 0}^{r} a_α := \sum_{k = 0}^{r} \sum_{\substack{α ∈ ℕ^n \\ \abs{α} = k}} a_α\]
  #+end_remark
  #+begin_ex latex
  Für $n = 3$ sind die Multiindizes $α = (α_1, α_2, α_3)$ der Ordnung $\abs{α} = 2$ gegeben durch
  \[(2, 0, 0), (0, 2, 0), (0, 0, 2), (1, 1, 0), (1, 0, 1), (0, 0, 1)\]
  Die zugehörigen partiellen Ableitungen sind
  \begin{align*}
  \partial^α f &= (\partial_{x_1}^2 f, \partial_{x_2}^2 f, \partial_{x_3}^2 f, \partial_{x_1}\partial_{x_2}f, \partial_{x_2}\partial_{x_3}f, \partial_{x_2}\partial_{x_3}f) \\
  α! &= (2, 2, 2, 1, 1, 1)
  \end{align*}
  Schließlich ist
  \[\sum_{\abs{α} = 2} \partial^α f = \partial_{x_1}^2 f+ \partial_{x_2}^2 f+ \partial_{x_3}^2 f+ \partial_{x_1}\partial_{x_2}f+ \partial_{x_2}\partial_{x_3}f+ \partial_{x_2}\partial_{x_3}f\]
  #+end_ex
  #+ATTR_LATEX: :options [Taylor-Formel]
  #+begin_thm latex
  Sei $D ⊂ ℝ^n$ eine offene Menge und $f: D \to ℝ$ eine \((r + 1)\)-mal stetig differenzierbare Funktion.
  Dann gilt für jeden Vektor $h ∈ ℝ^n$ mit $x + sh ∈ D, s ∈ [0, 1]$ die Taylor-Formel
  \[f(x + h) = \sum_{\abs{α} < r} \frac{\partial^α f(x)}{α!}h^α + R_{r + 1}^f(x, h)\]
  in differentieller Form
  \[R_{r + 1}^f (x, h) = \sum_{\abs{α} = r + 1} \frac{\partial^α f(x + θh)}{α!} h^α, θ ∈ (0, 1)\]
  oder in integraler Form
  \[R_{r + 1}^f (x, h) = (r + 1) ∫_0^1 \sum_{\abs{α} = r + 1} \frac{\partial^α f(x + th)}{α!} h^α (1 - t)^r \d t\]
  #+end_thm
  #+begin_proof latex
  Wir nehmen $g:[0, 1] \to ℝ$ mit $g(t) := f(x + th)$. $g$ ist $(r + 1)$ mal stetig differenzierbar mit der k-ten Ableitung
  \[g^{(k)}(t) = \sum_{i_1, \dots, i_k = 1}^{n} \partial_{i_k} \dots \partial_{i_1} f(x + th) h_{i_1}\dots h_{i_k}\]
  Wir zeigen des durch Induktion nach $k$ (mit Hilfe von Kettenregel). Für $k = 1$ gilt
  \[g'(t) = \dd{}{t} f(x_1 + th_1, \dots, x_n + th_n) = \sum_{i = 1}^{n} \partial_i f h_i\]
  Sei die Behauptung als richtig angenommen für $k - 1 \geq 1$. Dann gilt
  \begin{align*}
  g^{(k)}(t) &= \dd{}{t} g^{(k - 1)}(t) = \dd{}{t} (\sum_{i_1, \dots, i_{k - 1} = 1}^{n} \partial_{i_{k - 1}} \dots \partial_{i_1}f(x + th) h_1 \dots h_{i_{k - 1}}) \\
  &= \sum_{i = 1}^{n} \partial_i (\sum_{i_1 \dots i_{k - 1} = 1}^{n}\partial_{i_{k - 1}} \dots \partial_{i_1} f(x + th) h_{i_1} \dots h_{i_{k - 1}}) h_1 \\
  &= \sum_{i_1 \dots i_k = 1}^{n} \partial_{i_k} \dots \partial_{i_1} f(x + th) h_{i_1} \dots h_{i_k}
  \end{align*}
  Es gilt
  \[\partial_{i_k} \dots \partial_{i_1} f(x + th)h_{i_1} \dots h_{i_k} = \partial_1^{α_1} \dots \partial_n^{α_n}f(x + th) h_1^{α_1} \dots h_n^{α_n}\]
  (der Index $i ∈ \{1, \dots, n\}$ kommt genau $α_i$ mal vor und wegen Vertauschbarkeit der Ableitungen).
  Die Anzahl der \(k\)-Tupel $(i_1, \dots, i_k)$ von Zahlen $i_j ∈ \{1, \dots, n\}$, bei denen die Zahl $i ∈ \{1, \dots, n\}$ genau \(α_i\)-mal vorkommt mit $α_1 + \dots + α_n = k$ ist
  \[\frac{k!}{α_1! \dots α_n !}\]
  (Lemma unten)
  Wir bekommen
  \begin{align*}
  g^{(k)}(t) &= \sum_{\abs{α} = k} \frac{k!}{α_1! \dots α_n!} \partial_1^{α_1} \dots \partial_n^{α_n} f(x + th) h_1^{α_1} \dots h_n^{α_n} \\
  &= \sum_{\abs{α} = k} \frac{k!}{α!}\partial^α f(x + th) h^α
  \end{align*}
  Wir wenden die 1-dimensionale Taylor-Formel auf $g(t)$ an. $∃ θ ∈ [0, 1]$ sodass
  \[g(1) = \sum_{k = 0}^{r} \frac{g^{k}(0)}{k!} + \frac{g^{(r + 1)}(θ)}{(r + 1)!} = \sum_{k = 0}^{n} \frac{g^{(k)}}{k!} + \frac{1}{r!}∫_0^1 g^{(r + 1)}(t)(1 - t)^r \d t\]
  Man erhält
  \begin{align*}
  \frac{g^{(k)}(0)}{k!} &= \sum_{\abs{α} = k} \frac{\partial^α f(x)}{α!} h^α \\
  \frac{g^{(r + 1)}(θ)}{(r + 1)!} &= \sum_{\abs{α} = r + 1} \frac{\partial^α f(x + θh)}{α!} h^α \\
  \frac{1}{r!} ∫_0^1 g^{(r + 1)}(t)(1 - t)^r \d t &= (r + 1) ∫_0^1 \sum_{\abs{α} = r + 1} \frac{\partial^α f(x + th)}{α!} h^α (1 - t)^r \d t
  \end{align*}
  Dies impliziert die Taylor-Formel mit den Restgliedern in differentieller oder integraler Form.
  #+end_proof
  #+ATTR_LATEX: :options [2.20]
  #+begin_lemma latex
  Sei $α = (α_1, \dots, α_n)$ mit $\abs{α} = k \geq 1$. Dann ist die Anzahl $N_α (k)$ der k-Tupel von Zahlen $i_j = \{1, \dots, n\}$, bei denen die Zahl $i ∈ \{1, \dots, n\}$
  genau	\(α_i\)-mal vorkommt, bestimmt durch
  \[N_α(k) = \frac{k!}{α_1! \dots α_n!}\]
  #+end_lemma
  #+begin_proof latex
  Wir ordnen die Indizes in dem k-Tupel
  \[(i_1, \dots, i_k) = (\underbrace{1, \dots, 1}_{α_1}, \underbrace{2, \dots, 2}_{α_2}, \dots, \underbrace{n, \dots, n}_{α_n \text{ mal}})\]
  $α_1 + \dots + α_n = k$. Die Anzahl der möglichen Permutationen der $k$ Elemente des k-Tupel ist $k!$. Das k-Tupel bleibt unverändert bei Permutationen von gleichen Elementen $i$.
  Insgesamt bekommen wir
  \[N_α(k) = \frac{k!}{α!}\]
  #+end_proof
  #+ATTR_LATEX: :options [2.21]
  #+begin_korollar latex
  Sei $D ⊂ ℝ^n$ eine offene Menge und $f: D \to ℝ$ eine $r + 1$ mal stetig differenzierbare Funktion. Dann gilt für $x ∈ D$ und $h ∈ ℝ^n$ mit $x + sh ∈ D, s ∈ [0, 1]$:
  \[f(x + h) = \sum_{\abs{α} \leq r + 1} \frac{\partial^a f(x)}{α!} h^α + ω_{r + 1}(x, h)\]
  wobei $ω_{r + 1}(x, 0) = 0$ und $ω_{r + 1}(x, h) = \mathcal{o}(\norm{h}_2^{r + 1})$. \\
  Im Fall $r = 0$ gilt
  \[f(x + h) = f(x) + (∇f(x), h)_2 + ω_1(x, h)\]
  Im Fall $r = 1$ gilt:
  \[f(x + h) = f(x) + (∇f(x), h)_2 + \frac{1}{2} (H_f(x)h, h)_2 + ω_2(x, h)\]
  #+end_korollar
  #+begin_proof latex
  \begin{align*}
  f(x + h) &= \sum_{\abs{α} \leq r} \frac{\partial^α f(x)}{α!} h^α + \sum_{\abs{α} = r + 1} \frac{\partial^α f(x + θh)}{α!}h^α \\
  &= \sum_{\abs{α} \leq r + 1} \frac{\partial^α f(x)}{α!} h^α + \sum_{\abs{α} = r + 1} r_α(x, h)h^α
  \end{align*}
  wobei
  \[r_α(x, h) := \frac{\partial^α f(x + θh) - \partial^α f(x)}{α!}\]
  $\lim_{h \to 0} r_α(x, h) = 0$, wegen der Stetigkeit von $\partial^α f$ für $\abs{α} = r + 1$.
  Wir setzen $ω_{r + 1}(x, h) := \sum_{\abs{α} = r + 1} r_α(x, h) h^α$. Es gilt
  \[\lim_{h \to 0} \frac{ω(h)}{\norm{h}_2^{r + 1}} = 0\]
  weil
  \[\frac{\abs{h^α}}{\norm{h}_2^α} = \frac{\abs{h_1^{α_1}} · \dots · h_n^{α_n}}{\norm{h}_2^{α_1} · \dots · \norm{h}_2^{α_n}} \leq 1 \qquad \abs{α} = r + 1\]
  Für $r = 0$ gilt
  \begin{align*}
  f(x + h) &= \sum_{\abs{α} \leq 1} \frac{\partial^α f(x)}{α!} h^α + ω_1(x, h) \\
  &= f(x) + \sum_{\abs{α} = 1} \frac{\partial^α f(x)}{α!} h^α + ω_1(x, h) \\
  &= f(x) + \sum_{i = 1}^{n} \partial_i f(x)h_i + ω_1(x, h) \\
  &= f(x) + (∇f(x), h)_2 + ω_1(x, h)
  \end{align*}
  Für $r = 1$
  \begin{align*}
  f(x + h) &= \sum_{\abs{α} \leq 2} \frac{\partial^α f(x)}{α!} h^α + ω_2(x, h) \\
  &= f(x) (∇f(x), h)_2 + \sum_{\abs{α} = 2} \frac{\partial^α f(x)}{α!} h^α + ω_2(x, h) \\
  &= f(x) + (∇f(x), h)_2 + \frac{1}{2} \sum_{i,j = 1}^{n} \partial_i \partial_j f(x) h_i h_j + ω_2(x, h) \\
  &= f_1(x) + (∇f(x), h)_2 + \frac{1}{2} (H_f(x)h, h)_2 + ω_2(x, h)
  \end{align*}
  #+end_proof
  #+begin_defn latex
  Sei $D ⊂ ℝ^n$ eine offene Menge, $x ∈ D$ und $f: D \to ℝ$ beliebig oft differenzierbar.
  \[F_∞^f(x + h) = \sum_{\abs{α} = 0}^{∞} \frac{\partial^α f(x)}{α!} h^α\]
  heißt die Taylor-Reihe von $f$ in $x$
  #+end_defn
  #+begin_korollar latex
  Sei $D ⊂ ℝ^n$ eine offene Menge, $f: D \to ℝ$ beliebig oft differenzierbar. Dann konvergiert die Taylor-Reihe von $f$ und stellt $f$ dar, wenn
  \[R_{r + 1}^f (x, h) \xrightarrow{r \to ∞} 0 \quad x ∈ D\]
  Hinreichend dafür ist, dass die partielle Ableitung gleichmäßig beschränkt sind:
  \[\sup_{\abs{α} \geq 0} \sup_{x ∈ D} \abs{\partial^α f(x)} < ∞\]
  #+end_korollar
  #+begin_proof latex
  \[\norm{R_{r + 1}^f(x, h)}_∞ \leq \sum_{\abs{α} = r + 1} \frac{\abs{\partial^α f(x + θh)}}{α!} \norm{h}_∞^{\abs{α}} \leq M(f) \sum_{\abs{α} = r + 1} \frac{1}{α!} \norm{h}_∞^{\abs{α}} \to 0\]
  #+end_proof
  #+begin_defn latex
  Eine Funktion $f: D \to ℝ$ hat in einem Punkt $x ∈ D ⊂ ℝ^n$ ein lokales Extremum, wenn auf einer $K_σ(x) ⊂ ℝ^n$ (Kugelumgebung) gilt
  \[f(x) = \sup_{y ∈ K_σ(x) ∩ D} f(y) \quad \text{oder} \quad f(x) = \inf_{y ∈ K_σ(x) ∩ D} f(x)\]
  Das Extremum heißt strikt, wenn es in $K_σ(x) ∩ D$ nur in dem Punkt angenommen wird.
  Das Extremum heißt global, wenn $f(x) = \sup_{y ∈ D} f(y)$ (oder $\inf_{y ∈ D}$)
  #+end_defn
  #+ATTR_LATEX: :options [Notwendige Extremalbedingung]
  #+begin_thm latex
  Sei $f: D \to ℝ$ stetig differenzierbar, $D$ offen. Hat $f$ in einem Punkt $\hat x ∈ D$ ein lokales Extremum, so gilt $∇f(\v x) = 0$
  #+end_thm
  #+begin_proof latex
  Angenommen $f: D \to ℝ$ hat in $x ∈ D$ ein lokales Extremum. Wir nehmen
  $g_i(t) := f(\v x + t e^{(1)}), i = 1, \dots, n, e^{(i)}$ Einheitsvektor in $ℝ^n$.
  $g_i$ ist auf einem nichtleeren $(-δ_i, δ_i) ⊂ ℝ$ definiert und hat lokales Extremum in $t = 0 ⇒ g'_i(0) = 0$
  \[0 = g'_i(0) = \sum_{j = 1}^{n} \partial_j f(\v x) δ_{ij} = \partial_i f(\v x) \quad i = 1, \dots, n ⇒ ∇ f(\v x) = 0\]
  #+end_proof
  #+ATTR_LATEX: :options [Hinreichende Extremalbedingung]
  #+begin_thm latex
  Sei $D ⊂ ℝ^n$ offen und $f:D \to ℝ$ zweimal stetig differenzierbar und $∇f(\v x) = 0$ in einem $\v x ∈ D$. Ist die Hesse Matrix $H_f(x)$ in $\v x$
  *positiv definit* (das heißt alle Eigenwerte positiv), so liegt in $\v x$ ein striktes lokales Minimum. Ist sie negativ definit (das heißt alle Eigenwerte negativ),
  so liegt in $\v x$ ein striktes lokales Maximum. Ist sie indefinit (hat sowohl positive als auch negative Eigenwerte), so kann in $\v x$ kein lokales Extremum liegen.
  #+end_thm
  #+begin_proof latex
  Nach Korollar 2.21 gilt
  \[f(x + h) = f(x) + (∇f(x), h)_2 + \frac{1}{2} (H_f(x) h, h)_2 + ω_2(x, h)\]
  wobei
  \[\lim_{\substack{h \to 0 \\ h \neq 0}} \frac{ω_2(x, h)}{\norm{h}_2^2} =0 \]
  \[∇f(\v x) = 0 ⇒ f(\v x + h) - f(\v x) = \frac{1}{2}(H_f(\v x)h, h)_2 + ω_2(\v x, h)\]
  Ist $H_f(\v x)$ positiv definit, so gilt
  \[(H_f(\v x)h, h)_2 \geq λ\norm{h}_2^2, h ∈ ℝ^n\]
  wobei $λ$ der kleinste Eigenwert ist.
  \[⇒ f(\v x + h) - f(\v x) \geq \frac{1}{2} λ\norm{h}_2^2 + ω(\v x, h)\]
  Für kleines $\norm{h}_2 < σ, h \neq 0$ ist
  \[\abs{ω_2(\v x, h)} < \frac{1}{2} λ \norm{h}_2^2\]
  und somit
  \[f(\v x + h) - f(\v x) > \frac{1}{2} λ \norm{h}_2^2 - \frac{1}{2} λ \norm{h}_2^2 = 0\]
  $⇒ \v x$ ist ein lokales Maximum. Ist $H_f(\v x)$ negativ definit $⇒ \v x$ ist ein lokales Maximum (analog). \\
  Ist $H_f(\v x)$ indefinit $⇒ ∃λ_+ > 0$ (mit Eigenvektor $z_+$) und $∃λ_- < 0$ (mit EV $z_-$)
  \begin{align*}
  \string(H_f(\v x)z_+, z_+\string)_2 &= λ_+ \norm{z_+}_2^2 > 0 \\
  \string(H_f(\v x)z_-, z_-\string)_2 &= λ_- \norm{z_-}_2^2 < 0 \\
  \end{align*}
  Für genügend kleines $t > 0$ gilt dann
  \[f(\v x+ tz_+) - f(\v x) > 0 \qquad f(\v x + tz_-) - f(\v x) < 0\]
  $⇒$ kein Extremum in $\v x$
  #+end_proof
  #+begin_ex latex
  1. $f_1(x) = a + x_1^2 + x_2^2$
     \[∇f_2(x) = (2x_1, 2x_2) = 0 ⇔ \v x_1 = 0 \wedge \v x_2 = 0 \]
	 \[H_{f_1}(x) = \begin{pmatrix}2 & 0 \\ 0 & 2\end{pmatrix}\]
	 positiv definit $⇒ \v x = 0$ ist Minimum.
  2. $f_2(x) = a - x_1^2 - x_2^2$
	 \[∇f_2(x) = (-2x_1, -2x_2) ⇒ \v x = 0, H_{f_2}(x) = \begin{pmatrix}-2 & 0 \\ 0 & -2\end{pmatrix}\]
	 negativ definit $⇒ \v x = 0$ ist Maximum.
  #+end_ex
  #+begin_remark latex
  Ist die Hesse Matrix in einer Nullstelle das Gradienten semidefinit (des heißt $∃ λ_i = 0$), so lassen
  sich keine allgemeinen Aussagen über lokale Extrema machen.
  #+end_remark
** Satz über implizite Funktionen und der Umkehrsatz
   Problemstellung: $F(x, y) = x^2 + y^2 - 1$. Betrachte $F(x, y) = 0$
   \[⇒ y(x) = \pm \sqrt{1 - x^2}\]
   #+ATTR_LATEX: :options [Satz über implizite Funktionen]
   #+begin_thm latex
   Sei $U_1 ⊆ ℝ^n, U_2 ⊆ ℝ^m$ offene Menge und $F: U_1 × U_2 \to ℝ^m, (x, y) ↦ F(x, y)$ sei eine stetig differenzierbare Funktion.
   Sei $(a, b) ∈ U_1 × U_2$	mit $F(a, b) = 0$. Die $(m × n)$ Matrix
   \[\pp{F}{y} := (\pp{F_i}{l_j})_{i,j = 1, \dots, m}\]
   in $(a, b)$ invertierbar. Dann gibt es offene Mengen $V_1 ⊆ U_1, V_2 ⊆ U_2$, $V_1$ Umgebung von $a$,	$V_2$ Umgebung von $b$ sowie eine eindeutige stetig differenzierbare Funktion
   $φ: V_1 \to V_2$ mit $φ(a) = b$ und $F(x, φ(x)) = 0 ∀ x ∈ V_1$. (Eindeutigkeit: Ist $(x, y) ∈ V_1 × V_2$ mit $F(x, y) = 0 ⇒ y = φ(x)$.)
   #+end_thm
   #+begin_proof latex
   Ohne Beschränkung der Allgemeinheit $(a, b) = (0, 0)$. Wir setzen
   \[B := \pp{F}{y}(0, 0) ∈ \GL(m, ℝ)\]
   und betrachten $G: U_1 × U_2 \to ℝ^m$ durch $G(x, y) := y - B^{-1} F(x, y)$ definiert. $G$ ist stetig differenzierbar, weil $F$ es ist.
   Dann gilt
   \[\pp{G}{y} = \mathbb{1} - B^{-1} \pp{F}{y}(x, y)\]
   mit
   \[\pp{G}{y}(0, 0) = \mathbb{1} - B^{-1}B = 0\]
   Es gilt: $F(x, y) = 0 ⇔ G(x, y) y$. \\
   Aufgrund der Stetigkeit von $\pp{G}{y}$ gibt es $W_1 ⊆ U_1, W_2 ⊆ U_2$ (jeweils um $0$), sodass
   \[\norm{\pp{G}{y}}_2 \leq \frac{1}{2}\,∀(x, y) ∈ W_1 × W_2\]
   Wähle $r > 0$, sodass $V_2 := \{y ∈ ℝ^n \mid \norm{y}_2 \leq r\} ⊆ W_2$ und da $G(0, 0) = 0$ gibt es offene Umgebung $V_1 ⊂ W_1$, sodass
   \[\sup_{x ∈ V_1} \norm{G(x, 0)}_2 =: ε \leq \frac{r}{2}\]
   Es gilt für alle $x ∈ V_1$ und $y, η ∈ V_2$:
   \[\norm{G(x, y) - G(x, y)} \leq \frac{1}{2} \norm{y - η}\]
   Ferner gilt
   \begin{align*}
   \norm{G(x, y)} &\leq \norm{G(x, y) - G(x, 0)} + \norm{G(x, 0)} \\
   &\leq \frac{1}{2} \norm{y} + \frac{r}{2} \leq \frac{r}{2} + \frac{r}{2} = r
   \end{align*}
   Die Abbildung $y ↦ G(x, y)$ bildet $V_2$ in sich selbst ab und ist eine Kontraktion. Also existiert ein eindeutiger Fixpunkt $y$ nach Banachschem Fixpunktsatz sodass
   $G(x, y) = y$ beziehungsweise $y = φ(x), F(x, φ(x)) = 0$. Wir setzen
   \[A := \{φ ∈ C_b(V_1, ℝ^m) \mid \norm{φ}_∞ \leq r\} = \{φ ∈ C_b(V_1, ℝ^m) \mid φ(V_1) ⊂ V_2\}\]
   Definiere $Φ:A \to A, φ ↦ G(x, φ(x))$.
   \begin{align*}
   \norm{Φ(φ_1) - Φ(φ_2)}_∞ &= \sup_{x ∈ V_1} \norm{G(x, φ_1(x)) - G(x, φ_2(x))} \leq \frac{1}{2} \sup_{x ∈ V_1}\norm{φ_1(x) - φ_2(x)} \\
   &= \frac{1}{2}\norm{φ_1 - φ_2}_∞
   \end{align*}
   $⇒$ es existiert ein eindeutiges $φ ∈ C_b(V_1, ℝ^m)$ mit $Φ(φ) = φ ⇔ G(x, φ(x)) = φ(x)$. Nach eventueller Verkleinerung von $V_1$ könne wir annehmen, dass
   $\pp{F}{y}$ in jedem Punkt $(x, (φ(x))), x ∈ V_1$ invertierbar ist. Wir zeigen de Differenzierbarkeit von $φ$ nur in 0.
   \[A := \pp{F}{x}(0, 0) ∈ M(m × n, ℝ), \quad B := \pp{F}{y}(0, 0) ∈ \GL(m, ℝ)\]
   Aus der Differenzierbarkeit von $F$ in $(0, 0)$ folgt: $F(x, y) = Ax + By + ω(x, y)$. Nun gilt $F(x, φ(x)) = 0 ∀ x ∈ V_1$, das heißt
   \[φ(x) = -B^{-1} A x - B^{-1} ω(x, φ(x))\]
   Es muss also gezeigt werden, dass $ω(x, φ(x)) = \mathcal{o}(\norm{(x, φ(x))})$. Zeige dazu, dass es eine Umgebung $V_1 ⊂ V_1$ von $0$ gibt und eine Konstante $K > 0$, sodass
   \[\norm{φ(x)} \leq K \norm{x}\,∀ x ∈ V_1' \quad p_1 := \norm{B^{-1}A} \quad c_2 := \norm{B^{-1}}\]
   und wegen $ω(x, y) = \mathcal{0}(\norm{x, y})$ gibt es zu $ε := 1 / (2 c_2)$ eine Umgebung $V' ⊂ V_1 × V_2$ von $0, 0$, sodass
   \[\norm{ω(x, y)} = ε\norm{(x, y)} \leq \frac{1}{2 c_2}(\norm{x} + \norm{y})\,∀ (x, y) ∈ V'\]
   Wegen der Stetigkeit von $φ$ gibt es eine Nullumgebung $V_1' ⊂ V_1$, sodass der Graph $φ\mid_{V_1'}$ ganz in $V'$ enthalten ist. Damit gilt
   \[\norm{ω(x, φ(x))} \leq \frac{1}{2 c_2}(\norm{x} + \norm{φ(x)})\]
   Außerdem gilt
   \begin{align*}
   \norm{φ(x)} &\leq c_1 \norm{x} + c_2 \norm{ω(x, φ(x))} \\
   &\leq (c_1 + \frac{1}{2})\norm{x} + \frac{1}{2}\norm{φ(x)} \\
   ⇒ \norm{φ(x)} &\leq \underbrace{2(c_1 + \frac{1}{2})}_{=: K} \norm{x}
   \end{align*}
   #+end_proof
   #+begin_ex latex
   $F(x, y) = x^2 + y^2 - 1 = 0 ⇒ D_y F = 2y$. Wir können demnach in einer Umgebung von $(\hat x^2, \hat y^2), \hat x^2 + \hat y^2 - 1 = 0$ mit $\hat y \neq 0$ eindeutig nach $y$ auflösen und erhalten
   \[y = \pm \sqrt{1 - x^2}\]
   #+end_ex
   #+ATTR_LATEX: :options [2.27]
   #+begin_defn latex
   Sei $D ⊂ ℝ^n$ offen und $f: D \to ℝ^n$ heißt *regulär* in einem Punkt $\hat x ∈ D$, wenn $f$ in einer Umgebung $K_δ(\hat x) ⊂ D$ von $\hat x$ stetig differenzierbar und die
   Jacobi-Matrix $J_f$ regulär ist. (invertierbar). $f$ heißt regulär in $D$, wenn $f$ in jedem Punkt regulär ist.
   #+end_defn
   #+ATTR_LATEX: :options [Satz von der Umkehrabbildung]
   #+begin_thm latex
   Sei $D ⊂ ℝ^n$ offen, $f: D \to ℝ^n$ regulär in einem Punkt $\hat x ∈ D$. Dann gibt es eine offene Umgebung $V(\hat x) ⊂ D$, die von $f$ bijektiv auf eine offene Umgebung $U(\hat y) ⊂ ℝ^n$
   ($\hat y = f(\hat x)$) abgebildet wird. Die Umkehrabbildung ist ebenfalls regulär in $\hat y$. $f^{-1}: U(\hat y) \to V(\hat x)$. Für die Fundamentalmatrix und -determinante gilt:
   \[J_{f^{-1}}(\hat y) = (J_f (\hat x))^{-1}, \quad \det J_{f^{-1}}(\hat y) = \frac{1}{\det J_f(\hat x)}\]
   #+end_thm
   #+begin_proof latex
   Sei $\hat x ∈ D$ und definiere $\hat y := f(\hat x)$. Betrachte $F: ℝ^n × D \to ℝ^n$, $F(x, y) = y - f(x)$ und offenbar gilt $F(\hat y, \hat x) = 0$ und $D_x F(y, x) = -J_f(x)$
   und damit regulär in $\hat x$. Nach dem Satz über implizite Funktionen existieren Umgebungen $U(\hat y)$ und $U(\hat x)$, sowie eine eindeutige, stetige differenzierbare Funktion
   $φ: U(\hat y) \to U(\hat x)$ sodass $0 = F(y, φ(y)) = y - f(φ(y)), y ∈ U(\hat y)$. Das bedeutet zu jedem $y ∈ U(\hat y)$ kann man genau ein $x = φ(y) ∈ U(\hat x)$ finden mit $y = f(x)$.
   Wir setzen
   \[V(\hat x) := U(\hat x) ∩ f^{-1}(U(\hat y)) = \{x ∈ U(\hat x) \mid f(x) ∈ U(\hat y)\}\]
   $V(\hat x)$ offen. Ferner wird $V(\hat x)$ bijektiv von $f$ abgebildet mit zugehörigen Umkehrabbildung $f^{-1} = φ$. Wegen $J_{f \circ f^{-1}} = J_{\id} = I$ und der Kettenregel
   gilt
   \[J_f(x) · J_{f^{-1}}(f(x)) = I ⇒ J_{f^{-1}}(f(x)) = (J_f(x))^{-1}\]
   #+end_proof
   #+begin_ex latex
   Transformation der Polarkoordinaten auf kartesische Koordinaten. Polarkoordinaten: $(r, θ) \to$ kartesische Koordinaten $(x_1, x_2)$.
   \[(x_1, x_2) = f(r, θ) := (r \cos θ, r \sin θ) \qquad f: ℝ_+ × ℝ \to ℝ^2\]
   \[J_f(r, θ) = \begin{pmatrix}\cos θ & -r\sin θ \\ \sin θ & r\cos θ\end{pmatrix} \quad \det J_f(r, θ) = r > 0\]
   $f$ ist also auf $D = ℝ_+ × ℝ$ regulär. Nach dem Satz über Umkehrabbildung ist $f$ also überall in $D$ lokal umkehrbar
   \[J_{f^{-1}}(x_1, x_2) = J_f(r, θ)^{-1} = \begin{pmatrix}\cos θ & \sin θ \\ - r^{-1} \sin θ & r^{-1} \cos θ\end{pmatrix}\]
   Umrechnung in die Variablen $(x_1, x_2) = (r \cos θ, r \sin θ)$ liefert
   \[r = \sqrt{x_1^2 + x_2^2}, \cos θ = \frac{x_1}{r}, \sin θ = \frac{x_2}{r}\]
   \[J_{f^{-1}}(x_1, x_2) = \frac{1}{\sqrt{x_1^2 + x_2^2}} \begin{pmatrix}x_1 \sqrt{x_1^2 + x_2^2} & x_1 \sqrt{x_1^2 + x_2^2} \\ -x_2 & x_1\end{pmatrix}\]
   Wir haben bekommen die Jacobi-Matrix von $f^{-1}$ ohne $f^{-1}$ explizit zu berechnen. Wir berechnen jetzt die $f^{-1}$
   $f: U \to V$ mit $U := ℝ_+ × (-\frac{π}{2}, \frac{π}{2}), V := ℝ_+ × ℝ$ ist bijektiv
   \[f^{-1}(x_1, x_2)(\sqrt{x_1^2 + x_2^2}, \arctan(\frac{x_2}{x_1}))\]
   #+end_ex
** Extremalaufgaben mit Nebenbedingungen
   Sei $f: D \to ℝ$ und $g: D \to ℝ$ differenzierbare Funktionen auf einer offenen Menge $D ⊂ ℝ^n$. Wir suchen $\hat x ∈ D$, sodass
   \[f(\hat x) = \inf\{f(x) \mid x ∈ U(\hat x), g(\hat x) = 0\}\]
   für eine Umgebung $U(\hat x)$ von $\hat x$, oder
   \[f(\hat x) = \sup\{f(x) \mid x ∈ U(\hat x), g(\hat x) = 0\}\]
   #+ATTR_LATEX: :options [Lagrange Multiplikatoren]
   #+begin_thm latex
   Sei $D ⊂ ℝ^n$ offen und $f, g: D \to ℝ$ stetig differenzierbar. Ferner sei $\hat x ∈ D$ ein Punkt, in dem $f$ ein lokales Extremum unter der Nebenbedingung $g(\hat x) = 0$ hat. Das heißt
   \[f(\hat x) = \inf_{x ∈ U ∩ Ng} f(x) \qquad \sup_{x ∈ U ∩ Ng} f(x)\]
   wobei $Ng := \{x ∈ D \mid g(x) = 0\}$.
   Ist dass $∇g(\hat x) \neq 0$, so gilt es ein $\hat λ ∈ ℝ$
   \[∇f(\hat x) = \hat λ ∇g(\hat x)\]
   Der Parameter $\hat λ$ ist der sogenannte *Lagrange-Multiplikator*.
   #+end_thm
   #+begin_proof latex
   Wegen $∇ g(\hat x) \neq 0$ können wir (nach eventueller Umnummerierung der Koordinaten) annehmen, dass $\partial_n g(\hat x) \neq 0$
   \[\hat x := (\hat x', \hat x_n) ∈ ℝ^n, \hat x' ∈ ℝ^{n - 1}\]
   Nach dem Impliziten Funktionen Satz existieren für die Gleichung $F(x', x_n) := g(x) = 0$ die Umgebungen $U(\hat x') ⊂ ℝ^{n - 1}$ und $U(\hat x_n) ⊂ ℝ$ mit $U(\hat x') × U(\hat x_n) ⊂ D$
   und eine eindeutige Funktion	$φ: U(\hat x') \to U(\hat x_n)$ stetig differenzierbar und sodass
   \[F(x', φ(x')) = 0 \quad x' ∈ U(\hat x)\]
   \[Ng ∩ (U(\hat x_n) × U(\hat x)) = \{x ∈ U(\hat x_n) × U(\hat x'): x_n = φ(x')\}\]
   Mit Hilfe der Kettenregel bekommen wir
   \[\partial_i g(\hat x) + \partial_n g(\hat x) \partial_i φ(\hat x') = 0 \quad i = 1, \dots, n - 1\]
   Da $f$ auf $Ng$ im Punkt $\hat x$ ein lokales Extremum hat, hat die Funktion $f(x', φ(x'))$ auf $U(\hat x')$ ein lokales Extremum.
   \begin{align*}
   ⇒ 0 &= \partial_i f(\hat x) + \partial_n f(\hat x) \partial_i φ(\hat x)\quad i = 1, \dots, n - 1 \\
   ⇒ \partial_n f(\hat x) &= \hat λ \partial_n g(\hat x) \qquad \hat λ_n :=	\frac{\partial_n f(\hat x)}{\partial_n g(\hat x)} \\
   ⇒ \partial_i f(\hat x) &= \hat λ \partial_i g(\hat x)\quad i = 1, \dots, n \\
   ⇒ ∇f(\hat x) &= \hat λ ∇g (\hat x)
   \end{align*}
   #+end_proof
   #+begin_remark latex
   Jedes lokale	Minimum $\v x$ der Funktion $f$ unter der Nebenbedingung $g(\hat x) = 0$ korrespondiert zu einem sogenannten "stationären Punkt der Lagrange Funktion"
   \[\mathcal{L}(x, λ) := f(x) - λg(x) \quad (x, λ) ∈ D × ℝ\]
   \[∇_{x, λ} \mathcal{L}(\hat x, \hat y) = \begin{pmatrix}∇_x f(\hat x) - \hat λ ∇_x g(\hat x) \\ g(\hat x)\end{pmatrix} = 0\]
   #+end_remark
   #+begin_ex latex
   $f(x) := (x_1 · \dots · x_n)^2, f: ℝ^n \to ℝ$. Wir suchen das Maximum von $f$ auf der Sphäre $S_1 = \{x ∈ ℝ^n \mid \norm{x}_2 = 1\}$
   das heißt
   \[g(x) := \norm{x}_2 - 1 = \sum_{i = 1}^{n} x_1^2 - 1\]
   Nebenbedingung: $g(x) = 0$. $s ⊂ ℝ^n$ kompakt $⇒ f$ nimmt auf $S_1$ sein Maximum und Minimum an.
   \[\mid_{x ∈ S_1} f(x) = 0 \qquad \max_{x ∈ S_1} f(x) > 0\]
   Ferner $∇g(x) = 2x \neq 0$ auf $S_1$. Nach dem Satz 2.30 sind die Extremalpunkte die Lösungen $(x, λ) ∈ ℝ^n × ℝ$ vom Gleichungssystem
   \[\partial_i f(x) = λ \partial_i g(x) \quad i = 1, \dots, n\]
   \[⇒ 2(x_1 · \dots · x_n)^2 = 2 λ x_i\]
   \[⇒ (x_1 · \dots · x_n)^2 = λ x_i^2 \quad i = 1, \dots, n\]
   Weil $x_i \neq 0$ im Maximum $⇒ λ \neq 0$
   \[⇒ \sum_{i = 1}^{n} (x_1 · \dots · x_n)^2 = λ \sum_{i = 1}^{n} x_i^2 = λ\]
   \[⇒ n(x_1 · \dots · x_n)^2 = λ\]
   \[⇒ x_i^2 = \frac{1}{n}\quad i = 1, \dots, n\]
   #+end_ex
