* Bilinearformen
  In diesem Abschnitt sei $V$ stets ein K-VR.
  #+begin_defn latex
  $γ: V × V \to K$ heißt eine Bilinearform auf $V$, genau dann wenn die folgenden Bedingungen erfüllt sind:
  - (B1) $γ(v_1 + v_2, w) = γ(v_1, w) + γ(v_2, w), γ(λ v, w) = λ γ(v, w)$
  - (B2) $γ(v, w_1 + w_2) = γ(v, w_1) + γ(v, w_2), γ(v, λ w) = λ γ(v, w)$
  $∀ v, w, v_1, v_2, w_1, w_2 ∈ V, λ ∈ K$.
  #+end_defn
  #+begin_ex latex
  1. $K = ℝ, V = ℝ^n, γ: ℝ^n × ℝ^n \to ℝ, γ(\cvec{x_1; \dots; x_n}, \cvec{y_1; \dots; y_n}) = x_1 y_1 + \dots + x_n y_2$
	 ist eine Bilinearform auf $ℝ^n$.
  2. $K = ℝ, V = l[0, 1], γ: l[0, 1] × l[0, 1] ↦ ℝ, γ(f, g) := ∫_0^1 f(t) g(t) \d t$
	 ist eine Bilinearform auf $l[0, 1]$.
  3. $K = ℝ, V = ℝ^2, γ: ℝ^2 × R^2 \to ℝ, γ(\cvec{x_1; x_2}, \cvec{y_1; y_2}) = x_1 y_1 + 2 x_1 y_2 - x_2 y_2$
	 ist eine Bilinearform auf $ℝ^2$.
  #+end_ex
  #+begin_defn latex
  $V$ endlichdimensional, $\mathcal{B} = (v_1, \dots, v_n)$ Basis von $V, γ$ Bilinearform auf $V$
  \[M_{\mathcal{B}}(γ) = (γ(v_i, v_j))_{\substack{1 \leq i \leq n\\ 1 \leq j \leq n}} ∈ M(n × n, K)\]
  heihßb die *Darstellungsmatrix* (*Fundamentalmatrix*) von $γ$ bezüglich $\mathcal{B}$.
  #+end_defn
  #+begin_ex latex
  1. In	20.2a ist für $\mathcal{B} = (e_1, \dots, e_n): M_{\mathcal{B}}(γ) = E_n$
  2. In 20.2p ist für $\mathcal{B} = (e_1, e_2): M_{\mathcal{B}}(γ) = \begin{pmatrix}1 & 2 \\ 0 & -1\end{pmatrix}$
  #+end_ex
  #+begin_remark latex
  $V$ endlichdimensional, $\mathcal{B} = (v_1, \dots, v_n)$ Basis von $V, γ$ Bilinearform auf $V, A = M_{\mathcal{B}}(γ)$,
  $Φ_{\mathcal{B}}: K^n \to V$ Koordinatensystem zu $\mathcal{B}, v, w ∈ V, x = \cvec{x_1; \vdots; v_n} = Φ_{\mathcal{B}}^{-1}(v)$, das heißt $v = x_1 v_1 + \dots + x_n v_n$,
  \[y = \cvec{y_1; \vdots; y_n} = Φ_{\mathcal{B}}^{-1}(w)\]
  das heißt $w = q_1 v_1 + \dots + y_n v_n$. Dann gilt:
  \[γ(v, w) = Φ_{\mathcal{B}^{-1}}^T A Φ_{\mathcal{B}}^{-1}(w) = x^t A y = \begin{pmatrix}x_1 & \dots & x_n\end{pmatrix} A \cvec{y_1; \vdots; y_n}\]
  #+end_remark
  #+begin_proof latex
  Es ist
  \begin{align*}
  y(v, w) &= γ(x_1 v_1 + \dots + x_n v_n, y_1 v_1 + \dots + y_n v_n) = \sum_{i = 1}^{n} \sum_{j = 1}^{n} x_i y_j γ(v_i, v_j) \\
  &= \sum_{i = 1}^{n} x_i \sum_{j = 1}^{n} γ(v_i, y_j) y_j = x^T A y
  \end{align*}
  #+end_proof
  #+begin_remark latex
  $V$ endlichdimensional, $\mathcal{B} = (v_1, \dots, v_n)$ Basis von $V, A ∈ M(n × n, K)$. Dann gilt: Durch
  \[Δ_A^{\mathcal{B}}: V × V \to K, (v, w) ↦ Φ_{\mathcal{B}}^{-1}(v)^T A Φ_{\mathcal{B}}^{-1}(w)\]
  ist eine Bilinearform auf $V$ gegeben.
  #+end_remark
  #+begin_proof latex
  Nachrechnen.
  #+end_proof
  #+ATTR_LATEX: :options [wichtiger Spezialfall von 20.6]
  #+begin_ex latex
  $V = K^n, \mathcal{B} = (e_1, \dots, e_n), A ∈ M(n × n, K) ⇒ Φ_{\mathcal{B}} = \id_{K^n}$. Durch
  \[Δ_A^{(e_1, \dots, e_n)}: K^n × K^n \to K, (v, w) ↦ v^t A w\]
  ist eine Bilinearform auf $K^n$ gegeben. Wir setzen kurz $Δ(A) := Δ_A := Δ_A^{(e_1, \dots, e_n)}$
  #+end_ex
  #+begin_remdef latex
  $\Bil(V):= \{γ: V × V \to K \mid γ \text{ ist Bilinearform }\}$ ist ein K-VR, ist ein UVR vom K-VR $\Abb(V × V, K)$
  #+end_remdef
  #+begin_remark latex
  $V$ endlichdimensional, $\mathcal{B} = (v_1, \dots, v_n)$ Basis von $V$. Dann gilt: Die Abbildung
  \[M_{\mathcal{B}}: \Bil(V) \to M(n × n, K)\]
  ist ein Isomorphismus von K-VR mit Umkehrabbildung
  \[Δ^{\mathcal{B}}: M(n × n, K) \to \Bil(V), A ↦ Δ_A^{\mathcal{B}}\]
  #+end_remark
  #+begin_proof latex
  1. $M_{\mathcal{B}}$ linear: nachrechnen.
  2. $Δ^{\mathcal{B}} \circ M_{\mathcal{B}} = \id_{\Bil(V)}$, denn: Sei $γ ∈ \Bil(V)$
	 \begin{align*}
	 ⇒ (Δ^{\mathcal{B}} \circ M_{\mathcal{B}})(γ)(v_i, v_j)	&= Δ_{M_{\mathcal{B}}(γ)}^{\mathcal{B}}(v_i, v_j) = Φ_{\mathcal{B}}^{-v}(v_1)^t M_{\mathcal{B}}(γ) Φ_{\mathcal{B}}^{-1}(v_j) \\
	 &= e_i^T M_{\mathcal{B}}(γ)e_j = γ(v_i, v_j)
     \end{align*}
  3. $M_{\mathcal{B}} \circ Δ^{\mathcal{B}} = \id_{M(n × n, K)}$, denn: Sei $A = (a_{ij}) ∈ M(n × n, K), B = (b_{ij}) = (M_{\mathcal{B}} \circ Δ^{\mathcal{B}})(A) = M_\mathcal{B} \circ Δ_A^{\mathcal{B}}$
	 \[b_{ij} = Δ_A^{\mathcal{B}}(v_i, v_j) = Φ_{\mathcal{B}}^{-1}(v_i)^T A Φ_{\mathcal{B}}(v_j) = e_i^T A e_j = a_{ij}\]
	 $⇒ B = A$
  #+end_proof
  #+begin_thm latex
  $V$ endlichdimensional, $\mathcal{A}, \mathcal{B}$ Basen von $V, γ$ Bilinearform auf $V$. Dann gilt:
  \[M_{\mathcal{B}}(γ) = (T_{\mathcal{A}}^{\mathcal{B}})^T M_{\mathcal{A}}(γ) T_{\mathcal{A}}^{\mathcal{B}}\]
  #+end_thm
  #+begin_proof latex
  Für $v, w ∈ V$ ist
  \begin{align*}
  Φ_{\mathcal{B}}^{-1}(v)^T M_{\mathcal{B}}(w) &= γ(v, w) = Φ_{\mathcal{A}}^{-1}(v)^T M_{\mathcal{A}}(γ) Φ_{\mathcal{A}}^{-1}(w) \\
  \intertext{16.2.2: $\tilde T_{\mathcal{A}}^{\mathcal{B}} = Φ_{\mathcal{A}}^{-1} \circ Φ_{\mathcal{B}}$}
  &= (T_{\mathcal{A}}^{\mathcal{B}} Φ_{\mathcal{B}}^{-1}(v))^T M_{\mathcal{A}}(γ) T_{\mathcal{A}}^{\mathcal{B}} Φ_{\mathcal{B}}^{-1}(w) \\
  &= (Φ_{\mathcal{B}}^{-1})^T (T_{\mathcal{A}}^{\mathcal{B}})^T M_{\mathcal{A}}(γ) T_{\mathcal{A}}^{\mathcal{B}} Φ_{\mathcal{B}}^{-1}(w) \\
  ⇒ Δ^{\mathcal{B}}(M_{\mathcal{B}}(γ))(v, w) &= Δ^{\mathcal{B}}((T_{\mathcal{A}}^{\mathcal{B}})^T M_{\mathcal{A}}(γ) T_{\mathcal{A}}^{\mathcal{B}})(v, w) \\
  ⇒ Δ^{\mathcal{B}}(M_{\mathcal{B}}(γ)) &= Δ^{\mathcal{B}}((T_{\mathcal{A}}^{\mathcal{B}})^T M_{\mathcal{A}}(γ) T_{\mathcal{A}}^{\mathcal{B}}) \\
  \intertext{$Δ^{\mathcal{B}}$ Isomorphismus}
  ⇒ M_{\mathcal{B}}(γ) = (T_{\mathcal{A}}^{\mathcal{B}})^T M_{\mathcal{A}}(γ) T_{\mathcal{A}}^{\mathcal{B}}
  \end{align*}
  #+end_proof
  #+begin_defn latex
  $V$ endlichdimensional, $γ$ Bilinearform auf $V$. Wir setzen $\Rang(γ) := \Rang M_{\mathcal{B}}(γ)$, wobei $\mathcal{B}$ eine Basis von $V$ ist.
  #+end_defn
  #+begin_note latex
  Dies ist wohldefiniert. (folgt aus 20.10, da die Matrizen $T_{\mathcal{A}}^{\mathcal{B}}$ invertierbar sind)
  #+end_note
  #+begin_remdef latex
  Es gilt:
  1. Ist $γ: V × V \to K$ eine Bilinearform, dann induziert $γ$ die linearen Abbildungen
	 \begin{align*}
	 Γ_l: V \to V^{\ast}, w ↦ γ(·, w) &\qquad γ(·, w): V \to K, v ↦ γ(v, w) \\
	 Γ_r: V \to V^{\ast}, v ↦ γ(v, ·) &\qquad γ(v, ·): V \to K, v ↦ γ(v, w) \\
     \end{align*}
  2. Jede lineare Abbildung $Γ: V \to V^{\ast}$ induziert Bilinearformen
	 \begin{align*}
	 γ_l: V × V \to K, γ_l(v, w) &:= Γ(w)(v) \\
	 γ_r: V × V \to K, γ_r(v, w) &:= Γ(v)(w) \\
     \end{align*}
  Die Zuordnungen aus 1., 2. induzieren den Isomorphismus $\Bil(V) \cong \Hom_K(V, V^{\ast})$
  #+end_remdef
  #+begin_proof latex
  Nachrechnen.
  #+end_proof
  #+begin_defn latex
  $γ$ Bilinearform auf $V$. $γ$ heißt *nicht-ausgeartet* $⇔$ $Γ_l$ und $Γ_r$ sind injektiv.
  \[⇔ γ(v, w) = 0 ∀ v ∈ V ⇒ w = 0\]
  (Injektivität von $Γ_l$), und
  \[⇔ γ(v, w) = 0 ∀ w ∈ V ⇒ v = 0\]
  (Injektivität von $Γ_r$). \\
  $γ$ heißt *perfekt* $⇔$ $Γ_l$ und $Γ_r$ sind Isomorphismen.
  #+end_defn
  #+begin_remark latex
  $V$ endlichdimensional, $γ$ Bilinearform auf $V, \mathcal{B} = (v_1, \dots, v_n)$ Basis von $V, \mathcal{B}^{\ast}$ duale Basis zu $\mathcal{B}$. Dann gilt:
  \[M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_l) = M_{\mathcal{B}}(γ) = (M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_r))^T\]
  #+end_remark
  #+begin_proof latex
  Behauptung: Es ist $Γ_l(v_i) = γ(v_1, v_i)v_1^{\ast} + \dots + γ(v_n, v_i) v_n^{\ast}$, denn $Γ_l(v_i)(v_j) = γ(v_j, v_i)$ nach Definition
  \[(γ(v_1, v_i)v_1^{\ast} + \dots + γ(v_n, v_i)v_n^{\ast})(v_j) = γ(v_j = v_i)\]
  Somit: $M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_l) = M_{\mathcal{B}}(γ)$. \\
  Analog: $Γ_r(v_i) = γ(v_i, v_1) v_1^{\ast} + \dots + γ(v_i, v_n) v_n^{\ast} ⇒ M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_r) = (M_{\mathcal{B}}(γ))^T$
  #+end_proof
  #+begin_conc latex
  $V$ endlichdimensional, $γ$ Bilinearform auf $V, \mathcal{B}$ Basis von $V$. Dann sind äquivalent:
  1. $γ$ ist nich-ausgeartet
  2. $γ$ ist perfekt
  3. $ M_{\mathcal{B}}(γ)$ invertierbar
  4. $Γ_l$ injektiv
  5. $Γ_r$ injektiv
  #+end_conc
  #+begin_proof latex
  1. $⇔$ 2. wegen $\dim V = \dim V^{\ast}$ und 12.12
  $γ$ perfekt $⇔ Γ_l, Γ_r$ Isomorphismen $⇔ M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_l), M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_r)$ invertierbar $⇔ M_{\mathcal{B}}(γ)$
  invertierbar.	$M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_l), M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_r) ⇔ Γ_l$ Isomorphismus $⇔ M_{\mathcal{B}^{\ast}}^\mathcal{B}$ invertierbar.
  #+end_proof
  #+begin_defn latex
  $γ$ Bilinearform auf $V$. \\
  $γ$ heißt *symmetrisch* $⇔ γ(v, w) = γ(w, v) ∀ v, w ∈ V$ \\
  $γ$ heißt *antisymmetrisch* $⇔ γ(v, w) = -γ(w, v) ∀ v, w ∈ V$ \\
  $γ$ heißt *alterniernd* $⇔ γ(v, v) = 0 ∀ v ∈ V$.
  #+end_defn
  #+begin_note latex
  - $γ$ symmetrisch $⇒ Γ_l = Γ_r$
  - Für $\cha(K) \neq 2$ gilt: $γ$ alternierned $⇔ γ$ antisymmetrisch
  - Für $\cha(K) = 2$ gilt immer noch $γ$ alternierend $⇒ γ$ (anti)symmetrisch
	Die Umkehrung ist falsch: $γ: \mathbb{F}_2^3 × \mathbb{F}_2^3 \to \mathbb{F}, γ(x, y) = x_1 y_1 + x_2 y_2 + x_3 y_3$
	ist (anti)symmetrisch, aber nicht alternierend:
	\[γ(\cvec{\bar 1; \bar 0; \bar 0}, \cvec{\bar 1; \bar 0; \bar 0}) = \bar 1 \neq \bar 0\]
  #+end_note
  #+begin_remark latex
  $V$ endlichdimensional, $\mathcal{B}$ Basis von $V, γ$ Bilinearform auf $V$. Dann gilt:
  1. $γ$ symmetrisch $⇔ M_{\mathcal{B}}(γ)$ ist symmetrisch, das heißt $M_{\mathcal{B}}(γ)^T = M_{\mathcal{B}}(γ)$
  2. $γ$ antisymmetrisch $⇔ M_{\mathcal{B}}(γ)$ ist antisymmetrisch, das heißt $M_{\mathcal{B}}(γ)^T = -M_{\mathcal{B}}(γ)$
  #+end_remark
  #+begin_proof latex
  1. "$⇒$" klar \\
	 "$\impliedby$" Sei $M_{\mathcal{B}}(γ) = M_{\mathcal{B}}(γ)^T ⇒$ Für $v, w$ ist
	 \begin{align*}
	 γ(v, w) &= Φ_{\mathcal{B}}^{-1}(v)^T M_{\mathcal{B}}(γ)Φ_{\mathcal{B}}^{-1}(w) = Φ_{\mathcal{B}}^{-1}(v)^T M_{\mathcal{B}}(γ)^T Φ_{\mathcal{B}}^{-1}(w)^T \\
	 &= \underbrace{(Φ_{\mathcal{B}}^{-1}(w)^T M_{\mathcal{B}}(γ)Φ_{\mathcal{B}}^{-1})^T}_{∈ K} = Φ_{\mathcal{B}}^{-1}(w)^T M_{\mathcal{B}}(γ) Φ_{\mathcal{B}}^{-1}(v) = γ(w, v).
     \end{align*}
  2. analog.
  #+end_proof
* Quadratische Räume
  #+ATTR_LATEX: :options [Quadratische Form]
  #+begin_defn latex
  $V$ K-VR. Eine Abbildung $q: V \to K$ heißt eine *quadratische Form* auf $V$, genau dann wenn folgende Bedingungen erfüllt sind:
  - (Q1) $q(λ v) = λ^2 q(v) ∀ λ ∈ K, v ∈ V$
  - (Q2) Die Abbildung $ε_q: V × V \to K, (v, w) ↦ q(v + w) - q(v) - q(w)$ ist eine
	(automatisch symmetrische) Bilinearform
  #+end_defn
  #+begin_ex latex
  $K = ℝ, V = ℝ^2, q(\cvec{x_1; x_2}) = x_1^2 + x_1 x_2 + x_2^2$ ist eine quatratische Form auf $ℝ^2$
  (Q1) ist erfüllt, (Q2) ist ebenfalls erfüllt, denn
  \begin{align*}
  ε_q(\cvec{x_1; x_2}, \cvec{y_1; y_2}) &= q(\cvec{x_1 + y_1; x_2 + y_2}) - q(\cvec{x_1; x_2}) - q(\cvec{y_1; y_2}) \\
  &= (x_1 + y_1)^2 + (x_1 + y_1)(x_2 + y_2) + (x_2 + y_2)^2 - x_1^2 - x_1 x_2 - x_2^2 - x_2^2 - y_1^2 - y_1 y_2 - y_2^2 \\
  &= 2x_1 y_1 + x_1 y_2 + x_2 y_1 + 2x_2 y_2
  \end{align*}
  das heißt $ε_q$ ist symmetrische Bilinearform.
  #+end_ex
  #+begin_remark latex
  $\cha K \neq 2, V$ K-VR, $\SymBil(V) := \{γ: V × V \to K \mid γ \text{ ist symmetrische Bilinearform}\}, \Quad(V) := \{q: V \to K \mid q \text{ ist eine quadratische Form}\}$. Dann sind die Abbildungen
  \[Φ: \SymBil(V) \to \Quad(V), γ ↦ q_γ \quad q_γ:V \to K, v ↦ γ(v, v)\]
  \[Ψ: \Quad(V) \to \SymBil(V), q ↦ γ_q \frac{1}{2}ε_q\]
  zueinander inverse Bijektionen.
  #+end_remark
  #+begin_proof latex
  1. $Φ$ ist wohldefiniert, das heißt $q_γ ∈ \Quad(V) ∀ γ ∈ \SymBil(V)$. \\
	 Q1: Sei $λ ∈ K, v ∈ V ⇒ q_γ(λ v) = γ(λv, λv) = λ^2 γ(v, v) = λ^2 q_γ(v)$ \\
	 Q2:
	 \begin{align*}
     ε_{q_γ} &= q_γ(v + w) - q_γ(v) - q_γ(w) = γ(v + w, v + w) - γ(v, v) - γ(w, w) \\
     &= γ(v, w) + γ(w, v) = 2γ(v, w)
     \end{align*}
	 $⇒ ε_{q_γ}$ symmetrische Bilinearform.
  2. $Ψ$ ist wohldefiniert, denn für jedes $q ∈ \Quad(V)$ ist $γ_q = (1/2) ε_q ∈ \SymBil(V)$, da $ε_q ∈ \SymBil(V)$
  3. $Φ \circ Ψ = \id_{\Quad(V)}$: Für $q ∈ \Quad(V), v ∈ V$ ist
	 \[(Φ \circ Ψ)(q)(v) = Φ(γ_q)(v) = γ_q(v, v) = \frac{1}{2}(q(v + v) - q(v) - q(v)) = q(v)\]
  4. $Ψ \circ Φ = \id_{\SymBil(v)}$: Für $γ ∈ \SymBil(v), v, w ∈ V$ ist
	 \[(Ψ \circ Φ)(γ)(v, w) = Ψ(q_γ)(v, w) = \frac{1}{2} ε_{q_γ}(v, w) = γ(v, w)\]
  #+end_proof
  #+begin_note latex
  Philosophie dahinter: symmetrische Bilinearformen, quadratische Formen auf $K$ sind für $\cha K \neq 2$ fast dasselbe. Für
  $\cha k = 2$ kann man die Abblidung $Φ$ immer noch definieren, $Φ$ ist im allgemeinen aber weder injekiv, noch surjektiv.
  Exemplarisch: Für $K = \mathbb{F}_2, V = \mathbb{F}_2^2$ liegt die quadratische Form $q: \mathbb{F}_2^2 \to \mathbb{F}, \cvec{x_1; x_2} ↦ x_1^2 + x_1 x_2 + x_2^2$ liegt nicht im Bild vom $Φ$.
  #+end_note
  Für den Rest dieses Abschnittes sei $K$ stets ein Körper mit $\cha K \neq 2$
  #+ATTR_LATEX: :options [Quadratischer Raum]
  #+begin_defn latex
  Ein *quadratischer Raum* ist ein Paar $(V, γ)$, bestehend aus endlichdimensionalem K-VR $V$ und einer symmetrischen Bilinearform $γ$ auf $V$.
  $v, w ∈ V$ heißen *orthogonal* bezüglich $γ ⇔ γ(v, w) = 0$. $(v_i)_{i ∈ I}$ Familie von Vektoren aus $V$ heißt orthogonal bezüglich $γ ⇔ γ(v_i, v_j) = 0 ∀ i, j ∈ I, i \neq j$.
  Eine Familie $(v_1, \dots, v_n)$ von Vektoren aus $V$ heißt eine *Orthogonalbasis* (OB) von $(V, γ) ⇔ (v_1, \dots, v_n)$ ist eine Basis von $V$ und ist orthogonal bezüglich $γ$.
  #+end_defn
  #+begin_note latex
  - Ist $γ$ aus dem Kontext klar, wird es auch häufig weggelassen.
  -	Ist $\mathcal{B}$ eine Basis von $V$, dann gilt $\mathcal{B}$ OB von $(V, γ) ⇔ M_{\mathcal{B}}(γ)$ ist eine Diagonalmatrix.
  #+end_note
  #+begin_defn latex
  $(V, γ_v), (W, γ_w)$ quadratische Räume, $f: V \to W$ lineare Abbildung. $f$ heißt *Homomophismus quadratischer Räume* $⇔$
  \[γ_w(f(v_1), f(v_2)) = γ_v(v_1, v_2) ∀ v_1, v_2 ∈ V\]
  $f$ heißt *Isomorphismus quadratischer Räume* $⇔ f$ ist ein Isomorphismus von K-VR und ein Homomophismus quadratischer Räume.
  Notation: Wir schreiben häufig $f:(V, γ_v) \to (W, γ_w)$ für Abbildungen / Homomorphismen quadratischer Räume.
  #+end_defn
  #+begin_note latex
  Ist $f: (V, γ_v) \to (W, γ_w)$ ein Isomorphismus quadratischer Räume, dann ist $f^{-1}: (W, γ_w) \to (V, γ_v)$ ebenfalls ein Isomorphismus quadratischer Räume, und es ist $\Rang(γ_v) = \Rang(γ_w)$ (nachrechnen...)
  #+end_note
  *Ziel*: Klassifiziere quadratische Räume bis auf Isomorphie quadratischer Räume.
  #+begin_thm latex
  $(V, γ)$ quadratischer Raum. Dann besitzt $(V, γ)$ eine OB.
  #+end_thm
  #+begin_proof latex
  per Induktion nach $n = \dim V$. \\
  IA: $n= 0$: leere Familie ist OB. \\
  IS: Sei $n \geq 1$
  1. Fall: $γ(v, v) = 0 ∀ v ∈ V$
     \[⇒ ∀ v, w ∈ V: 0 = γ(v + w, v + w) = γ(v, v) + γ(w, w) + 2 γ(v, w) = 2γ(v, w)\]
	 $⇒ γ(v, w) = 0 ∀ v, w ∈ V ⇒$  Jede Basis von $V$ ist OB von $(V, γ)$
  2. $∃ v_1 ∈ V: γ(v_1, v_1) \neq 0$. Sei $Γ: V \to V^{\ast}, v ↦ γ(v, ·)$ die zu $γ$ gemäß 20.10 gehörige lineare Abbildung. Setze $H = \ker(Γ(v_1)) = \{w ∈ W \mid γ(v_1, w) = 0\}$
	 \[⇒ \dim H = \dim V - \underbrace{\dim \im(Γ(v_1))}_{\mathclap{\leq K \text{ beachte: } Γ(v_1) ∈ V^{\ast}}} ∈ \{n, n - 1\}\]
	 Es ist $v_1 \not ∈ H$ wegen $γ(v_1, v_1) \neq 0 ⇒ \dim H = n - 1 ⇒ V = \Lin((v_1)) \oplus H$. $(H, γ \mid_{H × H})$ ist ein quadratischer Raum der Dimension $n - 1$. Wegen IV	existiert eine OB
	 $(v_2, \dots, v_n)$ von $(H, γ\mid_{H × H}) ⇒ (v_1, v_2, \dots, v_n)$ ist OB von $(V, γ)$
  #+end_proof
  #+begin_conc latex
  $A ∈ M(n × n, K)$ symmetrisch. Dann existiert $T ∈ \GL(n, K)$, sodass $T^T A T$ eine Diagonalmatrix.
  #+end_conc
  #+begin_proof latex
  $A$ definiert eine symmetrische Bilinearform $Δ(A) = Δ_A^{(e_1, \dots, e_n)}$ auf $K^n$ (vergleiche 20.7, $Δ(A)(v, w) = v^T A w$).
  Nach 21.6 existiert eine OB $\mathcal{B}$ von $(K^n, Δ(A)) ⇒ M_{\mathcal{B}}(Δ(A))$ ist Diagonalmatrix, und es ist
  \[M_{\mathcal{B}}(Δ(A)) = \underbrace{(T_{(e_1, \dots, e_n)}^{\mathcal{B}})^T}_{= T^T} \underbrace{M_{(e_1, \dots, e_n)}(Δ(A))}_{A} \underbrace{T_{(e_1, \dots, e_n)}^{\mathcal{B}}}_{=: T}\]
  #+end_proof
  #+begin_conc latex
  $(V, γ)$ quadratischer Raum, $n = \dim V, r = \Rang(γ)$. Dann existieren $λ_1, \dots, λ_r ∈ K \setminus \{0\}$ und ein Isomorphismus von quadratischen Räumen
  \[Φ: (K^n, Δ(\begin{pmatrix}λ_1 &   &   &   & 0 &   \\   & \ddots &   &   &   &   \\   &   & λ_r &   &   &   \\   &   &   & 0 &   &   \\   &   &   &   & \ddots &   \\   & 0  &   &   &  & 0 \end{pmatrix})) \to  (V, γ)\]
  #+end_conc
  #+begin_proof latex
  Wegen 21.6 existiert eine OB $\mathcal{B} = (v_1, \dots, v_n)$ von $(V, γ)$. Nach Umordnung von $v_1, \dots, v_n$ sei $γ(v_i, v_i) \neq 0$ für $i = 1, \dots, s$ und $γ(v_i, v_i) = 0$ für $i = s + 1, \dots, n$
  \[⇒ M_{\mathcal{B}}(γ) = \begin{pmatrix} λ_1 & & & & & 0 \\ & \ddots & & & & \\ & & λ_s & & & \\ & & & 0 & & \\ & & & & \ddots & \\ 0 & & & & & 0\end{pmatrix} \quad λ_1, \dots, λ_s ∈ K\setminus \{0\}, r = \Rang(γ) = \Rang M_{\mathcal{B}}(γ) = s\]
  Setze $Φ:= Φ_{\mathcal{B}}: K^n \to V, e_i ↦ v_i$ (Koordinatensystem zu $\mathcal{B}$, vegleiche 15.2). $Φ$ ist Isomorphismus
  \begin{align*}
  γ(Φ_{\mathcal{B}}(v), Φ_{\mathcal{B}}(w)) &= Φ_{\mathcal{B}}^{-1}(Φ_{\mathcal{B}}(v))^T M_{\mathcal{B}}(γ) Φ_{\mathcal{B}}^{-1}(Φ_{\mathcal{B}}(w)) = v_t M_{\mathcal{B}}(γ) w \\
  &= v^T \begin{pmatrix} λ_1 & & & & & 0 \\ & \ddots & & & & \\ & & λ_r & & & \\ & & & 0 & & \\ & & & & \ddots & \\ 0 & & & & & 0\end{pmatrix} w = Δ(\begin{pmatrix}λ_1 &   & 0 \\   & \ddots &   \\ 0 &   & λ_r\end{pmatrix})(v, w)
  \end{align*}
  #+end_proof
  #+begin_note latex
  $λ_1, \dots, λ_r$ sind im allgemeinen nicht eindeutig bestimmt.
  #+end_note
  *Frage:* Kann man über speziellen Körpern mehr sagen? Wir werden $K = ℂ, ℝ$ untersuchen.
  #+begin_thm latex
  $(V, γ)$ quadratischer Raum über $ℂ, n = \dim V, r = \Rang γ$. Dass existiert eine Orthogonalbasis $\mathcal{B}$ von $(V, γ)$ mit
  \[M_{\mathcal{B}}(γ) = \begin{pmatrix}E_r & 0 \\ 0 & 0\end{pmatrix}\]
  Insbesondere existiert ein Isomorphismus quadratischer Räume $Φ(ℂ^n, Δ(\begin{pmatrix}E_r & 0 \\ 0 & 0\end{pmatrix})) \to (V, γ)$
  #+end_thm
  #+begin_proof latex
  Sei $(\tilde v_1, \dots, \tilde v_n)$ eine Orthogonalbasis von $(V, γ)$. Setze
  \[v_i := \begin{cases} \tilde v_i & γ(\tilde v_i, \tilde v_i) = 0 \\ \frac{1}{\sqrt{\tilde v_i, \tilde v_i}} \tilde v_i & γ(\tilde v_i, \tilde v_i) \neq 0 \end{cases}\]
  Hierber ist $\sqrt{γ(\tilde v_i, \tilde v_i)}$ eine komplexe Zahl	$α$ mit $α^2 = γ(\tilde v_i, \tilde v_i)$. Falls $γ(\tilde v_i, \tilde v_i) \neq 0$, dass ist
  \[γ(v_i, v_i) = γ(\frac{1}{\sqrt{γ(\tilde v_i, \tilde v_i)}}, \frac{1}{\sqrt{γ(\tilde v_i, \tilde v_i)}}) = \frac{1}{γ(\tilde v_i, \tilde v_i)} γ(\tilde v_i, \tilde v_i) = 1\]
  Außerdem: $γ(v_i, v_j) = 0 ∀ i \neq j$, da $γ(\tilde v_i, \tilde v_j) = 0 ∀ i \neq 0$.
  Setze $\mathcal{B} := (v_1, \dots, v_n)$. Nach eventueller Umnummerierung von $v_1, \dots, v_n$ ist
  \[M_{\mathcal{B}}(γ) = \begin{pmatrix}E_r & 0 \\ 0 & 0\end{pmatrix}\]
  wobei $r = \Rang M_{\mathcal{B}}(γ) = \Rang γ$.
  #+end_proof
  #+begin_conc latex
  $A ∈ M(n × n, ℂ)$ symmetrisch, $r = \Rang A$. Dass existiert ein $T ∈ \GL(n, ℂ)$, sodass
  \[T^T A T = \begin{pmatrix}E_r & 0 \\ 0 & 0\end{pmatrix}\]
  #+end_conc
  #+ATTR_LATEX: :options [21.11]
  #+begin_conc latex
  $(V, γ_V), (W, γ_W)$ quadratische Räume über $ℂ$. Dann sind äquivalent:
  1. Es gibt einen Isomorphismus quadratischer Räume $(V, γ_V) \to (W, γ_W)$
  2. $\dim V = \dim W$ und $\Rang γ_V = \Rang γ_W$
  #+end_conc
  #+begin_proof latex
  1. $⇒$ 2. vergleiche Anmerkung nach 21.5
  2. $⇒$ 1. Sei $n = \dim V = \dim W, r = \Rang γ_V = \Rang γ_W$. $⇒ (V, γ_V), (W, γ_W)$ sind als quadratische Räume isomorph zu $(ℂ^n, Δ(\begin{pmatrix}E_r &   \\   &  \end{pmatrix}))$,
	 also auch $(V, γ_V) \cong (W, γ_W)$
  #+end_proof
  #+begin_defn latex
  $(V, γ)$ quadratischer Raum, $U_1, \dots, U_m ⊆ V$ UVR mit $V = U_1 \oplus \dots \oplus U_n$. Die direkte Summe heißt
  *orthogonale direkte Summe*
  \[(V = U_1 \hat oplus \dots \hat \oplus U_m) \xLeftrightarrow{\text{Def}} γ(u_i, u_j) = 0 ∀ u_i ∈ U_i, u_j ∈ U_j, i \neq j\]
  alternativ $\operp$
  #+end_defn
  #+begin_thm latex
  $(V, γ)$ quadratischer Raum über $ℝ, n = \dim V$. Dann existiert eine Orthogonalbasis $\mathcal{B}$ von $(V, γ)$, sowie $r_+, r_- ∈ \{0, \dots, \dim V\}$ mit
  \[M_{\mathcal{B}}(γ) = \begin{pmatrix}E_{r_+} &   & 0 \\   & -E_{r_-} &   \\ 0 &   & 0\end{pmatrix}\]
  Insbesondere existiert ein Isomorphismus quadratischer Räume
  \[(ℝ^n, Δ(\begin{pmatrix}E_{r_+} &   & 0 \\   & -E_{r_-} &   \\ 0 &   & 0\end{pmatrix})) \to (V, γ)\]
  Die Zahlen $r_+, r_-$ sind unabhängig von der Wahl einer solchen Basis. Wir nennen $\Signatur(γ) := (r_+, r_-)$ heißt die *Signatur* von $γ$.
  #+end_thm
  #+begin_proof latex
  1. Sei $(\tilde v_1, \dots, \tilde v_n)$ eine Orthogonalbasis von $(V, γ)$. Wir setzen
	 \[v_i := \begin{cases} \tilde v_i & γ(\tilde v_i, \tilde v_i) = 0 \\ \frac{1}{\sqrt{\abs{γ(\tilde v_i, \tilde v_i)}}} & γ(\tilde v_i, \tilde v_i) \neq 0\end{cases}\]
	 Falls $γ(\tilde v_i, \tilde v_i) \neq 0$, dass ist
	 \begin{align*}
	 γ(v_i, v_i) &= γ(\frac{1}{\sqrt{\abs{γ(\tilde v_i, \tilde v_i)}}} \tilde v_i, \frac{1}{\sqrt{\abs{γ(\tilde v_i, \tilde v_i)}}} \tilde v_i) \\
	 &=	\frac{1}{\abs{γ(\tilde v_i, \tilde v_i)}} γ(\tilde v_i, \tilde v_i) ∈ \{\pm 1\}
     \end{align*}
	 $γ(v_i, v_j) = 0$ für $i \neq j$.
	 Setze $\mathcal{B} := (v_1, \dots, v_n)$. Nach eventueller Umnummerierung von $v_1, \dots, v_n$ ist
	 \[M_{\mathcal{B}}(γ) = \begin{pmatrix} 1 & & & & & & & & \\ & \ddots & & & & & & & \\ & & 1 & & & & & & \\ & & & -1 & & & & & \\ & & & & \ddots & & & & \\ 1 & & & & & -1 & & & \\ & & & & & & 0 & & \\ & & & & & & & \ddots & \\ & & & & & & & & 0 \end{pmatrix} = \begin{pmatrix}E_{r_+} &   & 0 \\   & -E_{r_-} &   \\ 0 &   & 0\end{pmatrix}\]
	 mit geeigneten $r_+, r_- ∈ \{0, \dots, n\}$
  2. $r_+, r_-$ sind basisunabhängig: Es ist $r_+ + r_- = \Rang γ$, dies ist basisunabhängig. Es gilt zu zeigen: $r_+$ ist basisunabhängig.
	 Setze $V_+ := \Lin((v_1, \dots, v_{r_+})), V_- = \Lin((v_{r_+ + 1} ,\dots, v_{r_+ + r_-})), V_0 := \Lin((v_{r + + r_- + 1}, \dots, v_n))$
	 $⇒ V = V_+ \hat \oplus V_- \hat \oplus V_0$. Setze
     \[s := \max\{\dim W \mid W ⊆ V \text{ UVR mit } γ(w, w) > 0 ∀ w ∈ W, w \neq 0\}\]
     dies ist wohldefiniert. $V_+$ ist ein UVR von $V$ mit $γ(w, w) > 0 ∀ w ∈ V_+, w \neq 0$, denn für $w = λ_1 v_1 + \dots + λ_{r_+} v_{r_+}$ ist
	 \[γ(w, w) = λ_1^2 \underbrace{γ(v_1, v_1)}_{= 1} + \dots + λ_{r_+}^2\underbrace{v_{r_+}, v_{r_+}}_{= 1} = λ_1^2 + \dots + λ_{r_+}^2 > 0 \text{ falls } w \neq 0\]
	 $⇒ s \geq \dim V_+ = r_+$
	 Annahme: Es existiert ein UVR $W ⊆ V$ mit $γ(w, w) > 0 ∀ w ∈ W, w \neq 0$ und $\dim W > r_+$
     \[⇒ \underbrace{\dim W}_{> r_+} + \underbrace{\dim V_-}_{= r_-} + \underbrace{\dim V_0}_{n - (r_+ + r_-)} > n\]
     \begin{align*}
     ⇒ \dim(W ∩ (V_- \hat\oplus V_0)) &= \dim W + \dim(V_- \hat\oplus V_0) - \dim(W + (W_- \hat\oplus V_0)) \\
     &= \underbrace{\dim W + \dim V_- + \dim V_0}_{> n} - \underbrace{\dim(W + (V_- \hat\oplus V_0))}_{\mathclap{\leq n, \text{ da } W + (V_- \hat\oplus W_0) \text{ UVR von } V}} \\
     &= >0
     \end{align*}
     $⇒$ Es existiert $w ∈ W, w \neq 0$ mit $w ∈ W_- \hat \oplus V_0$. \\
     $⇒$ Es existiert $w_- ∈ V_-, w_0 ∈ V_0$ mit $w = w_- + w_0$ \\
     $⇒$ $γ(w, w) = γ(w_- + w_0, w_- + w_0) = \underbrace{γ(w_-, w_-)}_{< 0} + \underbrace{γ(w_0, w_0)}_{= 0} < 0$
	 Andererseits: $γ(w, w) > 0$ wegen $w ∈ W, w \neq 0 \lightning$. Somit: $r_+ = s$, insbesondere unabhängig von Basiswahl.
  #+end_proof
  #+ATTR_LATEX: :options [Sylvesterscher Trägheitssatz]
  #+begin_concdef latex
  $A ∈ M(n × n, ℝ)$ symmetrisch. Dann existieren $T ∈ \GL(n, ℝ), r_+, r_- ∈ \{0, \dots, n\}$ mit
  \[T^T A T = \begin{pmatrix}E_{r_+} &   & 0 \\   & -E_{r_-} &   \\ 0 &   & 0\end{pmatrix}\]
  Die Zahlen $r_+, r_-$ sind unabhängig von der Wahl eines solchen $T$. $\Signatur(A) := (r_+, r_-)$ heißt *Signatur* von $A$.
  #+end_concdef
  #+begin_proof latex
  folgt aus 21.13 (analog zum Beweis von 21.7).
  #+end_proof
  #+begin_note latex
  Ist $S ∈ \GL(n, ℝ)$, dann haben die Matrixen $A$ und $S^T A S$ diesselbe Signatur, denn: Ist $\tilde T ∈ \GL(, ℝ)$ mit \[\tilde T^T(S^T A S) T = \begin{pmatrix}E_{r_+} &   & 0 \\   & -E_{r_-} &   \\ 0 &   & 0\end{pmatrix}\], dann ist
  \[(S\tilde T)^T A (S\tilde T) = \begin{pmatrix}E_{r_+} &   & 0 \\   & -E_{r_-} &   \\ 0 &   & 0\end{pmatrix}\]
  #+end_note
  #+begin_conc latex
  $(V, γ_V), (W, γ_W)$ quadratische Räume über $ℝ$. Dann sind äquivalent:
  1. Es gibt einen Isomorphismus quadratischer Räume $(V, γ_V) \to (W, γ_W)$
  2. $\dim V = \dim W$ und $\Signatur(γ_V) = \Signatur(γ_W)$
  #+end_conc
  #+begin_proof latex
  1. $⇒$ 2. Für $\Signatur(γ_V) = \Signatur(γ_W)$ verwende Charakterisierung von $r_+$ aus dem Beweis von 21.3.
  2. $⇒$ 1. aus 21.13, analog zum Beweis von 21.11
  #+end_proof
  #+begin_note latex
  Man kann Folgerung 21.11/21.15 verwenden, um quadratische Formen über $ℂ$ beziehungsweise $ℝ$ bis auf Äquivalenz zu klassifizieren (vergleiche Übungen)
  #+end_note
* Euklidische Räume
  #+begin_defn latex
  $V ℝ$ -VR, $γ: V × V \to ℝ$ symmetrische Bilinearform. $γ$ heißt
  - *positiv definit* $\xLeftrightarrow{\text{Def}} γ(v, v) > 0 ∀ v ∈ V \setminus\{0\}$
  - *positiv semidefinit* $\xLeftrightarrow{\text{Def}} γ(v, v) \geq 0 ∀ v ∈ V \setminus\{0\}$
  - *negativ definit* $\xLeftrightarrow{\text{Def}} γ(v, v) < 0 ∀ v ∈ V \setminus\{0\}$
  - *negativ semidefinit* $\xLeftrightarrow{\text{Def}} γ(v, v) \leq 0 ∀ v ∈ V \setminus\{0\}$
  - *indefinit* $\xLeftrightarrow{\text{Def}} γ$ ist weder positiv noch negativ semidefinit.
  Eine positiv definite symmetrische Bilinearform nennt man auch ein *Skalarprodukt*.
  #+end_defn
  #+begin_ex latex
  1. $V = ℝ^n, <·,·>:ℝ^n × ℝ^n \to ℝ, <\cvec{x_1; \vdots; x_n}, \cvec{y_1; \vdots; y_n}> := x_1 y_1 + \dots + x_n y_n$
	 ist ein Skalarprodukt auf dem $ℝ^n$. Positiv Definitheit:
	 \[<\cvec{x_1; \vdots; x_n}, \cvec{x_1; \vdots; x_n}> = x_1^2 + \dots + x_n^2 > 0, \text{ falls } \cvec{x_1; \vdots; x_n} \neq 0\]
	 $<·, ·>$ heißt das *Standardskalarprodukt* auf dem $ℝ^n$.
  2. $V= \mathcal{C}[0, 1]$
	 \[γ: \mathcal{C}[0, 1] × \mathcal{C}[0, 1] \to ℝ, (f, g) ↦ ∫_0^1 f(t) g(t) \d t\]
	 ist ein Skalarprodukt.
  #+end_ex
  #+begin_note latex
  Um die Definitheit einer symmetrischen Bilinearform nachzuweisen, genügt es nicht, das Verhalten auf den Basisvektoren zu untersuchen:
  Sei $γ: ℝ^2 × ℝ^2 \to ℝ$ gegeben durch
  \[γ = Δ(\begin{pmatrix}1 & -1 \\ -2 & 1\end{pmatrix})\]
  das heißt
  \[M_{(e_1, e_2)}(γ) = \begin{pmatrix}1 & -2 \\ -2 & 1\end{pmatrix}\]
  Dann ist $γ(e_1, e_1) = 1, γ(e_2, e_2) = 1$ aber
  \[γ(\cvec{1;1}, \cvec{1; 1}) = \begin{pmatrix}1 & 1\end{pmatrix} \begin{pmatrix}1 & -2 \\ -2 & 1\end{pmatrix} \begin{pmatrix}1 \\ 1\end{pmatrix} = -2 < 0\]
  das heißt $γ$ ist indefinit.
  #+end_note
  #+begin_defn latex
  Ein *Euklidischer Raum* ist ein Paar $(V, γ)$, bestehend aus einem endlichdimensionalen $ℝ$ -VR $V$ und einem Skalarprodukt $γ$ auf $V$.
  Für den Rest dieses Abschsittes sei $(V, γ)$ ein Euklidischer Raum.
  #+end_defn
  #+begin_defn latex
  $v ∈ V$
  \[\norm{v} := \sqrt{γ(v, v)}\]
  heißt die *Norm* auf $V$. \\
  $(v_i)_{i ∈ I}$ Familie von Vektoren aus $V$ heißt *orthonormal* $\xLeftrightarrow{\text{Def}} (v_i)_{i ∈ I}$ ist orthogonal und $\norm{v_i} = 1 ∀ i ∈ I$. \\
  $\mathcal{B} = (v_1, \dots, v_n)$ heißt *Orthonormalbasis von $V ((V, γ))$ (ONB) $⇔ \mathcal{B}$ ist Basis von $V$ und $\mathcal{B}$ ist orthonormal.
  #+end_defn
  #+begin_remark latex
  $(v_1, \dots, v_n)$ orthogonale Familie von Vektoren aus $V \setminus \{0\}$. Dann gilt:
  1. $(\frac{v_1}{\norm{v_1}}, \dots, \frac{v_n}{\norm{v_n}})$ ist eine orthonormale Familie
  2. $(v_1, \dots, v_n)$ ist linear unabhängig.
  #+end_remark
  #+begin_proof latex
  1. $\norm{v_i}^2 = γ(v_i, v_i) \neq 0$, da $γ$ positiv definit und $v_i \neq 0$.
	 \[γ(\frac{v_i}{\norm{v_i}}, \frac{v_j}{\norm{v_j}}) = \frac{1}{\norm{v_i}\norm{v_j}} γ(v_i, v_j) = \begin{cases} 0 & i \neq j \\ \frac{γ(v_i, v_i)}{\norm{v_i}^2} = 1 & i = j\end{cases}\]
  2. Sei $λ_1 v_1 + \dots + λ_n v_n = 0$
	 \begin{align*}
	 ⇒ λ_1 γ(v_1, v_i) + \dots + λ_n γ(v_n, v_i) &= 0 \\
	 ⇒ λ_i &= 0
     \end{align*}
  #+end_proof
  #+begin_remark latex
  Es gilt:
  1. $(V, γ)$ besitzt eine Orthonormalbasis
  2. $γ$ ist nicht-ausgeartet
  3. Es gibt eine Basis $\mathcal{B}$ von $V$ mit $M_{\mathcal{B}}(γ) = E_n$, wobei	$n = \dim V$
  #+end_remark
  #+begin_proof latex
  Der quadratische Raum $(V, γ)$ hat eine Orthogonalbasas $(v_1, \dots, v_n)$
  \[⇒ \mathcal{B} := (\frac{v_1}{\norm{v_1}}, \dots, \frac{v_n}{\norm{v_n}})\]
  ist eine Orthonormalbasis von $(V, γ)$. Es ist $M_{\mathcal{B}}(γ) = E_n$ ($⇒$ 3.),
  insbesodere ist $M_{\mathcal{B}}(γ)$ invertierbar $⇒ γ$ nich ausgeartet $⇒$ 2.
  #+end_proof
  #+begin_remark latex
  $\mathcal{B} = (v_1, \dots, v_n)$ Orthonormalbasis von $(V, γ), v ∈ V$. Dann gilt: Ist $v = λ_1 v_1 + \dots + λ_n v_n$, dann ist $λ_i = γ(v, v_i) ∀ i = 1, \dots, n$
  #+end_remark
  #+begin_proof latex
  $γ(v, v_i) = λ_1 γ(v_1, v_i) + \dots + λ_n γ(v_n, v_i) = λ_i \underbrace{γ(v_i, v_i)}_{= 1} = λ_i$
  #+end_proof
  #+begin_remdef latex
  $U ⊆ V$ Untervektorraum.
  \[U^{\perp} := \{v ∈ V \mid γ(v, u) = 0 ∀ u ∈ U\}\]
  heißt das *orthogonale Komplement* zu $U$. $U^{\perp}$ ist ein Untervektorraum von $V$.
  #+end_remdef
  #+begin_proof latex
  leicht nachzurechnen
  #+end_proof
  #+begin_defthm latex
  $U ⊆ V$ Untervektorraum. Dann gilt:
  1. $V = U \oplus U^{\perp}$
  2. $\dim U^{\perp} = \dim V - \dim U$
  3. $(U^{\perp})^{\perp} = U$
  4. Ist $(u_1, \dots, u_m)$ eine Orthogonalbasis von $(U, γ\mid_{U × U})$, und ist $v ∈ V$ mit $v = u + v', u ∈ U, v' ∈ U^{\perp}$, dass ist
	 \[u = \sum_{j = 1}^{m} γ(v, u_j) u_j\]
	 Die lineare Abbildung
     \[π_u: V\to U, v ↦ \sum_{j = 1}^{m} γ(v, u_j) u_j\]
	 hießt die *Orthogonalprojektion* von $V$ auf $U$.
  #+end_defthm
  #+begin_proof latex
  1. $U + U^{\perp} = V$, denn: \\
	 Sei $(u_1, \dots, u_m)$ eine Orthogonalbasis von $(U, γ\mid_{n × n}), v ∈ V$. Setze
	 \begin{align*}
     v' &:= V - \sum_{j = 1}^{m} γ(v, u_j) u_j \\
	 ⇒ γ(v', u_i) &= γ(v, u_i) - \sum_{j = 1}^{m} γ(v, u_j) γ(u_j, u_i) = γ(v, u_i) - γ(v, u_i) = 0 ∀ i = 1, \dots, m \\
	 ⇒ v' &∈ U^{\perp} \\
	 ⇒ v &= \underbrace{\sum_{j = 1}^{m} γ(v, u_j) u_j}_{∈ U} + \underbrace{v'}_{\mathclap{∈ U^{\perp}}} \\
	 ⇒ V &= U + U^{\perp}
     \end{align*}
	 $U ∩ U^{\perp} = \{0\}$, denn: $u ∈ U ∩ U^{\perp} ⇒ γ(u, u) = 0 ⇒ u = 0$ (da $γ$ Skalarprodukt)
  2. aus 1., 2.
  3. Sei $u ∈ U ⇒ γ(u, w) = 0 ∀ w = U^{\perp} ⇒ u ∈ (U^{\perp})^{\perp}$, das heißt $U ⊆ U^{\perp\perp}$.
	 Wegen $\dim(U^{\perp})^{\perp} = \dim V - \dim U^{\perp} = \dim V - (\dim V - \dim U) = \dim U$ foglt $U = U^{\perp\perp}$
  #+end_proof
  #+begin_note latex
  Insbesondere gilt für alle $v ∈ V: v - π_U(v) ∈ U^{\perp}$
  #+end_note
  #+begin_ex latex
  $(V, γ) = (ℝ^2, <·,·>), U = \Lin(\cvec{1; 1}) ⇒ U^{\perp} = \Lin(\cvec{-1; 1})$, denn $\cvec{-1; 1} ∈ U^{\perp}$ wegen $<\cvec{-1; 1}, \cvec{1; 1}> = 0$, und es ist
  $\dim U^{\perp} = 2 - \dim U = 2 - 1 = 1$. Jedes Element aus $V$ lässt sich eindeutig schreiben als
  \[v = λ\cvec{1; 1} + μ\cvec{-1; 1}\]
  das heißt
  \[π_u: v = \underbrace{λ\cvec{1; 1}}_{∈ U} + μ \underbrace{\cvec{-1; 1}}_{∈ U^{\perp}} ↦ λ\cvec{1; 1} = γ(v, \cvec{1; 1})\vec{1; 1}\]
  #+end_ex
  *Frage:* Wie bestimmt man explizit eine Orthogonalbasis eines Euklidischen Raumes?
  #+ATTR_LATEX: :options [Gram-Schmidt-Verfahren]
  #+begin_algorithm latex
  *Eingabe*: $(v_1, \dots, v_n)$ Basis von $V$. \\
  *Ausgabe*: Orthonormalbasis $(w_1, \dots, w_n)$ von $(V, γ)$ \\
  *Durchführung:*
  1. Setze
     \[w_1 := \frac{v_1}{\norm{v_1}}\]
  2. Setze für $k = 2, \dots, n$
	 \[\tilde w_k := v_k - \sum_{i = 1}^{k - 1}γ(v_k, w_i) w_i, \quad w_k := \frac{\tilde w_k}{\norm{\tilde w_k}}\]
  3. $(w_1, \dots, w_n)$ ist eine Orthonormalbasis von $(V, γ)$
  #+end_algorithm
  #+begin_proof latex
  Sei $U_k := \Lin((v_1, \dots, v_k))$ für $k = 1, \dots, n$. Wir zeigen per Induktion nach $k$, dass $(w_1, \dots, w_k)$ eine Orthogonalbasis von $(U_k, γ\mid_{U_k × U_k})$ ist
  (Behauptung folgt dann aus $k = n$). \\
  Induktionsanfang: $k = 1$ klar \\
  Induktionsschritt: Sei $π_{k - 1} := π_{U_{k - 1}}: V \to V_{k - 1}$ die orthogonale Projektion.
  \[⇒ \tilde w_k = v_k - π_{k - 1}(v_k)\]
  da $(w_1, \dots, w_{k - 1})$ Orthogonalbasis von $U_{k - 1}$ nach Induktionsvorraussetzung. $⇒\tilde w_k ∈ U_{k - 1}^{\perp}$.
  Außerdem $\tilde w_k \neq 0$, da sonst $v_k = π_{k - 1}(v_k) ∈ U_{k - 1} \lightning$ zu $(v_1, \dots, v_k)$ Basis von U_k
  \[⇒ w_k = \frac{\tilde w_k}{\norm{\tilde w_k}} ∈ U_{k - 1}^{\perp}\]
  und es ist
  \[γ(w_k, w_i) = \begin{cases} 0 & i = 1, \dots, k - 1 \\ 1 & i = k\end{cases}\]
  $⇒ (w_1, \dots, w_k)$ Orthogonalbasis von $U_k$
  #+end_proof
  #+begin_ex latex
  Wir betrachten $(ℝ^3, <·, ·>), U = \Lin((v_1, v_2))$ mit $v_1 := \cvec{2; 0; 1}, v_2 := \cvec{-1; 1; 0}$. Gesucht ist eine Orthogonalbasis von $U$ bezüglich $<·, ·>$.
  Setze
  \begin{align*}
  w &:= \frac{v_1}{\norm{v_1}} = \frac{1}{\sqrt{5}}\cvec{2; 0; 1} \\
  \tilde w_2 &= v_2 - <v_2, w_1> w_1 = \cvec{-1; 1; 0} - <\cvec{-1; 1; 0}, \frac{1}{\sqrt{5}}\cvec{2; 0; 1}> \frac{1}{\sqrt{5}}\cvec{2; 0; 1} \\
  &= \cvec{-1; 1; 0} - \frac{1}{5}<\cvec{-1; 1; 0}, \cvec{2; 0; 1}>\cvec{2; 0; 1} = \cvec{-1; 1; 0} + \frac{2}{5}\cvec{2; 0; 1} = \cvec{-\frac{1}{5}; 1; \frac{2}{5}} = \frac{1}{2} \cvec{-1; 5; 2} \\
  w_2 &= \frac{\tilde w_2}{\norm{\tilde w_2}} = \frac{1}{\sqrt{30}}\cvec{-1; 5; 2}
  \end{align*}
  $⇒ (\frac{1}{\sqrt{5}}\cvec{2; 0; 1}, \frac{1}{\sqrt{30}}\cvec{-1; 5; 2})$ ist eine Orthogonalbasis von $U$.
  #+end_ex
  #+begin_defn latex
  $A ∈ M(n × n, ℝ)$ symmetrisch. $A$ heißt *positiv definit* (Notation: $A > 0$) $\xLeftrightarrow{\text{Def}}$ Die symmetrische Bilinearform
  \[Δ(A): ℝ^n × ℝ^n \to ℝ, (x, y) ↦ x^T A y\]
  ist positiv definit.
  #+end_defn
  #+begin_remark latex
  $A ∈ M(n × n, ℝ)$ symmetrisch. Dass sind äquivalent:
  1. $A > 0$
  2. $∃ T ∈ \GL(n, ℝ): A = T^T T$
  #+end_remark
  #+begin_proof latex
  1. $⇒$ 2. Sei $A > 0 ⇒ (ℝ^n, Δ(A))$ Euklidischer Raum. Sei $\mathcal{B}$ Orthogonalbasis von $(ℝ^n, Δ(A))$ $T := T_{\mathcal{B}}^{(e_1, \dots, e_n)}$
	 \[⇒ E_n = M_{\mathcal{B}}(Δ(A)) = \underbrace{(T_{(e_1, \dots, e_n)}^{\mathcal{B}})^T}_{= (T^{-1})^T} \underbrace{M_{(e_1, \dots, e_n)}(Δ(A))}_{= A} \underbrace{T_{(e_1, \dots, e_n)}^{\mathcal{B}}}_{= T^{-1}}\]
	 $⇒ A = T^T T$
  2. Sei $A = T^T T$ für ein $T ∈ \GL(n, ℝ)$. Für $x ∈ ℝ^n, x \neq 0$ ist
	 \[Δ(A)(x, x) = x^t A w = x^t T^t T x = (Tx)^T Tx = <Tx, Tx> > 0\]
  #+end_proof
  #+begin_note latex
  1., 2. sind äquivatent zu
  3. [@3] Es existiert eine obere Dreiecksmatrix $P$ mit Diagonaleinträgen, sodass $A = P^T P$ (siehe Übungen). Obiges $P$ ist sogar eindeutig bestimmt, eine solche Zerlegung heißt Cholesky-Zerlegung.
  #+end_note
  #+ATTR_LATEX: :options [Cauchy-Schwarz-Ungleichung]
  #+begin_thm latex
  $v, w ∈ V$. Dann gil:
  \[\abs{γ(v, w)} \leq \norm{v} \norm{w}\]
  Gleichheit gilt hierbar genau dann, wenn $(v, w)$ linear abhängig.
  #+end_thm
  #+begin_proof latex
  1. Beweis der Ungleichung: Falls $w = 0$, dass fertig. Im Folgenden sei $w \neq 0$. Für $λ, μ ∈ ℝ$ ist
	 \begin{align*}
	 0 &\leq γ(λv + μw, λv + μw) = λ^2γ (v, v) + μ^2 γ(w, w) + 2 λ μ γ(v, w) \\
	 \intertext{Setze $λ := γ(w, w) > 0$, dividiere durch $λ$}
	 0 &\leq γ(v, v) γ(w, w) + μ^2 + 2μ γ(v, w) \\
	 \intertext{Setze $μ := -γ(v, w)$}
	 0 &\leq γ(v, v)γ(w, w) + γ(v, w)^2 - 2γ(v, w)^2 \\
	 γ(v, w)^2 &\leq γ(v, v)γ(w, w) \\
	 \abs{γ(v, w)} &\leq \norm{v} \norm{w} \\
     \end{align*}
  2. Gleichheitsaussage: Für $w = 0$: $(v, w)$ linear abhängig und "$=$" gilt. Ab jetzt also $w \neq 0$. \\
	 "$\impliedby$" Sei $(v, w)$ linear abhängig $⇒ ∃ λ ∈ K: v = κw$
	 \[⇒ \abs{γ(v, w)}^2 = \abs{γ(λw, w)}^2 = \abs{λ^2}\abs{γ(w, w)}^2 = \abs{γ(w, w)}\abs{γ(λw, λw)} = \norm{w}^2 \norm{λw}^2\]
	 $⇒ \abs{γ(v, w)} = \norm{w}\norm{λ w} = \norm{w}\norm{v}$. \\
	 "$⇒$" Es gelte, sei also $\abs{γ(v, w)} = \norm{v}\norm{w}$. Führe die Rechnung wie in 1. rückwärts durch: Mit $λ := γ(w, w), μ = -γ(v, w)$ folgt
	 dass
     \[γ(λv + μw, λv + μw) = 0 ⇒ λv + μw = 0 ⇒ (v, w)\text{ linear abhängig}\]
  #+end_proof
  #+ATTR_LATEX: :options [Eigenscaften der Norm]
  #+begin_remark latex
  $v, w ∈ V, λ ∈ ℝ$. Dann gilt:
  1. $\norm{v} = 0 ⇔ v = 0$
  2. $\norm{λ v} = \abs{λ} \norm{v}$
  3. $\norm{v + w} \leq \norm{v} + \norm{w}$
  #+end_remark
  #+begin_proof latex
  1. klar, da $γ$ positiv definit
  2. $\norm{λ v}^2 = γ(λ v, λ v) = λ^2 γ(v, v) = λ^2 \norm{v} ⇒ \norm{λ v} = \abs{λ} \norm{v}$
  3.
     \begin{align*}
	 \norm{v + w}^2 &= γ(v + w, v + w) = \norm{v}^2 + \norm{w}^2 + 2γ(v, w) \leq \norm{v}^2 + \norm{w}^2 + 2\abs{γ(v, w)} \\
	 &\leq \norm{v}^2 + \norm{w}^2 + 2 \norm{v} \norm{w} = (\norm{v} + \norm{w})^2 \\
	 ⇒ \norm{v + w} &\leq \norm{v} + \norm{w}
     \end{align*}
  #+end_proof
  #+begin_remark latex
  $v, w ∈ V$. Dann gilt:
  1. $\norm{v + w}^2 = \norm{v}^2 + \norm{w}^2 ⇔ γ(v, w) = 0$ \hfill Satz des Pythagoras
  2. $\norm{v + w}^2 + \norm{v - w}^2 = 2\norm{v}^2 + 2\norm{w}^2$ \hfill Parallelogrammgleichung
  #+end_remark
  #+begin_proof latex
  1. $\norm{v + w}^2 = γ(v + w, v + w) = \norm{v}^2 + \norm{w}^2 + 2γ(v, w) ⇒$ Behauptung
  2. $\norm{v + w}^2 + \norm{v - w}^2 = γ(v + w, v + w) + γ(v - w, v - w) = 2\norm{v}^2 + 2\norm{w}^2$
  #+end_proof
  #+begin_note latex
  $V ℝ$ Vektorraum. Eine Abbildung $\norm{·}: V \to ℝ_{\geq 0}$ mit den Eigenschaften 1. bis 3. aus 22.16 heißt eine Norm auf $V$, $(V, \norm{·})$ ein normierter Vektorraum.
  Man kann zeigen: Ist $(V, \norm{·})$ ein normierter Vektorraum, in dem die Parallelogrammgleichung gilt, dann	ist durch
  \[γ(v, w) := \frac{1}{2}(\norm{v + w}^2 - \norm{v}^2 - \norm{w}^2)\]
  ein Skalarprodukt auf $V$ mit $\norm{v} = \sqrt{γ(v, v)}$, das heißt in diesen Fällen ist $(V, γ)$ ein euklidischer Vektorraum, dessen Norm mit die gegebenen übereinstimmt.
  #+end_note
* Die orthogonale Gruppe
  #+begin_defn latex
  $(V, γ_V), (W, γ_W)$ Euklidische Räume, $φ: V \to W$ lineare Abbildung. $φ$ heißt *orthogonal* $\xLeftrightarrow{\text{Def}}$ $φ$ ist ein Homomorphismus quadratischer Räume, das heißt
  \[γ_W(φ(v_1), φ(v_2)) = γ_V(v_1, v_2) ∀ v_1, v_2 ∈ V\]
  #+end_defn
  #+begin_remark latex
  $(V, γ_V), (W, γ_W)$ Euklidische Räume, $φ: V \to W$ orthogonale Abbildung. Dann gilt:
  1. $\norm{φ(v)}_W = \norm{v}_V ∀ v ∈ V$
  2. $v_1 \perp v_2 ⇔ φ(v_1) \perp φ(v_2) ∀ v_1, v_2 ∈ V$
  3. $φ$ ist injektiv
  #+end_remark
  #+begin_proof latex
  1. $\norm{φ(v)}_W^2 = γ_W(φ(v), φ(v)) = γ_V(v, v) = \norm{v}_V^2$
  2. $v_1 \perp v_2 ⇔ γ_V(v_1, v_2) = 0 ⇔ γ_W(φ(v_1), φ(v_2)) = 0 ⇔ φ(v_1) \perp φ(v_2)$
  3. Sei $v ∈ V$ mit $φ(v) = 0 ⇒ \norm{φ(v)}_W = 0 ⇒ \norm{v}_V = 0 ⇒ v = 0$
  #+end_proof
  #+begin_remark latex
  $(V, γ)$ Euklidischer Raum, $n = \dim V, \mathcal{B}$ Orthogonalbasis von $(V, γ)$. Dann ist das Koordinatensystem $Φ_{\mathcal{B}}: (ℝ^n, <·,·>) \to (V, γ)$ ein
  orthogonaler Isomorphismus.
  #+end_remark
  #+begin_proof latex
  $Φ_{\mathcal{B}}$ Isomorphismus: klar. $Φ_{\mathcal{B}}$ orthogonal, denn: Sei $\mathcal{B} = (v_1, \dots, v_n)$ dann ist
  \[γ(Φ_{\mathcal{B}}(e_i), Φ_{\mathcal{B}}(e_j)) = γ(v_1, v_j) = δ_{ij} = <e_i, e_j>\]
  #+end_proof
  #+begin_remark latex
  $(V, γ)$ Euklidischer Raum, $φ ∈ \End(V)$ orthogonal. Dann gilt:
  1. $φ$ ist Isomorphismus
  2. $φ^{-1}$ ist orthogonal
  3. $λ ∈ ℝ$ Eigenwert von $γ ⇒ \abs{λ} = 1$, das heißt $λ ∈ \{\pm 1\}$
  #+end_remark
  #+begin_proof latex
  1. aus 23.2.3 folgt: $φ$ injektiv $⇒$ $φ$ Isomorphismus
  2. $v_1, v_2 ∈ V ⇒ γ(φ^{-1}(v_1), φ^{-1}(v_2)) = γ(φ(φ^{-1}(v_1)), φ(φ^{-1}(v_2))) = γ(v_1, v_2)$ $⇒$ $φ^{-1}$ orthogonal
  3. Sei $v ∈ V$ Eigenvektor zum Eigenwert $λ ⇒ \norm{v} = \norm{φ(v)} = \norm{λv} = \abs{λ} \norm{v} ⇒ \abs{λ} = 1$
  #+end_proof
  #+begin_remark latex
  $(V, γ)$ Euklidischer Raum, $n = \dim V, \mathcal{B}$ Orthogonalbasis von $V, φ ∈ \End(V), A = M_{\mathcal{B}}(φ)$. Dann sind äquivalent:
  1. $φ$ ist orthogonal
  2. $A^T A = E_n$
  #+end_remark
  #+begin_proof latex
  Wir erhalten kommutierendes Diagramm
  #+begin_export latex
  \catcode`(=12
  \catcode`)=12
  #+end_export
  \begin{figure}[H]
	 \centering
  \begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=3em] {(V, γ) & (V, γ) \\ (ℝ^n,<·,·>) & (ℝ^n, <·,·>)\\};
  \path[-stealth]
  (m-1-1) edge node [left] {$φ$} (m-2-1)
  (m-1-2) edge node [below] {$Φ_{\mathcal{B}}$} (m-1-1)
  (m-2-2) edge node [below] {$Φ_{\mathcal{B}}$} (m-2-1)
  (m-1-2) edge node [left] {$φ$} (m-2-2);
  \end{tikzpicture}
  \end{figure}
  #+begin_export latex
  \catcode`(=\active
  \catcode`)=\active
  #+end_export
  Da $Φ_{\mathcal{B}}$ orthogonaler Isomorphismus nach 23.3 folgt:
  \begin{align*}
  φ \text{ orthogonal } &⇔ \tilde A = Φ_{\mathcal{B}}^{-1} = φ\circ Φ_{\mathcal{B}} \text{ orthogonal} \\
  &⇔ ∀ x, y ∈ ℝ^n: <Ax, Ay> = <x, y> \\
  &⇔ ∀ x, y ∈ ℝ^n: (Ax)^T Ay = x^T y \\
  &⇔ ∀ x, y ∈ ℝ^n: <Ax, Ay> = x^T A^T A y = x^T y \\
  &⇔ Δ(A^T A) = Δ(E_n) \\
  &⇔ A^T A = E_n
  \end{align*}
  #+end_proof
  #+begin_remdef latex
  $A$ heißt *orthogonal* $\xLeftrightarrow{\text{Def}} A^T A = E_n$
  \[O(n) :=\{A ∈ M(n × n, ℝ) \mid A \text{ ist orthogonal }\}\]
  $O(n)$ ist bezüglich die Matrixmultiplikation eine Gruppe, die *orthogonale Gruppe* vom Rang $n$
  #+end_remdef
  #+begin_proof latex
  Wohldefiniertheit von "$·$" (das heißt Abgeschlossenheit bezüglich "$·$"): $A, B ∈ O(n) ⇒ (AB)^T AB = B^T A^T A B = B^T B = E_n ⇒ AB ∈ O(n)$. \\
  Existenz des neutralen Elements: $E_n ∈ O(n)$ \\
  Assoziativität: klar \\
  Existenz von Inversen: Sei $A ∈ A(n) ⇒ A^T A = E_n ⇒ A^{-1} = A^t ⇒ (A^{-1})^T A^{-1} = (A^T)^T A^T = A A^T = A A^{-1} = E_n$
  #+end_proof
  #+begin_note latex
  $A ∈ O(n) ⇒ \det(A) ∈ \{\pm 1\}$, denn $1 = \det(E_n) = \det(A^T A) = \det(A^T)\det(A) = \det(A)^2$
  #+end_note
  #+begin_remark latex
  $A ∈ M(n × n, ℝ)$. Dann sind äquivalent:
  1. $A ∈ O(n)$
  2. $A A^T = E_n$
  3. $A^T A = E_n$
  4. Die Transponierten der Zeilen von $A$ bilden eine Orthogonalbasis von $(ℝ^n, <·,·>)$
  5. Die Spalten von $A$ bilden eine Orthogonalbasis von $(ℝ^n, <·,·>)$
  6. Die Abbildung $\tilde A: (ℝ^n, <·,·>) \to (ℝ^n, <·,·>)$ ist orthogonal
  #+end_remark
  #+begin_proof latex
  1. $⇔$ 2. $⇔$ 3. $⇔$ klar
  2. $⇔$ 4., 3. $⇔$ 5.
  1. [@1] $⇔$ 6. aus 23.5 (setze $V = (ℝ^n, <·, ·>), \mathcal{B} = (e_1, \dots, e_n)$)
  #+end_proof
  #+begin_thm latex
  $φ: ℝ^n \to ℝ^n$ (nicht notwendig linear) abstandstreu, das heißt
  \[\norm{φ(x) - φ(y)} = \norm{x - y} ∀ x, y ∈ ℝ^n\]
  wobie $\norm{·}$ die Norm auf $(ℝ^n, <·,·>)$ bezeichne. Dann existieren eindeutig bestimmte $A ∈ O(n), b ∈ ℝ^n$, sodass
  \[φ(x) = Ax + b\]
  für alle $x ∈ ℝ^n$
  #+end_thm
  #+begin_remdef latex
  $SO(n) := \{A ∈ O(n) \mid \det A = 1\}$ ist eine Untergruppe von $O(n)$ (das heißt $SO(n) ⊆ O(n)$ und ist eine Gruppe bezüglich der eingeschränkten Verknüpfung),
  die *spezielle orthogonale Gruppe* vom Rang $n$.
  #+end_remdef
  #+begin_proof latex
  Wohldefiniertheit von "$·$" (= Abgeschlossenheit bezüglich "$·$")
  \[A, B ∈ SO(n) ⇒ AB ∈ O(n) ∧ \det(AB) = \det(A)\det(B) = 1·1 = 1\]
  neutrales Element: $E_n ∈ SO(n)$ \\
  Assoziativität: klar \\
  Existenz von Inversem: $A ∈ SO(n) ⇒ A^{-1} ∈ O(n), \det(A^{-1}) = \det(A)^{-1} = 1 ⇒ A^{-1} ∈ SO(n)$
  #+end_proof
  #+begin_ex latex
  $n = 1: O(1) = \{\pm 1\}, SO(1) = \{0\}$
  #+end_ex
  #+begin_remark latex
  $A ∈ O(2)$. Dann gilt:
  1. $A ∈ SO(2) ⇔ ∃! α ∈ [0, 2π]$ mit
	 \[A = \begin{pmatrix}\cos α & -\sin α \\ \sin α & \cos α \end{pmatrix}\]
	 In diesem Fall beschreibt $A$ eine Drehung mit Zentrum $0$ um den Winkel $α$. Außer im Fall $α ∈ \{0, π\}$ besitzt $A$ keine Eigenwerte. Falls $α = 0$:
	 \[A = \begin{pmatrix}1 & 0 \\ 0 & 1\end{pmatrix}\]
	 einziger Eigenwert: $1$. Falls $α = π$:
	 \[A = \begin{pmatrix}-1 & 0 \\ 0 & -1\end{pmatrix}\]
	 einziger Eigenwert: $-1$.
  2. $A ∈ O(2) \setminus SO(2) ⇔ ∃! α ∈ [0, 2π]$ mit
	 \[A = \begin{pmatrix}\cos α & \sin α \\ \sin α & -\cos α\end{pmatrix}\]
	 In diesem Fall beschreibt $A$ eine Spiegelung an der Geraden $\Lin(\cvec{\cos \frac{α}{2}; \sin \frac{α}{2}})$. $A$ besitzt die Eigenwerte $\pm 1$, und es existiert eine Orthogonalbasis $\mathcal{B}$ von
	 $(ℝ^2, <·,·>)$ mit
	 \[M_{\mathcal{B}}(\tilde A) = \begin{pmatrix}1 & 0 \\ 0 & -1\end{pmatrix}\]
  #+end_remark
  #+begin_proof latex
  Sei $A = \begin{pmatrix}a & c \\ b & d\end{pmatrix} ∈ O(2)$
  \begin{align*}
  ⇒ 1 &= \norm{e_1}^2 = \norm{Ae_1}^2 = a^2 + b^2 \\
  ⇒ 1 &= \norm{e_2}^2 = \norm{Ae_2}^2 = c^2 + d^2 \\
  \end{align*}
  Außerdem: $e_1 \perp e_2 ⇒ A e_1 \perp A e_2$
  \[⇒ <\cvec{a;b}, \cvec{c; d}> = 0\]
  \[⇒ \begin{pmatrix}a & b\end{pmatrix} \begin{pmatrix}c \\ d\end{pmatrix} = 0 ⇒ \cvec{c; d} ∈ \Lin((\cvec{-b; a}))\]
  das heißt es existiert $λ ∈ ℝ$ mit
  \[\begin{pmatrix}c \\ d\end{pmatrix} = λ \begin{pmatrix}-b \\ a\end{pmatrix}\]
  \[⇒ A = \begin{pmatrix}a & -λ b \\ b & λa\end{pmatrix}, \det A = λ(a^2 + b^2) = λ ∈ \{\pm 1\}\]
  1. Fall: $λ = 1 ⇔ \det A = 1 ⇔ A ∈ SO(2)$
	 Wegen $a^2 + b^2 = 1$ ist $\cvec{a; b}$ ein Punkt auf dem Einheitskreis. $⇒ ∃! α ∈ [0, 2π\string)$ mit $a = \cos{α}, b = \sin{α}$.
	 Somit:
     \[A ∈ SO(2) ⇔ A = \begin{pmatrix}\cos α & -\sin α \\ \sin α & \cos α\end{pmatrix}\]
	 für eindeutig bestimmte $α ∈ [0, 2π\string)$. Sei $\cvec{x_1; x_2} = \cvec{\cos β; \sin β}$ ein Punkt auf dem Einheitskreis
	 \[A \cvec{\cos β; \sin β} = \begin{pmatrix}\cos α & -\sin α \\ \sin α & \cos α\end{pmatrix}\cvec{\cos β; \sin β} = \begin{pmatrix}\cos α \cos β - \sin α \sin β \\ \sin α \cos β + \cos α \sin β\end{pmatrix} = \cvec{\cos{α + β}; \sin{α + β}}\]
	 $⇒ A$ beschreibt eine Drehung mit Zentrum $0$ um den Winkel $α$. $A$ hat nur Eigenwerte, wenn $α = 0$ beziehungsweise $α = π$ (Eigenwert: $1$ beziehungsweise $-1$):
	 \[χ_A^{char} = t^2 - \Sp(A)t + \det A = t^2 - 2\cos α + 1\]
	 Eigenwerte: $λ_{1,2} = \cos α \pm \sqrt{\cos^2 α - 1}$, Eigenwert in $ℝ ⇔ \cos^2 α - 1 \geq 0 ⇔ α = 1$ oder $α = π$
  2. $λ= -1 ⇔ A ∈ O(2) \setminus SO(2)$:
	 \[⇔ A = \begin{pmatrix}a & b \\ b & -a\end{pmatrix}\]
	 Wegen $a^2 + b^2 = 1$ existiert genau ein $α ∈ [0, 2π\string)$ mit $a = \cos{α}, b = \sin{α}$. Sei $\cvec{x_1; x_2} = \cvec{\cos β; \sin β}$ ein Punkt auf dem Einheitskreis.
	 \[⇒ A \cvec{\cos β; \sin β} = \begin{pmatrix}\cos α & \sin α \\ \sin α & -\cos α\end{pmatrix} \cvec{\cos β; \sin β} = \cvec{\cos α \cos β + \sin α \sin β; \sin α \cos β - \cos α \sin β} = \cvec{\cos (α - b),  \sin{α - B}}\]
	 \[⇒ A \cvec{\cos(\frac{α}{2} + β); \sin(\frac{α}{2} + β)} = \cvec{\cos(\frac{α}{2} - β); \sin(\frac{α}{2} - β)}\]
	 $⇒ A$ beschreibt Spiegelung an der Geraden $\Lin(\cvec{\cos \frac{α}{2}; \sin \frac{α}{2}})$
	 \[χ_A^{char} = t^2 - \Sp(A) t + \det A = t^2 - 1 = (t + 1)(t - 1)\]
	 $⇒$ A diagonalisierbar und hat Eigenwert $\pm 1$. Sei $v_1$ Eigenvektor von $A$ zum Eigenwert $1$ mit $\norm{v_1} = 1, v_2$ Eigenvektor von $A$ zum Eigenwert $-1$ mit $\norm{v_2} = 1$
	 \[⇒ <v_1, v_2> = <A v_1, A v_2> = <v_1, -v_2> = -<v_1, v_2> ⇒ <v_1, v_2> = 0 ⇔ v_1 \perp v_2\]
	 Bezüglich der Orthogonalbasis $(v_1, v_2)$ des $(ℝ^2, <·, ·>)$ ist $M_{\mathcal{B}}(\tilde A) = \begin{pmatrix}1 & 0 \\ 0 & -1\end{pmatrix}$
  #+end_proof
  #+begin_conc latex
  $φ: (ℝ^2, <·, ·>) \to (ℝ^2, <·, ·>)$ orthogonale Abbildung. Dann existiert eine Orthogonalbasis $\mathcal{B}$ von $(ℝ^2, <·, ·>)$, sodass
  \[M_{\mathcal{B}}(φ) = \begin{pmatrix}\pm 1 & 0 \\ 0 & \pm 1\end{pmatrix} \text{ oder } M_{\mathcal{B}}(φ) = \begin{pmatrix}\cos α & -\sin α \\ \sin α & \cos α\end{pmatrix}, α ∈ (0, π)\]
  Die Anzahl der $\pm 1$ sowie $α$ sind unabhängig von der Wahl einer solchen Orthogonalbasis $\mathcal{B}$ (das heißt sind Invarianten von $φ$).
  #+end_conc
  #+begin_proof latex
  Existenz von $\mathcal{B}$: Sei $\mathcal{C} = (e_1, e_2), A := M_{\mathcal{C}}(φ)$, insbesondere $A ∈ O(2)$.
  1. Fall: $A ∈ SO(2) ⇒ ∃ β ∈ (0, 2π), β \neq π$ mit
	 \[A = \begin{pmatrix}\cos β & -\sin β \\ \sin β & \cos β\end{pmatrix} \text{ oder } A = \begin{pmatrix}1 & 0 \\ 0 & 1\end{pmatrix} \text{ oder } A = \begin{pmatrix}-1 & 0 \\ 0 & -1\end{pmatrix}\]
	 Falls $β ∈ (0, π)$, setze $α := β, \mathcal{B} = \mathcal{C}$. \\
	 Falls $β ∈ (π, 2π)$
	 \[⇒ M_{(e_2, e_1)}(φ) = \begin{pmatrix}\cos β & \sin β \\ -\sin β & \cos β\end{pmatrix}\]
	 Setze $α := 2π - B, \mathcal{B} := (e_2, e_1) ⇒ β = 2π - α ⇒ \cos β = \cos α, \sin β = - \sin β$
	 \[⇒ M_{\mathcal{B}}(φ) = \begin{pmatrix}\cos α & -\sin α \\ \sin α & \cos α\end{pmatrix}\]
  2. $A ∈ O(2) \setminus SO(2) ⇒ ∃$ Orthogonalbasis $\mathcal{B}$ von $(ℝ^2, <·, ·>)$ mit $M_{\mathcal{B}}(φ) = \begin{pmatrix}1 & 0 \\ 0 & -1\end{pmatrix}$.
  Eindeutigkeit: Falls $M_{\mathcal{B}}(φ) = \begin{pmatrix}\pm 1 & 0 \\ 0 & \pm -1\end{pmatrix}$, dann Anzahl der $\pm 1 = μ_{alg}$ der Eigenwirte $\pm 1$.
  Falls $M_{\mathcal{B}}(φ) = \begin{pmatrix}\cos α & -\sin α \\ \sin α & \cos α\end{pmatrix}$, dann $χ_φ^{char} = t^2 - 2\cos α t + 1 ⇒ \cos α$ ist
  unabhängig von der Wahl der Basis $\mathcal{B}$. Wegen $α ∈ (0, π)$ ist $α$ unabhängig von $\mathcal{B}$.
  #+end_proof
  #+begin_note latex
  Verallgemeinerung von 23.12 auf $(ℝ^n, <·,·>)$ ist möglich.
  #+end_note
* Der Spektralsatz
  In diesem Abschnitt sei $(V, γ)$ stets ein Euklidischer Raum.
  #+begin_remark latex
  Die Abbildung $Γ:V \to V^{\ast}, w ↦ γ(·, w)$ ist ein Isomorphismus.
  #+end_remark
  #+begin_proof latex
  $γ$ nicht ausgeartet nach 22.6 $⇒ γ$ perfekt, das heißt $Γ$ Isomorphismus.
  #+end_proof
  #+begin_note latex
  Insbesondere ist für einen Euklidischen Vektorraum $(V, γ)$ die Vektorräume $V$ und $V^{\ast}$ kanonisch isomorph.
  #+end_note
  #+begin_remark latex
  $\mathcal{B} = (v_1, \dots, v_n)$ Orthonormalbasis von $(V, γ), \mathcal{B}^{\ast} = (v_1^{\ast}, \dots, v_n^{\ast})$ duale Basis zu $\mathcal{B}$,
  $U ⊆ V$ Untervektorraum, $Γ:V\to V^{\ast}$ kanonische Abbildung aus 24.1. Dass gilt:
  1. $Γ(U^{\perp}) = U^0$
  2. $Γ(v_i) = v_i^{\ast}, i = 1, \dots, n$
  #+end_remark
  #+begin_proof latex
  1. $Γ(U^{\perp}) ⊆ U^0$, denn: Für $v ∈ U^{\perp}, u ∈ U$ ist $(Γ(v))(w) = γ(u, v) = 0 ⇒ Γ(U^{\perp}) ⊆ U^0$.
     \[\dim Γ(U^{\perp}) = \dim U^{\perp} = \dim V - \dim U = \dim U^0\]
  2. Es ist $Γ(v_i)(v_j) = γ(v_j, v_i) = δ_{ij} = v_i^{\ast}(v_j), j = 1, \dots, n$, das heißt $Γ(v_i) = v_i^{\ast}$
  #+end_proof
  #+begin_remdef latex
  $(V, γ_V), (W, γ_W)$ Euklidische Räume, $φ: V\to W$. Dass existiert genau eine lineare Abbildung $φ^{ad}: W \to V$ mit
  \[γ_W(φ(v), w) = γ_V(v, φ^{ad}(w)) ∀ v ∈ V, w ∈ W\]
  $φ^{ad}$ heißt die zu $φ$ *adjungierte Abbildung*
  #+end_remdef latex
  #+begin_proof latex
  Existenz: Wir betrachten das Diagramm
  #+begin_export latex
  \catcode`(=12
  \catcode`)=12
  #+end_export
  \begin{figure}[H]
	 \centering
  \begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=3em] {V & W \\ V^{\ast} & W^{\ast}\\};
  \path[-stealth]
  (m-2-1) edge node [left] {$φ^{ad}$} (m-1-1)
     	  edge node [below] {$Γ_W$} (m-2-2)
  (m-1-1) edge node [below] {$Γ_V$} (m-1-2)
  (m-2-2) edge node [left] {$φ^{\ast}$} (m-1-2);
  \end{tikzpicture}
  \end{figure}
  #+begin_export latex
  \catcode`(=\active
  \catcode`)=\active
  #+end_export
  und setzen $φ^{ad} := Γ_V^{-1} \circ φ^{\ast} \circ Γ_W$, $φ^{ad}$ ist linear nach Konstruktion. Es gilt für $v ∈ V, w ∈ W$:
  \begin{align*}
  γ_W(φ(v), w) &= Γ_W(w)(φ(v)) = (Γ_W(w) \circ φ)(v) = φ^{\ast}(Γ_W(w))(v) \\
  &= ((φ^{\ast} \circ Γ_W)(w))(v) = ((Γ_V \circ φ^{ad})(w))(v) = Γ_V(φ^{ad}(w))(v) \\
  &= γ(v, φ^{ad}(w))
  \end{align*}
  Eindeutigkeit: Damit obige Gleichung für alle $v ∈ V, w ∈ W$ gilt, muss das Diagramm kommutieren, das heißt $Γ_V \circ φ^{ad} = φ^{\ast} \circ Γ_W$, also $φ^{ad} = Γ_V^{-1} \circ φ^{\ast} \circ Γ_W$.
  #+end_proof
  #+begin_note latex
  Ist $φ$ orthogonal, dann ist $φ^{ad} = φ^{-1}$, denn für $v, w ∈ V$
  \[γ(φ(v), w) = γ(φ(v), φ(φ^{-1}(w))) = γ(v, φ(w))\]
  #+end_note
  #+begin_remark latex
  $(V, γ_V), (W, γ_W)$ euklidische Räume, $\mathcal{A}$ Orthonormalbasis von $(V, γ_V), \mathcal{B}$ Orthonormalbasis von $(W, γ_W)$, $φ: V \to W$ lineare Abbildung. Dass gilt
  \[M_{\mathcal{A}}^{\mathcal{B}}(φ^{ad}) = (M_{\mathcal{B}}^{\mathcal{A}}(φ))^T\]
  Insbesondere ist $(φ^{ad})^{ad} = φ$
  #+end_remark
  #+begin_proof latex
  \begin{align*}
  M_{\mathcal{A}}^{\mathcal{B}}(φ^{ad}) &= M_{\mathcal{A}}^{\mathcal{B}}(Γ_V^{-1} \circ φ^{\ast} \circ Γ_W) = \underbrace{M_{\mathcal{A}}^{\mathcal{A}^{\ast}}(Γ_V^{-1})}_{E_{\dim V}} \underbrace{M_{\mathcal{A}^{\ast}}^{\mathcal{B}^{\ast}}}_{(M_{\mathcal{B}}^{\mathcal{A}}(φ))^T} \underbrace{M_{BB^{\ast}}^{\mathcal{B}}(Γ_W)}_{= E_{\dim W}} \\
  &= (M_{\mathcal{B}}^{\mathcal{A}}(φ))^T
  \end{align*}
  #+end_proof
  #+begin_thm latex
  $(V, γ_V), (W, γ_W)$ euklidische Räume, $φ: V \to W$ lineare Abbildung. Dann gilt:
  1. $\ker (φ^{ad}) = (\im φ)^{\perp}$
  2. $\im (φ^{ad}) = (\ker φ)^{\perp}$
  #+end_thm
  #+begin_proof latex
  1. $w ∈ (\im φ)^{\perp} ⇔ γ_W(φ(v), w) = 0 ∀ v ∈ V ⇔ γ_V(v, φ^{ad}(w)) = 0 ∀ v ∈ V$, $γ$ nicht ausgeartet $⇒ φ^{ad}(w) = 0 ⇔ w ∈ \ker(φ^{ad})$
  2. $(\im(φ^{ad}))^{\perp} = \ker{(φ^{ad})^{ad}} = \ker φ ⇔ (\ker φ)^{\perp} = (\im(φ^{ad})^{\perp})^{\perp} = \im{φ^{ad}}$
  #+end_proof
  #+begin_conc latex
  $φ ∈ \End(V)$. Dann gilt:
  \[V = \ker φ \hat\oplus \im φ^{ad} \quad \text{sowie} \quad V = \ker φ^{ad} \hat\oplus \im φ\]
  #+end_conc
  #+begin_proof latex
  Es ist
  \[V = (\ker φ) \hat\oplus (\ker φ)^{\perp} = \ker φ \hat\oplus \im φ^{ad}\]
  andere Gleichung analog.
  #+end_proof
  #+ATTR_LATEX: :options [Selbstadjungiert]
  #+begin_defn latex
  $φ ∈ \End(V)$ heißt *selbstadjungiert* $⇔ φ = φ^{ad}$
  #+end_defn
  #+begin_remark latex
  $\mathcal{B}$ Orthonormalbasis von $(V, γ)$. Dann sind äquivalent:
  1. $φ$ selbstadjungiert
  2. $M_{\mathcal{B}}(φ)$ symmetrisch
  In diesem Fall $V = \ker φ \hat\oplus \im φ$
  #+end_remark
  #+begin_proof latex
  $φ$ selbstadjungiert $⇔ φ = φ^{ad} ⇔ M_{\mathcal{B}}(φ) = M_{\mathcal{B}}{φ^{ad}} = (M_{\mathcal{B}}(φ))^T$. Nach 24.6 ist dann $V = \ker φ \hat\oplus \im φ^{ad} = \ker φ \hat \oplus \im φ$
  #+end_proof
  #+begin_thm latex
  Es gilt:
  1. $φ ∈ \End(V)$ selbstadjungiert $⇒ γ': V × V \to ℝ, γ'(x, y) = γ(φ(x), y)$ ist eine symmetrische
	 Bilinearform
  2. Ist $γ': V × V \to ℝ$ eine symmetrische Bilinearform, dann existiert genau ein selbstadjungierter Endormorphisums $φ ∈ \End(V)$ mit
	 $γ'(x, y) = γ(φ(x), y) ∀ x, y ∈ V$
  In diesem Fällen gilt bezüglich jeder Orthonormalbasis $\mathcal{B}$ von $(V, γ)$:
  \[M_{\mathcal{B}}(γ') = M_{\mathcal{B}}(φ)\]
  #+end_thm
  #+begin_proof latex
  1. $φ$ selbstadjungiert $⇒ γ'(x, y) = γ(φ(x), y) = γ(x, φ(y)) = γ(φ(y), x) = γ'(y, x)$, $γ'$ bilinear klar.
  2. Sei $γ': V × V \to ℝ$ symmetrische Bilinearform, $x ∈ V$ $⇒ ρ_x := γ'(x, ·): V \to ℝ, γ ↦ γ'(x, y)$ ist ein Element von $V^{\ast}$.
	 Nach 24.1 ist $Γ:V \to V^{\ast}, w ↦ γ(·, w)$ ein Isomorphismus $⇒$ Es existiert genau ein $z ∈ V$ mit $Γ(z) = ρ_x$, das heißt mit
	 \[γ(y, z) = Γ(z)(y) = ρ_x(y) = γ'(x, y) ∀ y ∈ V\]
	 Wir definieren $φ: V \to V, x ↦ k$ mit $Γ(z) = ρ_x$ $⇒$ Für alle $x, y ∈ V$ ist $γ(φ(x), y) = γ(y, φ(x)) = γ'(x, y)$. \\
	 $φ$ ist linear: Seien $x_1, x_2, y ∈ V, λ, μ ∈ ℝ$
	 \begin{align*}
	 ⇒ Γ(φ(λ x_1 + μx_2) - λφ(x_1) - μ φ(x_2))(y) = γ(y, φ(λ x_1 + μ x_2) - λ φ(x_1) - μ φ(x_2)) \\
	 &= γ(y, φ(υ x_1 + μ x_2)) - λγ(y, χ(x_1)) - μ γ(y, φ(x_2)) \\
	 &= γ'(λ x_1 + μ x_2, y) - λ γ'(x_1, y) - μ γ'(x_2, y) \\
	 \intertext{$γ'$ bilnear}
	 &= 0 \\
	 \intertext{Das gilt für alle $y ∈ V$}
  	 ⇒ Γ(φ(λ x_1 + μ x_2) - λ φ(x_1) - μ φ(x_2)) &= 0 \\
	 ⇒ φ(λ x_1 + μ x_2) = λ φ(x_1) + μ φ(x_2)
     \end{align*}
	 $φ$ selbstadjudgiert: Für $x, y ∈ V$ ist
	 \[γ(φ(x), y) = γ'(x, y) = γ'(y, x) = γ(φ(y), x) = γ(x, φ(y)) ⇒ φ = φ^{ad}\]
	 $φ$ ist eindeutig: Sei $\tilde φ$ selbstadjudgiert mit $γ'(x, y) = γ(φ(x), y) = γ(\tilde φ(x), y) ∀ x, y ∈ V$
	 \begin{align*}
	 ⇒ Γ(φ(x))(y) &= Γ(\tilde φ(x))(y) ∀ x, y ∈ V \\
	 ⇒ Γ(φ(x)) &= Γ(\tilde φ(x)) \\
  	 \intertext{$Γ$ Isomorphismus}
	 ⇒ φ(x) = \tilde φ(x) ∀ x ∈ V \\
	 ⇒ φ &= \tilde φ
     \end{align*}
	 Darstellungsmatrizen: Sei $\mathcal{B}= (v_1, \dots, v_n)$ Orthogonalbasis von $(V, γ)$. $A = M_{\mathcal{B}}(φ) = (a_{ij})$
	 \[⇒ γ'(v_i, v_j) = γ(φ(v_i), v_j) = γ(\sum_{k = 1}^{n} a_{ki} v_k, v_j) = a_{ji} \overset{φ \text{ selbstadjudgiert}}{=} a_{ij}\]
	 $⇒ M_{\mathcal{B}}(γ') = M_{\mathcal{B}}(γ)$
  #+end_proof
  #+begin_note latex
  Interpretation für $(ℝ^n, <·, ·>)$: Ist $A ∈ M(n × n, ℝ)$ symmetrisch, dann ist $A$
  - Darstellungsmatrix bezüglich $(e_1, \dots, e_n)$ des selbstadjungierten Endomorphismus $\tilde A$ von $ℝ^n$
  - Darstellungsmatrix bezügilch $(e_1, \dots, e_n)$ der symmetrischen Bilinearform $γ' = Δ(A): (x, y) ↦ x^t A y$
  Es ist $γ'(x, y) = x^t A y = x^t A^t y = (A x)^t y = <Ax, y> = <\tilde A(x), y> ∀ x, y ∈ ℝ^n$. Bezüglich jeder Orthogonalbasis von $(ℝ^n, <·,·>)$ gilt $M_{\mathcal{B}}(\tilde A) = M_{\mathcal{B}}(γ')$
  #+end_note
  #+begin_remark latex
  $φ ∈ \End(V)$ selbstadjungiert, $U ⊆ V$ Untervektorraum mit $φ(U) ⊆ U$. Dann gilt $φ(U^{\perp}) ⊆ U^{\perp}$
  #+end_remark
  #+begin_proof latex
  Sei $v ∈ U^{\perp} ⇒ ∀ u ∈ U: γ(u, φ(v)) = γ(\underbrace{φ(u)}_{∈ U}, \underbrace{v}_{∈ U^{\perp}}) = 0 ⇒ φ(v) ∈ U^{\perp}$
  #+end_proof
  #+begin_remark latex
  $φ ∈ \End(V)$ selbstadjungiert. Dann zerfällt $χ_φ^{char}$ über $ℝ$ in Linearfaktoren.
  #+end_remark
  #+begin_proof latex
  Sei $\mathcal{B}$ eine Orthonormalbasis von $(V, γ), A = M_{\mathcal{B}}(φ) ⇒ χ_φ^{char} = χ_A^{char}, A = A^T$ wegen $φ$ selbstadjungiert.
  Wir betrachet die $ℂ$ -lineare Abbildung $\tilde A_{ℂ}: ℂ^n \to ℂ^n, z ↦ Az$. Es ist
  \[χ_A^{char} = χ_{\tilde A_{ℂ}}^{char} = (t - λ_1) · \dots · (t - λ_n), λ_1, \dots, λ_n ∈ ℂ\]
  Behauptung: $λ_i ∈ ℝ ∀ i = 1, \dots, n$, denn: Sei $z = \cvec{z_1; \vdots; z_n} ∈ ℂ^n$ ein Eigenvektor zum Eigenwert $λ_i$ von $\tilde A_{ℂ}$.
  Wir setzen $\bar z := \cvec{\bar z_1; \vdots; \bar z_n}$ und erhalten
  \[λ_i z^T \bar z = (λ_i z)^T \bar z = (A z)^T \bar z = z^T A^T \bar z = z^T A \bar z = z^T \overline{A z} = z^T \overline{λ_i z} = \bar λ_i z^T \bar z\]
  Es ist $z^T \bar z = (z_1, \dots, z_n) \cvec{\bar z_1; \vdots; \bar z_n} = z_1 \bar z_1 + \dots + z_n \bar z_n = \abs{z_1}^2 + \dots + \abs{z_n}^2 \neq 0$
  $⇒ λ_i = \bar λ_i ⇒ λ_i ∈ ℝ$
  #+end_proof
  #+ATTR_LATEX: :options [Spektralsetz für selbstadjungierte Endomorphismen]
  #+begin_thm latex
  $φ ∈ \End(V)$ selbstadjungierter Enddmorphismus. Dann existiert eine Orthonormalbasis von $(V, γ)$ aus Eigenvektoren von $φ$.
  Sind $λ_1, \dots, λ_r$ die verschiedenen Eigenwerte von $φ$, so ist
  \[V = Eig(φ, λ_1) \hat\oplus \dots \hat\oplus \Eig(φ, λ_r)\]
  #+end_thm
  #+begin_proof latex
  per Induktion nach $n = \dim V$. \\
  Induktionsanfang: $n = 0$: trivial \\
  Induktionsschritt: Sei $n \geq 1$. Nach 24.11 existiert ein Eigenwert $λ$ von $φ$ und es sei $w_1$
  ein Eigenvektor von $φ$ zum Eigenwert $λ$. Setze
  \[v_i := \frac{w_1}{\norm{w_1}}, U := \Lin((v_i)) ⇒ φ(U) ⊆ U ⇒ φ(U^{\perp} ⊆ U^{\perp})\]
  Wir setzen $ψ := φ \big|_{U^{\perp}}^{U^{\perp}}:U^{\perp} \to U^{\perp}$. $ψ$ ist selbstadjungiert, denn:
  Für alle $x, y ∈ U^{\perp}$ ist
  \[γ(ψ(x), y) = γ(φ(x), y) = γ(x, φ(y)) = γ(x, ψ(y))\]
  Nach 22.9 ist $V = U \hat\oplus U^{\perp}, \dim U^{\perp} = \dim V - \dim U = n - 1$.
  Nach Induktionsvorrausetzung existiert eine Orthonormalbasis von $(v_2, \dots, v_n)$ von $U^{\perp}$
  aus Eigenvektoren von $φ ⇒ (v_1, \dots, v_n)$ ist von Orthonormalbasis $(V, γ)$ aus Eigenvektoren von $φ$
  $⇒ V = \Eig(φ, λ_1) \hat\oplus \dots \hat\oplus \Eig(φ, λ_r)$
  #+end_proof
  #+begin_conc latex
  $γ':V × V: ℝ$ symmetrische Bilinearform, $n = \dim V$. Dann existiert eine Orthonormalbasis $\mathcal{B}$ von $(V, γ)$ bezüglich derer die Darstellungsmatrix von $γ'$ Diagonalbestalt hat:
  \[M_{\mathcal{B}}(γ') = \begin{pmatrix}λ_1 &   & 0 \\   & \ddots &   \\ 0 &   & λ_n\end{pmatrix}\]
  Hierbei sind $λ_i, \dots, λ_n$ die Eigenvektoren (mit Vielfachen) des zu $γ'$ gehörenden eindeutig bestimmten selbstadjungierten Endomorphismus $φ ∈ \End(V)$ mit $γ'(x, y) = γ(φ(x), y)$
  #+end_conc
  #+begin_proof latex
  Sei $φ ∈ \End(V)$ der entsprechende Endomorphismus von $V$ nach 24.9. Spektralsatz $⇒$ Es existiert eine Orthonormalbasis $\mathcal{B}$ von $(V, γ)$ aus Eigenvektoren von $φ$ zu Eigenwerten $λ_1, \dots, λ_n$
  (nicht notwendig verschieden)
  \[⇒ M_{\mathcal{B}}(γ') \overset{24.9}{=} M_{\mathcal{B}}(φ) = \begin{pmatrix}λ_1 &   & 0 \\   & \ddots &   \\ 0 &   & λ_n\end{pmatrix}\]
  #+end_proof
  #+begin_conc latex
  $A ∈ M(n × n, ℝ)$ symmetrisch. Dann existiert ein $T ∈ O(n)$, sodass
  \[T^{-1} A T = \begin{pmatrix}λ_1 &   & 0 \\   & \ddots &   \\ 0 & & λ_n\end{pmatrix}\]
  Hierbei sind $λ_i, \dots, λ_n$ die Eigenwerte (mit Vielfachheit) von $A$. Die Spalten von $T$ bilden eine Orthonormalbasas von $(ℝ^n, <·,·>)$ aus Eigenvektoren von $A$.
  #+end_conc
  #+begin_proof latex
  $\tilde A: ℝ^n \to ℝ^n$ ist selbstadjungierter Endomorphismus von $(ℝ^n, <·, ·>)$. Spektralsatz $⇒$ es existiert eine Orthonormalbasis $\mathcal{B}$ aus Eigenvektoren von $A$ des $(ℝ^n, <·, ·>)$ mit
  \[M_{\mathcal{B}}(\tilde A) = \begin{pmatrix}λ_1 &   & 0 \\   & \ddots &   \\ 0 &   & λ_n\end{pmatrix}\]
  Es ist
  \[M_{\mathcal{B}}(\tilde A) = \underbrace{(T_{(e_1, \dots, e_n)}^{\mathcal{B}})^{-1}}_{= T^{-1}} \underbrace{M_{(e_1, \dots, e_n)}^{(e_1, \dots, e_n)}(\tilde A)}_{A} \underbrace{T_{(e_1, \dots, e_n)}^{\mathcal{B}}}_{=: T}\]
  Es ist $T ∈ O(n)$, da $\mathcal{B}$ Orthogonalbasis von $(ℝ^n, <·,·>)$ (vergleiche 23.7)
  #+end_proof
  #+begin_note latex
  Man kann sogar stets $T ∈ SO(n)$ erreichen (indem man gegebenfalls eine Spalte $v_i$ von $T$ durch $- v_i$ ersetzt.)
  #+end_note
  #+ATTR_LATEX: :options [Hauptachsentransformation]
  #+begin_algorithm latex
  Eingabe: $A ∈ M(n × n, ℝ)$ symmetrisch \\
  Ausgabe: $T ∈ O(n)$, sodass $T^{-1} A T$ Diagonalmatrix \\
  Durchführung:
  1. Bestimme $χ_A^{char} ∈ ℝ[t]$ sowie eine Zerlegung
	 \[χ_A^{char} = (t - λ_1)^{T_1} · \dots · (t - λ_k)^{T_A}\]
	 mit $λ_1, \dots, λ_k$ paarweise verschieden
  2. Bestimme für $i = 1, \dots, k$ jeweils eine Basis von $\Eig(φ, λ_i)$
  3. Bestimme mit dem Gram-Schmidt-Verfahren für $i = 1, \dots, k$ eine Orthonormalbasis $\mathcal{B}_i = (v_{i,1}, \dots, v_{i, r_i})$ von $\Eig(φ, λ_i)$
  4. Die Orthogonalbasis $\mathcal{B}_i, i = 1, \dots, k$ bilden zusammen eine Orthonormalbasis
	 \[\mathcal{B} = (v_{1,1}, \dots, v_{1, r_1}, \dots, v_{k,1}, \dots, v_{k, r_k})\]
	 des $(ℝ^n, <·,·>)$ aus Eigenvektoren von $A$
  5. Schreibe die Basisvektoren aus $\mathcal{B}$ in Spalten von $T$. Es ist dann
	 \[T^{-1} A  T = (λ_1, \dots, λ_1, \dots, λ_k, \dots, λ_k) E_n\]
  #+end_algorithm
  #+begin_note latex
  Um $T ∈ SO(n)$ zu erreichen ersetze gegebenfalls $v_{1,1}$ durch $-v_{1,1}$.
  #+end_note
  #+begin_ex latex
  \[A = \begin{pmatrix}2 & -1 & 2 \\ -1 & 2 & 2 \\ 2 & 2 & -1\end{pmatrix} ∈ M(3 × 3, ℝ)\]
  Es ist $χ_A^{char} = t^3 - 3t^2 - 9 t + 27 = (t - 3)^2(t + 3)$. Es ist $\Eig(A, 3) = \dots = \Lin(\cvec{2; 0; 1}, \cvec{-1; 1; 0})$.
  Nach Beispiel 22.12 ist $(\frac{1}{\sqrt{5}} \cvec{2; 0; 1}, \frac{1}{\sqrt{30}} \cvec{-1; 5; 2})$ eine Orthonormalbasis von $\Eig(A, 3)$. \\
  $\Eig(A, -3) = \Lin(\cvec{1; 1; -2}) ⇒ (\frac{1}{\sqrt{6}} \cvec{1; 1; -2})$ ist Orthonormalbasis von $\Eig(A, -2)$.
  \[⇒ (\frac{1}{\sqrt{5}} \cvec{2; 0; 1}, \frac{1}{\sqrt{30}} \cvec{-1; 5; 2}, \frac{1}{\sqrt{6}} \cvec{1; 1; -2})\]
  ist Orthonormalbasis von $(ℝ^3, <·,_>)$ aus Eigenvektoren von $A$. Mit
  \[T = \begin{pmatrix}\frac{2}{\sqrt{5}} & - \frac{1}{\sqrt{30}} & \frac{1}{\sqrt{6}} \\ 0 & \frac{5}{\sqrt{30}} & \frac{1}{\sqrt{6}} \\ \frac{1}{\sqrt{5}} & \frac{2}{\sqrt{30}} & - \frac{2}{\sqrt{6}}\end{pmatrix} \quad \text{ist}\quad T^{-1} A T = \begin{pmatrix}3 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & -3\end{pmatrix}\]
  Es ist $\det(T) = -1$, also $T ∈ O(3) \setminus(3)$. Setzt man
  \[T' := \begin{pmatrix}-\frac{2}{\sqrt{5}} & - \frac{1}{\sqrt{30}} & \frac{1}{\sqrt{6}} \\ 0 & \frac{5}{\sqrt{30}} & \frac{1}{\sqrt{6}} \\ -\frac{1}{\sqrt{5}} & \frac{2}{\sqrt{30}} & - \frac{2}{\sqrt{6}}\end{pmatrix} \quad \text{ist}\quad T^{-1} A T = \begin{pmatrix}3 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & -3\end{pmatrix}\]
  und es ist $T' ∈ SO(3)$.
  #+end_ex
* Unitäre Räume
  #+ATTR_LATEX: :options [Sesquilinearform]
  #+begin_defn latex
  $V$ $ℂ$ Vektorraum, $h: V × V \to ℂ, (v, w) ↦ h(v, w)$ heißt eine *Sesquilinearform* auf $V$ genau dann wenn folgende Bedingungen erfüllt sind:
  - (S1) $h(v_1 + v_2, w) = h(v_1, w) + h(v_2, w), h(λ v, w) = λ(h(v, w))$
  - (S2) $h(v, w_1 + w_2) = h(v, w_1) + h(v, w_2), h(v, λ w) = \bar λ h(v, w)$
  für alle $v_1, v_2, w_1, w_2, v, w ∈ V, λ ∈ ℂ$
  #+end_defn
  #+begin_ex latex
  $h: ℂ^n × ℂ^n \to ℂ, h(x, y) := x^t \bar y$ ist eine Sesquilinearform auf $ℂ^n$ (beachte $h(x, λ y) = x^t \overline{λ y} = \bar λ x^t y$), aber keine Bilinearform auf $ℂ*n$
  #+end_ex
  #+begin_remark latex
  $V$ $ℂ$ Vektorraum, $h: V × V \to ℂ$ Sesquilinearform auf $V$. Dann induziert $h$ eine "semilineare" Abbildung
  \[Γ: V \to V^{\ast}, w ↦ h(·, w)\]
  das heißt $Γ(w_1 + w_2) = Γ(w_1) + Γ(w_2), Γ(λ w) = \bar λ Γ(w) ∀ w_1, w_2, w ∈ V, λ ∈ ℂ$
  #+end_remark
  #+ATTR_LATEX: :options [Darstellungsmatrix / Fundamentalmatrix]
  #+begin_defn latex
  $V$ endlichdimensional, $ℂ$ Vektorraum, $h$ Sesquilinearform auf $V$, $\mathcal{B} = (v_1, \dots, v_n)$ Basis von $V$
  \[M_{\mathcal{B}}(h) = (h(v_i, v_j))_{\substack{1 \leq i \leq n \\ 1 \leq j \leq n}}\]
  heißt die *Darstellungsmatrix* (*Fundamentalmatrix*) von $h$ bezüglich $\mathcal{B}$
  #+end_defn
  #+begin_remark latex
  $V$ endlichdimensionaler $ℂ$ Vektorraum, $\mathcal{B} = (v_1, \dots, v_n)$ Basis von $V$.
  \[\Sesq(V) := \{h: V × V \to ℂ \mid h \text{ ist eine Sesquilinearform}\}\]
  ist ein $ℂ$ Vektorraum und Untervektorraum von $\Abb(V × V, ℂ)$.
  Dann gilt: Die Abbildun $M_{\mathcal{B}} \to M(n × n, ℂ), h ↦ M_{\mathcal{B}}(h)$ ist ein Isomorphismus von $ℂ$ Vektorräumen mit Umkehrabbildung $Δ^{\mathcal{B}}: M(n × n, ℂ) \to \Sesq(V)$ mit
  \[Δ^{\mathcal{B}}(A)(v, w) = Φ_{\mathcal{B}}^{-1}(v)^T A \overline{Φ_{\mathcal{B}}^{-1}(w)}\]
  #+end_remark
  #+begin_thm latex
  $V$ endlichdimensionaler $ℂ$ Vektorraum, $\mathcal{A}, \mathcal{B}$ Basin von $V, h$ Sesquilinearform auf $V$. Dann gilt:
  \[M_{\mathcal{B}}(h) = (T_{\mathcal{A}}^{\mathcal{B}})^T M_{\mathcal{A}}(h) \overline{T_{\mathcal{B}}^{\mathcal{A}}}\]
  #+end_thm
  #+ATTR_LATEX: :options [hermitesch]
  #+begin_defn latex
  $V$ $ℂ$ Vektorraum, $h$ Sesquilinearform auf $V$. $h$ heißt *hermitesch* genau dann wenn:
  \[h(w, v) = \overline{h(v, w)} ∀ v, w ∈ V\]
  #+end_defn
  #+begin_note latex
  In diesem Fall ist $h(v, v) = \overline{h(v, v)}$, das heißt $h(v, v) ∈ ℝ ∀ v ∈ V$
  #+end_note
  #+begin_remark latex
  $V$ endlichdimensionaler $ℂ$ Vektorraum, $h$ Sesquilinearform auf $V, \mathcal{B}$ Basis von $V, A = M_{\mathcal{B}}(h)$. Dann sind äquivalent:
  1. $h$ ist hermitesch
  2. $\bar A^t = A$
  #+end_remark
  #+begin_note latex
  Matrizen $A ∈ M(n × n, ℂ)$ mit $\bar A^T = A$ heißen *hermitesche Matrizen*.
  #+end_note
  #+begin_defn latex
  $V$ $ℂ$ Vektorraum, $h$ hermitesche Form auf $V$. $h$ heißt *positiv definit* genau dann wenn
  \[h(v, v) > 0 ∀ v ∈ V, v \neq 0\]
  Eine positiv definite hermitesche Form nennt man auch ein *Skalarprodukt*.
  #+end_defn
  #+begin_ex latex
  $V = ℂ^n, <·,·>: ℂ^n × C^n \to ℂ, <x, y> := x^T \bar y$ ist ein Skalarprodukt auf $ℂ^n$ (das *Standardskalarprodukt* auf $ℂ^n$):
  - $<·,·>$ ist sesquilinear (vergleiche 25.2)
  - $<·,·>$ ist hermitesch: $<y, x> = y^T \bar x = (y^T \bar x)^T = \bar x^T y = \overline{x^T \bar y} = \overline{<x, y>}$
  - $<·,·>$ ist positiv definit:
	\begin{align*}
    <x, x> = x^T \bar x &= \begin{pmatrix}x_1 & \dots & x_n\end{pmatrix} \begin{pmatrix}\bar x_1 \\ \vdots \\ \bar x_n\end{pmatrix} = x_1 \bar x_1 + \dots + x_n \bar x_n \\
	&= \abs{x_1}^2 + \dots + \abs{x_n}^2 > 0 \text{ für } x \neq 0
    \end{align*}
  #+end_ex
  #+ATTR_LATEX: :options [Unitärer Raum]
  #+begin_defn latex
  Ein *unitärer Raum* ist ein Paar $(V, h)$, bestehend aus einem endlichdimensionalen $ℂ$ Vektorraum $V$ und einem Skalarprodukt $h$ auf $V$.
  #+end_defn
  Für den Rest des Abschnitts sei $(V, h)$ stets ein unitärer Raum.
  #+begin_note latex
  Analog zu Euklidischen Räumen definiert man die Begriffe:
  Norm, orthogonal, orthonormal, Orthogonalbasis, Orthonormalbasis, orthogonales Komplement.
  Es gilt dabei:
  - Cauchy-Schwarz-Ungleichung: $\abs{h(v, w)} \leq \norm{v} \norm{w} ∀ v, w ∈ V$
  - Gram-Schmidt-Verfahren (mit $h$ statt $γ$) liefert Orthonormalbasis
  -	$V = U \hat U^{\perp}, U^{\perp\perp} = U$ für $U ⊆ V$ Untervektorraum
  #+end_note
  #+begin_defn latex
  $(V, h_V), (W, h_W)$ unitäre Räume, $φ: V \to W$ lineare Abbildung. $φ$ heißt *unitär* genau dann wenn:
  \[h_W(φ(v_1), φ(v_2)) = h_V(v_1, v_2) ∀ v_1, v_2 ∈ V\]
  #+end_defn
  #+begin_remark latex
  $n = \dim V, \mathcal{B}$ Orthonormalbasis von $(V, h)$. Dann ist das Koordinatensystem $Φ_{\mathcal{B}}: (ℂ^n, <·,·>) \to (V, h)$ ein unitärer Isomorphismus.
  #+end_remark
  #+begin_remark latex
  $\mathcal{B}$ Orthonormalbasisv on $(V, h), φ ∈ \End(V), A = M_{\mathcal{B}}(φ)$. Dann sind äquivalent:
  1. $φ$ ist unitär
  2. $ \bar A^T A = E_n$
  #+end_remark
  #+begin_remdef latex
  $A ∈ M(n × n, ℂ)$. $A$ heißt *unitär* genau dann wenn: $\bar A^T A = E_n$.
  \[U(n) := \{A ∈ M(n × n, ℂ) \mid A \text{ ist unitär}\}\]
  $U(n)$ ist eine Gruppe bezüglich "$·$", die *unitäre Gruppe* vom Rang $n$
  \[SU(n) := \{A ∈ U(n) \mid \det A = 1\}\]
  ist eine Untergruppe von $U(n)$, die *spezielle unitäre Gruppe* von Rang $n$.
  #+end_remdef
  #+begin_remark latex
  $\mathcal{B} = (v_1, \dots, v_n)$ Orthonormalbasis von $(V, h), \mathcal{B}^{\ast} = (v_1^{\ast}, \dots, v_n^{\ast})$ duale Basis. Dann ist die Abbildung
  \[Γ: V \to V^{\ast}, w ↦ h(·, w)\]
  ein Semiisomorphismus mit $Γ(v_i) = v_i^{\ast}$ für $i = 1, \dots, n$.
  #+end_remark
  #+begin_defthm latex
  $(V, h_V), (W, h_W)$ unitäre Räume, $φ: V \to W$ lineare Abbildung, $\mathcal{A}$ Orthonormalbasis von $(V, h_V)$, $\mathcal{B}$ Orthonormalbasis von $(W, h_W)$. Dann gilt:
  1. Es gibt genau eine lineare Abbildung $φ^{ad}: W \to V$ mit $h_W(φ(v), w) = h_V(v, φ^{ad}(w)) ∀ v ∈ V, w ∈ W$, $φ^{ad}$ heißt die *zu $φ$ adjungierte Abbildung*
  2. $M_{\mathcal{A}}^{\mathcal{B}}(φ^{ad}) = \overline{M_{\mathcal{B}}^{\mathcal{A}}(φ)}^T$
  #+end_defthm
  #+begin_proof latex
  1. Wie im reellen Fall betrachte man das Diagramm
     #+begin_export latex
     \catcode`(=12
     \catcode`)=12
     #+end_export
     \begin{figure}[H]
     \centering
     \begin{tikzpicture}
     \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=3em] {V & W \\ V^{\ast} & W^{\ast}\\};
     \path[-stealth]
     (m-2-1) edge node [left] {$φ^{ad}$} (m-1-1)
         	 edge node [below] {$Γ_W$} (m-2-2)
     (m-1-1) edge node [below] {$Γ_V$} (m-1-2)
     (m-2-2) edge node [left] {$φ^{\ast}$} (m-1-2);
     \end{tikzpicture}
     \end{figure}
     #+begin_export latex
     \catcode`(=\active
     \catcode`)=\active
     #+end_export
	 und setzten $φ^{ad} := Γ_V^{-1} \circ φ^{\ast} \circ Γ_W$. $φ^{ad}$ ist linear, da sowohl $Γ_V$ als auch $Γ_W$ semilinear sind. 
	 Rest wie im reellen Fall
  2. Sei $\mathcal{A} = (v_1, \dots, v_n), \mathcal{B} = (w_1, \dots, w_n), M_{\mathcal{B}}^\mathcal{A}(φ) = (a_{ij}), M_{\mathcal{A}}^{\mathcal{B}}(φ^{ad}) = (b_ij)$
	 \begin{align*}
	 ⇒ φ(v_j) &= \sum_{k = 1}^{m} a_{kj} w_k, φ^{ad} = \sum_{k = 1}^{n} b_{ki} v_k \\
	 ⇒ a_{ij} &= h_W(\sum_{k = 1}^{m} a_{kj} w_k, w_i) = h_W(φ(w_j, w_i)) = h_V(v_j, φ^{ad} (w_i)) \\
	 &= h_V(v_j, \sum_{k = 1}^{m} b_{ki} v_k) = h_V(v_j, b_{ji} v_j) = \overline{b_{ji}} h(v_j, v_j) = \overline{b_{ji}}
     \end{align*}
  #+end_proof
  #+begin_remark latex
  $φ ∈ \End(V)$. Dann gilt:
  1. $\ker φ^{ad} = (\im φ)^{\perp}$
  2. $\im φ^{ad} = (\ker φ)^{\perp}$
  #+end_remark
  #+begin_defn latex
  $φ ∈ \End(V)$. $φ$ heißt *selbstadjungierte genau dann wenn: $φ = φ^{ad}$
  #+end_defn
  #+begin_remark latex
  $φ ∈ \End(V), \mathcal{B}$ Orthonormalbasis von $(V, h), A = M_{\mathcal{B}}(φ)$. Dann sind äquivalent:
  1. $φ$ selbstadjungiert
  2. $\bar A^T = A$, das heißt $A$ ist hermitesch
  #+end_remark
  #+begin_remark latex
  $φ ∈ \End(V)$ selbstadjungiert. Dann sind alle Eigenwerte von $φ$ reell.
  #+end_remark
  #+begin_proof latex
  Sei $λ ∈ ℂ$ Eigenwert von $φ, v$ Eigenvektor zum Eigenwert $λ$.
  \[⇒ λ h(v, v) = h(λ v, v) = h(φ(v), v) = h(v, φ^{ad}(v)) = h(v, φ(v)) = h(v, λ v) = \bar λ h(v, v)\]
  $⇒ λ = \bar λ ⇒ λ ∈ ℝ$
  #+end_proof
  #+begin_defn latex
  $φ ∈ \End(V)$. $φ$ heißt *normal* genau dann wenn: $φ^{ad} \circ φ = φ \circ φ^{ad}$. $A ∈ M(n × n, ℂ)$ heißt *normal* genau dann wenn: $\bar A^T A = A \bar A^T$
  #+end_defn
  #+begin_note latex
  Ist $\mathcal{B}$ eine Orthonormalbasis von $(V, h)$, dann: $φ$ normal $⇔ M_{\mathcal{B}}(φ)$ normal.
  #+end_note
  #+begin_remark latex
  $φ ∈ \End(V)$. Dann gilt:
  1. $φ$ unitär $⇒ φ$ normal
  2. $φ$ selbstadjungiert $⇒ φ$ normal
  Für $A ∈ M(n × n, ℂ)$ gilt: $A$ unitär $⇒ A$ normal, $A$ hermitesch $⇒ A$ normal.
  #+end_remark
  #+begin_proof latex
  1. Seien $v, w ∈ V ⇒ h(v, φ^{-1}(w)) = h(φ(v), φ(φ^{-1}(w))) = h(φ(v), w)$
	 $⇒ φ^{ad} = φ^{-1} ⇒ φ^{ad} \circ φ = φ^{-1} \circ φ = \id_V = φ \circ φ^{-1} = φ \circ φ^{ad}$
  2. $φ$ selbstadjungiert $⇒ φ = φ^{ad} ⇒ φ^{ad} \circ φ = φ \circ φ = φ \circ φ^{ad}$
  #+end_proof
  #+begin_thm latex
  $φ ∈ \End(V)$ normal. Dann gilt:
  1. $\ker φ^{ad} = \ker φ$
  2. $\im φ^{ad} = \im φ$
  Insbesondere ist $V = \ker φ \hat\oplus \im φ$
  #+end_thm
  #+begin_proof latex
  1. Es gilt:
     \begin{align*}
     v ∈ \ker φ ⇔ 0 &= h(φ(v), φ(v)) = h(v, φ^{ad}(φ(v))) = h(v, φ(φ^{ad}(v))) \\
     &= \overline{h(φ(φ^{ad}(v)), v)} = h(φ^{ad}(v), φ^{ad}(v)) ⇔ φ^{ad}(v) = 0 \\
     &⇔ v ∈ \ker φ^{ad}
     \end{align*}
  2. Es ist $\im φ^{ad} = (\ker φ)^{\perp} = (\perp φ^{ad})^{\perp} = ((\im φ)^{\perp})^{\perp} = \im φ$
  \[⇒ V = \ker φ \hat\oplus (\ker φ)^{\perp} = \ker φ \hat\oplus \im(φ^{ad}) = \ker φ \hat\oplus \im φ\]
  #+end_proof
  #+begin_remark latex
  $φ ∈ \End(V)$ normal, $λ ∈ ℂ$. Dann gilt:
  1. $φ - λ \id_V$ ist normal
  2. $\Eig(φ, λ) = \Eig(φ^{ad}, \bar λ)$
  #+end_remark
  #+begin_proof latex
  1. Setze $ψ := φ - λ \id_V$. Für $v, w ∈ V$ ist $h(λ v, w) = h(v, \bar λ w)$, das heißt $(λ \id_V)^{ad} = \bar λ \id_V$
	 \begin{align*}
     ⇒ ψ^{ad} &= φ^{ad} - \bar λ \id_V \\
	 ⇒ ψ^{ad} &= φ^{ad} - \bar λ \id_V \\
	 ⇒ ψ^{ad} \circ ψ &= (φ^{ad} - \bar λ \id_V) \circ (φ - λ \id_V) = \underbrace{φ^{ad} \circ φ}_{= φ \circ φ^{ad}} - \bar λ φ - λ φ^{ad} + λ \bar λ \id_V \\
	 &= (φ - λ \id_V) \circ (φ^{ad} - \bar λ \id_V) = ψ \circ ψ^{ad}
     \end{align*}
  2. $\Eig(φ, λ) = \ker ψ = \ker ψ^{ad} = \ker(φ^{ad} - \bar λ \id_V) = \Eig(φ^{ad}, \bar λ)$
  #+end_proof
  #+ATTR_LATEX: :options [Spektralsatz für normale Endomorphismen]
  #+begin_thm latex
  $φ ∈ \End(V)$. Dann sind äquivalent:
  1. Es gibt eine Orthonormalbasis von $(V, h)$ aus Eigenvektoren von $φ$.
  2. $φ$ ist normal
  #+end_thm
  #+begin_proof latex
  1. $⇒$ 2. Sei $\mathcal{B} = (v_1, \dots, v_n)$ eine Orthonormalbasis von $(V, h)$ aus Eigenvektoren von $φ$ zu Eigenwerten $λ_1, \dots, λ_n ∈ ℂ$. Es ist $(φ \circ φ^{ad})(v_i) = φ(φ^{ad}(v_i)) = φ(\bar λ_i, v_i) = \bar λ_i φ(v_i) = \bar λ_i λ_i v_i = (φ^{ad} \circ φ)(v_i) ∀ i = 1, \dots, n ⇒ φ\circ φ^{ad} = φ^{ad} \circ φ$
  2. $⇒$ 1. per Induktion nach $n = \dim V$. \\
	 Induktionsanfang: $n = 0$: trivial \\
	 Induktionsschritt: $n \geq 1$: Sei $λ_1 ∈ ℂ$ ein Eigenwert von $φ$. Sei $U = \Eig(φ, λ_1) = \ker(φ - λ_1 \id_V)$. Sei $(v_1, \dots, v_r)$ eine Orthonormalbasis von $(U, h \Big|_{n × n})$. Nach
	 25.25 ist $ψ := φ - λ_1 \id_V$ normal
	 \begin{align*}
	 V &= \ker ψ \hat\oplus \im ψ \\
	 &= \Eig(φ, λ_1) \hat\oplus	\underbrace{\im(φ - λ_1 \id_V)}_{=: W}
     \end{align*}
	 Es ist $φ(W) = φ(φ - λ_1 \id_V)(V) = ((φ - λ_1 \id_V) \circ φ)(V) = (φ - λ_1 \id_V)(\underbrace{φ(V)}_{⊆ V}) ⊆ \im(φ - λ_1 \id_V) = W$. Außerdem:
	 \begin{align*}
	 φ^{ad}(W) &= φ^{ad}(φ - λ_1 \id_V)(V) = (φ^{ad} \circ φ - λ_1 φ^{ad})(V) \\
	 &= (φ \circ φ^{ad} - λ_1 φ^{ad})(V) = ((φ - λ_1 \id_V) \circ φ^{ad})(V) ⊆ W
     \end{align*}
	 $φ\Big|_W^W$ ist normal, denn: Nach Eindeutigkeit der adjungierten Abbildung ist $(φ\Big|_W^W)^{ad} = (φ^{ad})\Big|_W^W$
	 \begin{align*}
	 \string(φ\Big|_W^W\string)^{ad} \circ φ\Big|_W^W &= (φ^{ad})\Big|_W^W \circ φ\Big|_W^W = (φ^{ad} \circ φ)\Big|_W^W = (φ \circ φ^{ad})\Big|_W^W \\
	 &=	φ\Big|_W^W \circ (φ^{ad})\Big|_W^W = φ\Big|_W^W \circ (φ\Big|_W^W)^{ad}
     \end{align*}
	 Nach Induktionsanfang existiert eine Orthonormalbasis $(v_{r + 1}, \dots, v_n)$ von $(V, h\Big|_{W × W})$ aus Eigenvektoren von $φ$ $⇒ (v_1, \dots, v_n)$ ist Orthonormalbasis von $(V, h)$ aus Eigenvektoren von $φ$.
  #+end_proof
  #+begin_note latex
  Insbesondere gilt:
  - Für jedes selbstadjungierten / unitären Endomorphismus existiert eine Orthonormalbasis aus Eigenvektoren
  - Jede reelle orthogonale Matrix ist *über $ℂ$* diagonalisierbar.
  Achtung: Über $ℝ$ reicht "normal" nich aus: Es gibt orthogonale Matrizen, die über $ℝ$ nich diagonalisierbar sind (zum Beispiel $\begin{pmatrix}0 & -1 \\ 1 & 0\end{pmatrix}$ (Drehung um $π / 2$))
  #+end_note
  #+begin_conc latex
  $A ∈ M(n × n, ℂ)$. Dann sind äquivalens:
  1. $A$ ist normal
  2. Es gibt eis $T ∈ U(n)$, sodass
	 \[T^{-1} A T = \begin{pmatrix}λ_1 &  & 0 \\   & \ddots &  \\ 0 &   & λ_n\end{pmatrix}\]
	 $λ_1, \dots, λ_n$ Eigenwerte von $A$
  #+end_conc
  #+begin_proof latex
  Wende 25.26 auf $(ℂ^n, <·,·>)$ und $φ = \tilde A$ an.
  #+end_proof
# Kapitel 5 Ringe und Moduln
* Ringe, Ideale und Teilbarkeit
  In diesem Abschnitt seien $R, S$ stets kommutative Rnge (bei uns immer mit Eins)
  #+ATTR_LATEX: :options [Ringhomomorphismus]
  #+begin_defn latex
  $φ: R \to S$ Abbildung. $φ$ heißt *Ringhomomorphismus* genau dann wenn folgende	Bedingungen erfüllt sind:
  - (RH1) $φ(a + b) = φ(a) + φ(b) ∀ a, b ∈ R$
  - (RH2) $φ(a b) = φ(a)φ(b) ∀ a, b ∈ R$
  -	(RH3) $φ(1_R) = 1_S$
  #+end_defn
  #+ATTR_LATEX: :options [Ideal]
  #+begin_defn latex
  $I ⊆ R$. $I$ heißt ein *Ideal* in $R$ genau dann wenn die folgenden Bedingungen erfüllt sind:
  - (I1) $0 ∈ I$
  - (I2) $a, b ∈ I ⇒ a + b ∈ I$
  -	(I3) $r ∈ R, a ∈ I ⇒ r a ∈ I$
  #+end_defn
  #+begin_ex latex
  1. $\{0\}, R$ sind Ideale in $R$
  2. FÜr $n ∈ ℤ$ ist $n ℤ \{n a \mid a ∈ ℤ\}$ ist ein Ideal
  #+end_ex
  #+begin_remdef latex
  $φ: R \to S$ Ringhomomorphismus. Dann gilt:
  1. $J ⊆ S$ Ideal $⇒ φ^{-1}(J) ⊆ R$ Ideal
  2. $\ker φ := \{a ∈ R \mid φ(a) = 0\} ⊆ R$ Ideal
  3. $φ$ injektiv $⇔ \ker φ = \{0\}$
  4. $I ⊆ R$ Ideal und $φ$ surjektiv $⇒ φ (I) ⊆ S$ Ideal
  5. $\im φ	:= φ(R)$ ist ein Unterrung von $S$ (das heißt ein Ring bezügilch der eingeschränkten Verknüpfungen.)
  #+end_remdef latex
  #+begin_proof latex
  1. (I1): $0 ∈ φ^{-1}(J)$, denn $φ(0) = φ(0 + 0) = φ(0) + φ(0) ⇒ φ(0) = 0 ∈ J ⇒ 0 ∈ φ^{-1}(J)$ \\
     (I2): $a, b ∈ φ^{-1}(J) ⇒ φ(a), φ(b) ∈ J ⇒ φ(a + b) = φ(a) + φ(b) ∈ J ⇒ a + b ∈ φ^{-1}(J)$ \\
	 (I3): $r ∈ R, a . φ^{-1}(J) ⇒ φ(a) ∈ J ⇒ φ(r a) = φ(r) φ(a) ∈ J ⇒ ra ∈ φ^{-1}(J)$
  2. aus 1., wegen $\ker φ = φ^{-1}(\{0\}), \{0\} ⊆ S$ Ideal
  3., 4., 5.: nachrechnen
  #+end_proof
  #+begin_note latex
  4. [@4] wird falsch, wenn man die Vorraussetzung $φ$ surjektiv weglässt: Die kanonische Inklusion $i: ℤ \to ℚ, x ↦ x$ ist ein Ringhomomorphismus, $ℤ$ ist ein Ideal in $ℤ$,
	 aber $ℤ = i(ℤ)$ ist kein Ideal in $ℚ$, denn:
	 \[\underbrace{\frac{1}{3}}_{∈ ℚ} · \underbrace{2}_{∈ ℤ} = \frac{2}{3} \dot ∈ ℤ\]
	 $ℤ$ ist zumindest ein Unterring von $ℚ$.
  #+end_note
  #+begin_defthm latex
  $I ⊆ R$ Ideal. Dann ist durch $r_1 \sim r_2 \xLeftrightarrow{\text{Def}} r_1 - r_2 ∈ I$ eine Äquivalenzrelation auf $R$ gegeben, welche die zusätzliche Eigenschaft
  \[r_1 \sim r_2, s_1 \sim s_2 ⇒ r_1 + s_1 \sim r_2 + s_2, r_1 s_1 \sim r_2 s_2\]
  hat ("Kongruenszrelation"). Die Äquivalenzklasse von $r ∈ R$ ist durch
  \[\bar r := r + I := \{r + a \mid a ∈ I\}\]
  gegeben und heßt die *Restklasse* von $r$ modulo $I$. Die Menge die Restklassen bezeichnen wir mit $\faktor{R}{I}$.
  #+end_defthm latex
  #+begin_proof latex
  1. "$\sim$" ist Äquivalenzrelation: nachrechnen
  2. Verträglichkeit mit $+, ·$: Sei $r_1 \sim r_2, s_1 \sim s_2 ⇒ r_1 - r_2 ∈ I, s_1 - s_2 ∈ I$
	 \begin{align*}
	 ⇒ (r_1 + s_1) - (r_2 - s_2) &= \underbrace{(r_1 - r_2)}_{∈ I} + \underbrace{(s_1 - s_2)}_{∈ I} ∈ I  ⇒ r_1 + s_1 \sim r_2 + s_2 \\
  	 \intertext{Außerdem:}
  	 r_1 s_1 - r_2 s_2 &= \underbrace{r_1 (s_1 - s_2)}_{∈ I} + \underbrace{s_2(r_1 - r_2)}_{∈ I} ∈ I ∈ r_1 s_1 \sim r_2 s_2
     \end{align*}
  #+end_proof
  #+begin_defthm latex
  $I ⊆ R$ Ideal. Dann wird $\faktor{R}{I}$ mit der Addition
  \[+: \faktor{R}{I} × \faktor{R}{I} \to \faktor{R}{I}, \bar r + \bar s := \overline{r + s}\]
  und der Multiplikation
  \[·:\faktor{R}{I} × \faktor{R}{I} \to \faktor{R}{I}, \bar r · \bar s := \overline{r s}\]
  zu einem kommutativen Ring, dam *Faktorring* (*Restklassenring*) $\faktor{R}{I}$. Die Abbildung $π: R \to \faktor{R}{I}, r ↦ \bar r$ ist ein surjektiver Ringhomomorphismus mit $\ker π = I$.
  #+end_defthm latex
  #+begin_proof latex
  *Wohldefiniertheit* von "$+$","$·$": Nach 26.5 ist für $r_1, r_2, s_1, s_2 ∈ R$ mit $r_1 \sim r_2, s_1 \sim s_2$ auch $r_1 + s_1 \sim r_2 + s_2, r_1 s_1 \sim r_2 s_2$. \\
  *Ringeigenschaften*: vererben sich aufgrund der vertreterweisen Definiton von $R$. \\
  $π$ ist Ringhomomorphismus nach Konstruktion: $π(a + b) = \overline{a + b} = \bar a + \bar b = π(a) + π(b)$, analog für "$·$", $π(1) = \bar 1$ \\
  $\ker π= \{r ∈ R \mid \bar r = \bar 0\} = \{r ∈ R \mid r \sim 0\} = \{r ∈ R \mid r - 0 ∈ I\} = I$
  #+end_proof
  #+begin_note latex
  Insbesondere sind die Ideale in $R$ genau die Kerne von Ringhomomorphismen, die von $R$ ausgehen.
  #+end_note
  #+begin_ex latex
  Ist $R = ℤ, I = n ℤ$ mit $n ∈ ℕ$, dann erhält man die aus der LA1 bekannten Restklassenringe $\faktor{ℤ}{nℤ}$ (vergleiche 6.4).
  #+end_ex
  #+ATTR_LATEX: :options [26.8 (Homomorphiesatz für Ring)]
  #+begin_thm latex
  $φ: R \to S$ Ringhomomorphismus. Dann gibt es einen Ringisomorphismus
  \[Φ: \faktor{R}{\ker φ} \to \im φ, \bar r = r + \ker φ ↦ φ(r)\]
  #+end_thm
  #+begin_proof latex
  *Wohldefiniertheit von $Φ$*: Seien $r_1, r_2 ∈ R$ mit $\bar r_1 = \bar r_2$
  \[⇒ r_1 - r_2 ∈ \ker φ ℝ φ(r_1 - r_2) = 0 ⇒ φ(r_1) = φ(r_2)\]
  *$Φ$ ist Ringhomomorphismus:
  \[Φ(\bar r_1 + \bar r_2) = Φ(\overline{r_1 + r_2}) = φ(r_1 + r_2) = φ(r_1) + φ(r_2) = Φ(\bar r_1) + Φ(\{bar r_2\})\]
  analog für "$·$", $Φ(\bar 1) = φ(1) = 1$ \\
  *$Φ$ ist injektiv*: Sei $r ∈ R$ mit $Φ(\bar r) = 0$
  \[⇒ φ(r) = 0 ⇒ r ∈ \ker φ ⇒ \bar r = r + \ker φ = \ker φ = \bar 0\]
  das heißt $\ker Φ = \{\bar 0\}$. \\
  *$Φ$ ist surjektiv*: nach	Konstruktion.
  #+end_proof
  #+begin_ex latex
  $K$ Körper, $R = K[t], φ: K[t] \to K, f ↦ f(0)$. $φ$ ist Ringhomomorphismus (nachrechnen), $\im φ = K, \ker φ = \{f ∈ K[t] \mid \im f(0) = 0\} = \{f g \mid g ∈ K[t]\} = t K[t]$.
  Wir erhalten einen Ringisomorphismus
  \[Φ: \faktor{K[t]}{t K[t]} \to K, f + t K[t] ↦ f(0)\]
  #+end_ex
  #+ATTR_LATEX: :options [26.10]
  #+begin_defn latex
  $x ∈ R$ heißt *Nullteiler* $\xLeftrightarrow{\text{Def}}$ Es existiert $y ∈ R, y \neq 0$ mit $x y = 0$. \\ $R$ heißt *Nullteiler* (*Integritätsbereich*) $\xLeftrightarrow{\text{Def}}$ $R \neq 0$ und $0 ∈ R$ der einzige Nullteiler in $R$.
  #+end_defn
  #+begin_note latex
  $R \neq 0 ⇒ 0$ ist ein Nullteiler in $R$ (wegen $0 · 1 = 0, 0 \neq 1$)
  #+end_note
  #+begin_ex latex
  1. $ℤ$ ist nullteilerfrei
  2. $\bar 2 ∈ \faktor{ℤ}{6ℤ}$ ist Nullteiler wegen $\bar 2 · \bar 3 = \bar 0$ ist $\faktor{ℤ}{6ℤ}$
  3. Analog zu $K[t]$ kann man den Polynomring $R[t]$ erklären. Es gilt dann: $R$ nullteilerfrei $⇒ R[t]$ nullteilerfrei. (Übungen)
  #+end_ex
  #+ATTR_LATEX: :options [Einheit]
  #+begin_remdef latex
  $v ∈ R$ heißt *Einheit* $\xLeftrightarrow{\text{Def}}$ es existiert ein $y ∈ R$ mit $x y = 1$.
  $R^{\ast} := \{x ∈ R \mid x \text{ ist Einheit }\}$ bildet eine abelsche Gruppe bezüglich "$·$".
  #+end_remdef latex
  #+begin_proof latex
  nachrechnen.
  #+end_proof
  #+begin_ex latex
  1. $ℤ^{\ast} = \{1, -1\}$, dann: $1 · 1 = 1, (-1)(-1) = 1, a b = 1 ⇒ \abs{a} \abs{b} = 1 ⇒ \abs{a} = \abs{b} = 1$
  2. $K$ Körper $⇒ K^{\ast} = K \setminus \{0\}$
  3. $R[t]^{\ast} = R^{\ast}$ (Übungen)
  #+end_ex
  #+begin_defn latex
  $a_1, \dots, a_n ∈ R, I ⊆ R$ Ideal.
  \[(a_1, \dots, a_n) := \{\sum_{i = 1}^{n} a_i r_i \mid r_1, \dots, r_n ∈ R\}\]
  heißt das *von $a_1, \dots, a_n$ erzeugte Ideal*. $I$ heißt *Hauptideal* $\xLeftrightarrow{\text{Def}}$ es existiert ein $a ∈ R$ mit $I = (a) = \{r a \mid r ∈ R\} =: R a$. \\
  $R$ heißt *Hauptidealring* (HIR) $\xLeftrightarrow{\text{Def}}$ $R$ ist nullteilerfrei und jedes Ideal in $R$ ist ein Hauptideal.
  #+end_defn
  #+begin_note latex
  $(a_1, \dots, a_n)$ ist ein Ideal in $R$ (leicht nachzurechnen)
  #+end_note
  #+begin_remark latex
  $Z$ ist ein Hauptidealring. Ist $I ⊆ ℤ$ ein Ideal, dann existiert ein eindeutig bestimmtes $n ∈ ℕ_0$ mit
  \[I = (n) = n ℤ\]
  #+end_remark
  #+begin_proof latex
  *$ℤ$ nullteilerfrei*: klar. \\
  *Existenz*: Sei $I ⊆ ℤ$ Ideal.
  1. Fall: $I = \{0\} = (0)$, dann fertig
  2. Fall: $I \neq \{0\}$. Mat $a ∈ I$ ist auch $-a = (-1) a ∈ I$ somit $I ∩ ℕ \neq \emptyset$. $I ∩ ℕ$ besitzt ein kleinstes Element $b$. Behauptung: $I = (b)$ \\
	 "$\supseteq$" $x ∈ (b) ⇒$ es existiert ein $r ∈ ℤ$ mit $x = r b ⇒ x ∈ I$ \\
	 "$⊆$" Sei $x ∈ I ⇒$ es	existieren $q, r ∈ ℤ$ mit $x = q b + r, 0 \leq r < b$
	 $⇒ r = x - q b ∈ I$. Wegen Minimalität von $b$ in $I ∩ N$ folgt $r = 0 ⇒ x = q b ∈ (b)$
  *Eindeutigkeit*: Seien $m, n ∈ ℕ_0$ mit $(m) = (n)$. Offenbar gilt: $m = 0 ⇔ n = 0$. Im Folgenden seien $m, n \neq 0$. Wegen $(m) = (n)$ ist $m ∈ (n), n ∈ (m) ⇒$ es existieren $r_1, r_2 ∈ ℤ$ mit
  $m = r_1 n$ und $n = r_2 m$
  \[⇒ m = r_1 n = r_1 r_2 m ⇒ r_1 r_2 = 1 ⇒ r_1 = r_2 = 1 ∨ r_1 = r_2 = -1 \xRightarrow{m, n ∈ ℕ_0} r_1 = r_2 = 1 ⇒ m = n\]
  #+end_proof
  #+begin_ex latex
  $ℤ[t]$ ist kein Hauptidealring: Es gibt $f ∈ ℤ[t]$ mit $(2, t) = (f)$, dann: Annahme: Es existiert $f ∈ ℤ[t]$ mit $2 = h f ⇒ \deg h = \deg f = 0$, das heißt $f$ ist konstantes Polynom, etwa $f = a$ für ein
  $a ∈ ℤ$. Außerdem existiert $\tilde h ∈ ℤ[t]$ mit $t = \tilde h f = h a ⇒ a = \pm 1 ⇒ f = \pm 1$. Aber: $\pm 1 \not ∈ (2, t)$, dann andernfalls existieren $u, v ∈ ℤ[t]$ mit $\pm 1 = 2 u + t v \xRightarrow{t = 0} \pm 1 = 2 u(0) + 0 · v(0) = 2 u(0)$
  #+end_ex
  #+begin_defn latex
  $R$ nullteilerfrei, $a, b ∈ R$. $b$ heißt ein *Teiler* von $a$ (Notation: $b \mid a$) $\xLeftrightarrow{\text{Def}}$ es existiert ein $c ∈ R$ mit $a = b c$. \\
  $a, b$ heißten assoziiert (Notation: $a \estimates b$) $\xLeftrightarrow{\text{Def}}$ $a \mid b$ und $b \mid a$
  #+end_defn
  #+begin_ex latex
  $R = ℤ, a ∈ ℤ ⇒ a \estimates - a$
  #+end_ex
  #+begin_remark latex
  $R$ nullteilerfrei, $a, b ∈ R$. Dann sind äquivialent:
  1. $a \estimates b$
  2. Es existiert $e ∈ ℝ^{\ast}$ mit $a = b e$
  3. $(a) = (b)$
  #+end_remark
  #+begin_proof latex
  1. $⇒$ 2. Sei $a \estimates b ⇒ a \mid b$ und $b \mid a ⇒$ es existieren $c, d ∈ R$ mit $b = a c, a = b d$
	 \[⇒ b = a c = b d c ⇒ b(1 - d c) = 0\]
	 1. Fall: $b = 0 ⇒ a = b d = 0$. Setze $e :=$, fertig: $a = b · 1$
	 2. Fall: $b \neq 0 ⇒ 1 - d c = 0 ⇒ d c = 1 ⇒ c, d ∈ R^{\ast}$. Setze $e := d$, dann $a = b d = b c$
  2. Sei $a = b e$ mit $e ∈ R^{\ast} ⇒ a ∈ (b) ⇒ (a) ⊆ (b)$. Wegen $e ∈ R^{\ast}$ ist $b = e^{-1} a ⇒ (b) ⊆ (a)$
  3. Sei $(a) = (b) ⇒ a ∈ (b) ⇒$ es existiert $c ∈ R$ mit $a = b c ⇒ b \mid a$. Analog: $a \mid b$ also $a \estimates b$
  #+end_proof
  #+begin_defn latex
  $R$ nullteilerfrei, $a_1, \dots, a_n ∈ R$. $d ∈ R$ heißt *größter gemeinsamer Teiler von $a_1, \dots, a_n \xLeftrightarrow{\text{Def}}$ Die folgenden Bedingungen sind erfüllt:
  - (GGT1) $d \mid a_1, \dots, d \mid a_n$
  -	(GGT2) $c \mid a_1, \dots, c \mid a_n ⇔ c \mid d$
  #+end_defn
  #+begin_proof latex
  Wir bezeichnent die Menge aller gröhten gemeinsamen Teiler von $a_1, \dots, a_n$ mit $\GGT(a_1, \dots, a_n)$.
  #+end_proof
  #+begin_note latex
  - Seien $d_1, d_2 ∈ \GGT(a_1, \dots, a_n)$, dann folgt $d_1 \mid d_2$ und $d_2 \mid d_1$, also $d_1 \estimates d_2$.
  - Ist $d ∈ \GGT(a_1, \dots, a_n)$ und $d' \estimates d$, dann ist $d' ∈ \GGT(a_1, \dots, a_n)$
  -	Ohne zusatzliche Vorraussetzungen an $R$ kann man im allgemeinen nicht erwarten, dass $\GGT(a_1, \dots, a_n) \neq \emptyset$. Zum Beispiel ist $R = ℤ[\sqrt{-3}] = \{a + b \sqrt{-3} \mid a, b ∈ ℤ\} ⊆ ℂ$ ist
	$\GGT(4, 2 · (1 + \sqrt{-3})) = \emptyset$ (Übungen)
  #+end_note
  #+begin_remark latex
  $R$ Hauptidealring, $a_1, \dots, a_n ∈ R$. Dann gilt:
  1. $\GGT(a_1, \dots, a_n) \neq \emptyset$
  2. $d ∈ \GGT(a_1, \dots, a_n) ⇔ (d) = (a_1, \dots, a_n)$
  #+end_remark
  #+begin_proof latex
  1. $R$ Hauptidealring $⇒$ es existiert $\tilde d ∈ R$ mit $(a_1, \dots, a_n) = (\tilde d)$. Behauptung: $\tilde d ∈ \GGT(a_1, \dots, a_n)$, denn: \\
     (GGT1): $a_1 ∈ (a_1, \dots, a_n) = (\tilde d) ⇒ \tilde d \mid a_i ∀ i = 1, \dots n$ \\
     (GGT2): Wegen $\tilde d ∈ (a_1, \dots, a_n)$ existieren $r_1, \dots, r_n ∈ R$ mit $\tilde d = r_1 a_1 + \dots + r_n a_n$. Ist $c ∈ R$ mit $c \mid a_1, \dots, c \mid a_n$, dann folgt
     $c \mid r_1 a_1 + \dots + r_n a_n = \tilde d$
  2. "$⇒$" $d ∈ \GGT(a_1, \dots, a_n) ⇒ d \estimates \tilde d ⇒ (d) = (\tilde d) = (a_1, \dots, a_n)$
	 "$\impliedby$" Sei $(d) = (a_1, \dots, a_n) ⇒ d ∈ \GGT(a_1, \dots, a_n)$ mit Argument aus dem Beweis von 1.
  #+end_proof
