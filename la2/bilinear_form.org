* Bilinearformen
  In diesem Abschnitt sei $V$ stets ein K-VR.
  #+begin_defn latex
  $γ: V × V \to K$ heißt eine Bilinearform auf $V$, genau dann wenn die folgenden Bedingungen erfüllt sind:
  - (B1) $γ(v_1 + v_2, w) = γ(v_1, w) + γ(v_2, w), γ(λ v, w) = λ γ(v, w)$
  - (B2) $γ(v, w_1 + w_2) = γ(v, w_1) + γ(v, w_2), γ(v, λ w) = λ γ(v, w)$
  $∀ v, w, v_1, v_2, w_1, w_2 ∈ V, λ ∈ K$.
  #+end_defn
  #+begin_ex latex
  1. $K = ℝ, V = ℝ^n, γ: ℝ^n × ℝ^n \to ℝ, γ(\cvec{x_1; \dots; x_n}, \cvec{y_1; \dots; y_n}) = x_1 y_1 + \dots + x_n y_2$
	 ist eine Bilinearform auf $ℝ^n$.
  2. $K = ℝ, V = l[0, 1], γ: l[0, 1] × l[0, 1] ↦ ℝ, γ(f, g) := ∫_0^1 f(t) g(t) \d t$
	 ist eine Bilinearform auf $l[0, 1]$.
  3. $K = ℝ, V = ℝ^2, γ: ℝ^2 × R^2 \to ℝ, γ(\cvec{x_1; x_2}, \cvec{y_1; y_2}) = x_1 y_1 + 2 x_1 y_2 - x_2 y_2$
	 ist eine Bilinearform auf $ℝ^2$.
  #+end_ex
  #+begin_defn latex
  $V$ endlichdimensional, $\mathcal{B} = (v_1, \dots, v_n)$ Basis von $V, γ$ Bilinearform auf $V$
  \[M_{\mathcal{B}}(γ) = (γ(v_i, v_j))_{\substack{1 \leq i \leq n\\ 1 \leq j \leq n}} ∈ M(n × n, K)\]
  heihßb die *Darstellungsmatrix* (*Fundamentalmatrix*) von $γ$ bezüglich $\mathcal{B}$.
  #+end_defn
  #+begin_ex latex
  1. In	20.2a ist für $\mathcal{B} = (e_1, \dots, e_n): M_{\mathcal{B}}(γ) = E_n$
  2. In 20.2p ist für $\mathcal{B} = (e_1, e_2): M_{\mathcal{B}}(γ) = \begin{pmatrix}1 & 2 \\ 0 & -1\end{pmatrix}$
  #+end_ex
  #+begin_remark latex
  $V$ endlichdimensional, $\mathcal{B} = (v_1, \dots, v_n)$ Basis von $V, γ$ Bilinearform auf $V, A = M_{\mathcal{B}}(γ)$,
  $Φ_{\mathcal{B}}: K^n \to V$ Koordinatensystem zu $\mathcal{B}, v, w ∈ V, x = \cvec{x_1; \vdots; v_n} = Φ_{\mathcal{B}}^{-1}(v)$, das heißt $v = x_1 v_1 + \dots + x_n v_n$,
  \[y = \cvec{y_1; \vdots; y_n} = Φ_{\mathcal{B}}^{-1}(w)\]
  das heißt $w = q_1 v_1 + \dots + y_n v_n$. Dann gilt:
  \[γ(v, w) = Φ_{\mathcal{B}^{-1}}^T A Φ_{\mathcal{B}}^{-1}(w) = x^t A y = \begin{pmatrix}x_1 & \dots & x_n\end{pmatrix} A \cvec{y_1; \vdots; y_n}\]
  #+end_remark
  #+begin_proof latex
  Es ist
  \begin{align*}
  y(v, w) &= γ(x_1 v_1 + \dots + x_n v_n, y_1 v_1 + \dots + y_n v_n) = \sum_{i = 1}^{n} \sum_{j = 1}^{n} x_i y_j γ(v_i, v_j) \\
  &= \sum_{i = 1}^{n} x_i \sum_{j = 1}^{n} γ(v_i, y_j) y_j = x^T A y
  \end{align*}
  #+end_proof
  #+begin_remark latex
  $V$ endlichdimensional, $\mathcal{B} = (v_1, \dots, v_n)$ Basis von $V, A ∈ M(n × n, K)$. Dann gilt: Durch
  \[Δ_A^{\mathcal{B}}: V × V \to K, (v, w) ↦ Φ_{\mathcal{B}}^{-1}(v)^T A Φ_{\mathcal{B}}^{-1}(w)\]
  ist eine Bilinearform auf $V$ gegeben.
  #+end_remark
  #+begin_proof latex
  Nachrechnen.
  #+end_proof
  #+ATTR_LATEX: :options [wichtiger Spezialfall von 20.6]
  #+begin_ex latex
  $V = K^n, \mathcal{B} = (e_1, \dots, e_n), A ∈ M(n × n, K) ⇒ Φ_{\mathcal{B}} = \id_{K^n}$. Durch
  \[Δ_A^{(e_1, \dots, e_n)}: K^n × K^n \to K, (v, w) ↦ v^t A w\]
  ist eine Bilinearform auf $K^n$ gegeben. Wir setzen kurz $Δ(A) := Δ_A := Δ_A^{(e_1, \dots, e_n)}$
  #+end_ex
  #+begin_remdef latex
  $\Bil(V):= \{γ: V × V \to K \mid γ \text{ ist Bilinearform }\}$ ist ein K-VR, ist ein UVR vom K-VR $\Abb(V × V, K)$
  #+end_remdef
  #+begin_remark latex
  $V$ endlichdimensional, $\mathcal{B} = (v_1, \dots, v_n)$ Basis von $V$. Dann gilt: Die Abbildung
  \[M_{\mathcal{B}}: \Bil(V) \to M(n × n, K)\]
  ist ein Isomorphismus von K-VR mit Umkehrabbildung
  \[Δ^{\mathcal{B}}: M(n × n, K) \to \Bil(V), A ↦ Δ_A^{\mathcal{B}}\]
  #+end_remark
  #+begin_proof latex
  1. $M_{\mathcal{B}}$ linear: nachrechnen.
  2. $Δ^{\mathcal{B}} \circ M_{\mathcal{B}} = \id_{\Bil(V)}$, denn: Sei $γ ∈ \Bil(V)$
	 \begin{align*}
	 ⇒ (Δ^{\mathcal{B}} \circ M_{\mathcal{B}})(γ)(v_i, v_j)	&= Δ_{M_{\mathcal{B}}(γ)}^{\mathcal{B}}(v_i, v_j) = Φ_{\mathcal{B}}^{-v}(v_1)^t M_{\mathcal{B}}(γ) Φ_{\mathcal{B}}^{-1}(v_j) \\
	 &= e_i^T M_{\mathcal{B}}(γ)e_j = γ(v_i, v_j)
     \end{align*}
  3. $M_{\mathcal{B}} \circ Δ^{\mathcal{B}} = \id_{M(n × n, K)}$, denn: Sei $A = (a_{ij}) ∈ M(n × n, K), B = (b_{ij}) = (M_{\mathcal{B}} \circ Δ^{\mathcal{B}})(A) = M_\mathcal{B} \circ Δ_A^{\mathcal{B}}$
	 \[b_{ij} = Δ_A^{\mathcal{B}}(v_i, v_j) = Φ_{\mathcal{B}}^{-1}(v_i)^T A Φ_{\mathcal{B}}(v_j) = e_i^T A e_j = a_{ij}\]
	 $⇒ B = A$
  #+end_proof
  #+begin_thm latex
  $V$ endlichdimensional, $\mathcal{A}, \mathcal{B}$ Basen von $V, γ$ Bilinearform auf $V$. Dann gilt:
  \[M_{\mathcal{B}}(γ) = (T_{\mathcal{A}}^{\mathcal{B}})^T M_{\mathcal{A}}(γ) T_{\mathcal{A}}^{\mathcal{B}}\]
  #+end_thm
  #+begin_proof latex
  Für $v, w ∈ V$ ist
  \begin{align*}
  Φ_{\mathcal{B}}^{-1}(v)^T M_{\mathcal{B}}(w) &= γ(v, w) = Φ_{\mathcal{A}}^{-1}(v)^T M_{\mathcal{A}}(γ) Φ_{\mathcal{A}}^{-1}(w) \\
  \intertext{16.2.2: $\tilde T_{\mathcal{A}}^{\mathcal{B}} = Φ_{\mathcal{A}}^{-1} \circ Φ_{\mathcal{B}}$}
  &= (T_{\mathcal{A}}^{\mathcal{B}} Φ_{\mathcal{B}}^{-1}(v))^T M_{\mathcal{A}}(γ) T_{\mathcal{A}}^{\mathcal{B}} Φ_{\mathcal{B}}^{-1}(w) \\
  &= (Φ_{\mathcal{B}}^{-1})^T (T_{\mathcal{A}}^{\mathcal{B}})^T M_{\mathcal{A}}(γ) T_{\mathcal{A}}^{\mathcal{B}} Φ_{\mathcal{B}}^{-1}(w) \\
  ⇒ Δ^{\mathcal{B}}(M_{\mathcal{B}}(γ))(v, w) &= Δ^{\mathcal{B}}((T_{\mathcal{A}}^{\mathcal{B}})^T M_{\mathcal{A}}(γ) T_{\mathcal{A}}^{\mathcal{B}})(v, w) \\
  ⇒ Δ^{\mathcal{B}}(M_{\mathcal{B}}(γ)) &= Δ^{\mathcal{B}}((T_{\mathcal{A}}^{\mathcal{B}})^T M_{\mathcal{A}}(γ) T_{\mathcal{A}}^{\mathcal{B}}) \\
  \intertext{$Δ^{\mathcal{B}}$ Isomorphismus}
  ⇒ M_{\mathcal{B}}(γ) = (T_{\mathcal{A}}^{\mathcal{B}})^T M_{\mathcal{A}}(γ) T_{\mathcal{A}}^{\mathcal{B}}
  \end{align*}
  #+end_proof
  #+begin_defn latex
  $V$ endlichdimensional, $γ$ Bilinearform auf $V$. Wir setzen $\Rang(γ) := \Rang M_{\mathcal{B}}(γ)$, wobei $\mathcal{B}$ eine Basis von $V$ ist.
  #+end_defn
  #+begin_note latex
  Dies ist wohldefiniert. (folgt aus 20.10, da die Matrizen $T_{\mathcal{A}}^{\mathcal{B}}$ invertierbar sind)
  #+end_note
  #+begin_remdef latex
  Es gilt:
  1. Ist $γ: V × V \to K$ eine Bilinearform, dann induziert $γ$ die linearen Abbildungen
	 \begin{align*}
	 Γ_l: V \to V^{\ast}, w ↦ γ(·, w) &\qquad γ(·, w): V \to K, v ↦ γ(v, w) \\
	 Γ_r: V \to V^{\ast}, v ↦ γ(v, ·) &\qquad γ(v, ·): V \to K, v ↦ γ(v, w) \\
     \end{align*}
  2. Jede lineare Abbildung $Γ: V \to V^{\ast}$ induziert Bilinearformen
	 \begin{align*}
	 γ_l: V × V \to K, γ_l(v, w) &:= Γ(w)(v) \\
	 γ_r: V × V \to K, γ_r(v, w) &:= Γ(v)(w) \\
     \end{align*}
  Die Zuordnungen aus 1., 2. induzieren den Isomorphismus $\Bil(V) \cong \Hom_K(V, V^{\ast})$
  #+end_remdef
  #+begin_proof latex
  Nachrechnen.
  #+end_proof
  #+begin_defn latex
  $γ$ Bilinearform auf $V$. $γ$ heißt *nicht-ausgeartet* $⇔$ $Γ_l$ und $Γ_r$ sind injektiv.
  \[⇔ γ(v, w) = 0 ∀ v ∈ V ⇒ w = 0\]
  (Injektivität von $Γ_l$), und
  \[⇔ γ(v, w) = 0 ∀ w ∈ V ⇒ v = 0\]
  (Injektivität von $Γ_r$). \\
  $γ$ heißt *perfekt* $⇔$ $Γ_l$ und $Γ_r$ sind Isomorphismen.
  #+end_defn
  #+begin_remark latex
  $V$ endlichdimensional, $γ$ Bilinearform auf $V, \mathcal{B} = (v_1, \dots, v_n)$ Basis von $V, \mathcal{B}^{\ast}$ duale Basis zu $\mathcal{B}$. Dann gilt:
  \[M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_l) = M_{\mathcal{B}}(γ) = (M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_r))^T\]
  #+end_remark
  #+begin_proof latex
  Behauptung: Es ist $Γ_l(v_i) = γ(v_1, v_i)v_1^{\ast} + \dots + γ(v_n, v_i) v_n^{\ast}$, denn $Γ_l(v_i)(v_j) = γ(v_j, v_i)$ nach Definition
  \[(γ(v_1, v_i)v_1^{\ast} + \dots + γ(v_n, v_i)v_n^{\ast})(v_j) = γ(v_j = v_i)\]
  Somit: $M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_l) = M_{\mathcal{B}}(γ)$. \\
  Analog: $Γ_r(v_i) = γ(v_i, v_1) v_1^{\ast} + \dots + γ(v_i, v_n) v_n^{\ast} ⇒ M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_r) = (M_{\mathcal{B}}(γ))^T$
  #+end_proof
  #+begin_conc latex
  $V$ endlichdimensional, $γ$ Bilinearform auf $V, \mathcal{B}$ Basis von $V$. Dann sind äquivalent:
  1. $γ$ ist nich-ausgeartet
  2. $γ$ ist perfekt
  3. $ M_{\mathcal{B}}(γ)$ invertierbar
  4. $Γ_l$ injektiv
  5. $Γ_r$ injektiv
  #+end_conc
  #+begin_proof latex
  1. $⇔$ 2. wegen $\dim V = \dim V^{\ast}$ und 12.12
  $γ$ perfekt $⇔ Γ_l, Γ_r$ Isomorphismen $⇔ M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_l), M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_r)$ invertierbar $⇔ M_{\mathcal{B}}(γ)$
  invertierbar.	$M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_l), M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_r) ⇔ Γ_l$ Isomorphismus $⇔ M_{\mathcal{B}^{\ast}}^\mathcal{B}$ invertierbar.
  #+end_proof
  #+begin_defn latex
  $γ$ Bilinearform auf $V$. \\
  $γ$ heißt *symmetrisch* $⇔ γ(v, w) = γ(w, v) ∀ v, w ∈ V$ \\
  $γ$ heißt *antisymmetrisch* $⇔ γ(v, w) = -γ(w, v) ∀ v, w ∈ V$ \\
  $γ$ heißt *alterniernd* $⇔ γ(v, v) = 0 ∀ v ∈ V$.
  #+end_defn
  #+begin_note latex
  - $γ$ symmetrisch $⇒ Γ_l = Γ_r$
  - Für $\cha(K) \neq 2$ gilt: $γ$ alternierned $⇔ γ$ antisymmetrisch
  - Für $\cha(K) = 2$ gilt immer noch $γ$ alternierend $⇒ γ$ (anti)symmetrisch
	Die Umkehrung ist falsch: $γ: \mathbb{F}_2^3 × \mathbb{F}_2^3 \to \mathbb{F}, γ(x, y) = x_1 y_1 + x_2 y_2 + x_3 y_3$
	ist (anti)symmetrisch, aber nicht alternierend:
	\[γ(\cvec{\bar 1; \bar 0; \bar 0}, \cvec{\bar 1; \bar 0; \bar 0}) = \bar 1 \neq \bar 0\]
  #+end_note
  #+begin_remark latex
  $V$ endlichdimensional, $\mathcal{B}$ Basis von $V, γ$ Bilinearform auf $V$. Dann gilt:
  1. $γ$ symmetrisch $⇔ M_{\mathcal{B}}(γ)$ ist symmetrisch, das heißt $M_{\mathcal{B}}(γ)^T = M_{\mathcal{B}}(γ)$
  2. $γ$ antisymmetrisch $⇔ M_{\mathcal{B}}(γ)$ ist antisymmetrisch, das heißt $M_{\mathcal{B}}(γ)^T = -M_{\mathcal{B}}(γ)$
  #+end_remark
  #+begin_proof latex
  1. "$⇒$" klar \\
	 "$\impliedby$" Sei $M_{\mathcal{B}}(γ) = M_{\mathcal{B}}(γ)^T ⇒$ Für $v, w$ ist
	 \begin{align*}
	 γ(v, w) &= Φ_{\mathcal{B}}^{-1}(v)^T M_{\mathcal{B}}(γ)Φ_{\mathcal{B}}^{-1}(w) = Φ_{\mathcal{B}}^{-1}(v)^T M_{\mathcal{B}}(γ)^T Φ_{\mathcal{B}}^{-1}(w)^T \\
	 &= \underbrace{(Φ_{\mathcal{B}}^{-1}(w)^T M_{\mathcal{B}}(γ)Φ_{\mathcal{B}}^{-1})^T}_{∈ K} = Φ_{\mathcal{B}}^{-1}(w)^T M_{\mathcal{B}}(γ) Φ_{\mathcal{B}}^{-1}(v) = γ(w, v).
     \end{align*}
  2. analog.
  #+end_proof
* Quadratische Räume
  #+ATTR_LATEX: :options [Quadratische Form]
  #+begin_defn latex
  $V$ K-VR. Eine Abbildung $q: V \to K$ heißt eine *quadratische Form* auf $V$, genau dann wenn folgende Bedingungen erfüllt sind:
  - (Q1) $q(λ v) = λ^2 q(v) ∀ λ ∈ K, v ∈ V$
  - (Q2) Die Abbildung $ε_q: V × V \to K, (v, w) ↦ q(v + w) - q(v) - q(w)$ ist eine
	(automatisch symmetrische) Bilinearform
  #+end_defn
  #+begin_ex latex
  $K = ℝ, V = ℝ^2, q(\cvec{x_1; x_2}) = x_1^2 + x_1 x_2 + x_2^2$ ist eine quatratische Form auf $ℝ^2$
  (Q1) ist erfüllt, (Q2) ist ebenfalls erfüllt, denn
  \begin{align*}
  ε_q(\cvec{x_1; x_2}, \cvec{y_1; y_2}) &= q(\cvec{x_1 + y_1; x_2 + y_2}) - q(\cvec{x_1; x_2}) - q(\cvec{y_1; y_2}) \\
  &= (x_1 + y_1)^2 + (x_1 + y_1)(x_2 + y_2) + (x_2 + y_2)^2 - x_1^2 - x_1 x_2 - x_2^2 - x_2^2 - y_1^2 - y_1 y_2 - y_2^2 \\
  &= 2x_1 y_1 + x_1 y_2 + x_2 y_1 + 2x_2 y_2
  \end{align*}
  das heißt $ε_q$ ist symmetrische Bilinearform.
  #+end_ex
  #+begin_remark latex
  $\cha K \neq 2, V$ K-VR, $\SymBil(V) := \{γ: V × V \to K \mid γ \text{ ist symmetrische Bilinearform}\}, \Quad(V) := \{q: V \to K \mid q \text{ ist eine quadratische Form}\}$. Dann sind die Abbildungen
  \[Φ: \SymBil(V) \to \Quad(V), γ ↦ q_γ \quad q_γ:V \to K, v ↦ γ(v, v)\]
  \[Ψ: \Quad(V) \to \SymBil(V), q ↦ γ_q \frac{1}{2}ε_q\]
  zueinander inverse Bijektionen.
  #+end_remark
  #+begin_proof latex
  1. $Φ$ ist wohldefiniert, das heißt $q_γ ∈ \Quad(V) ∀ γ ∈ \SymBil(V)$. \\
	 Q1: Sei $λ ∈ K, v ∈ V ⇒ q_γ(λ v) = γ(λv, λv) = λ^2 γ(v, v) = λ^2 q_γ(v)$ \\
	 Q2:
	 \begin{align*}
     ε_{q_γ} &= q_γ(v + w) - q_γ(v) - q_γ(w) = γ(v + w, v + w) - γ(v, v) - γ(w, w) \\
     &= γ(v, w) + γ(w, v) = 2γ(v, w)
     \end{align*}
	 $⇒ ε_{q_γ}$ symmetrische Bilinearform.
  2. $Ψ$ ist wohldefiniert, denn für jedes $q ∈ \Quad(V)$ ist $γ_q = (1/2) ε_q ∈ \SymBil(V)$, da $ε_q ∈ \SymBil(V)$
  3. $Φ \circ Ψ = \id_{\Quad(V)}$: Für $q ∈ \Quad(V), v ∈ V$ ist
	 \[(Φ \circ Ψ)(q)(v) = Φ(γ_q)(v) = γ_q(v, v) = \frac{1}{2}(q(v + v) - q(v) - q(v)) = q(v)\]
  4. $Ψ \circ Φ = \id_{\SymBil(v)}$: Für $γ ∈ \SymBil(v), v, w ∈ V$ ist
	 \[(Ψ \circ Φ)(γ)(v, w) = Ψ(q_γ)(v, w) = \frac{1}{2} ε_{q_γ}(v, w) = γ(v, w)\]
  #+end_proof
  #+begin_note latex
  Philosophie dahinter: symmetrische Bilinearformen, quadratische Formen auf $K$ sind für $\cha K \neq 2$ fast dasselbe. Für
  $\cha k = 2$ kann man die Abblidung $Φ$ immer noch definieren, $Φ$ ist im allgemeinen aber weder injekiv, noch surjektiv.
  Exemplarisch: Für $K = \mathbb{F}_2, V = \mathbb{F}_2^2$ liegt die quadratische Form $q: \mathbb{F}_2^2 \to \mathbb{F}, \cvec{x_1; x_2} ↦ x_1^2 + x_1 x_2 + x_2^2$ liegt nicht im Bild vom $Φ$.
  #+end_note
  Für den Rest dieses Abschnittes sei $K$ stets ein Körper mit $\cha K \neq 2$
  #+ATTR_LATEX: :options [Quadratischer Raum]
  #+begin_defn latex
  Ein *quadratischer Raum* ist ein Paar $(V, γ)$, bestehend aus endlichdimensionalem K-VR $V$ und einer symmetrischen Bilinearform $γ$ auf $V$.
  $v, w ∈ V$ heißen *orthogonal* bezüglich $γ ⇔ γ(v, w) = 0$. $(v_i)_{i ∈ I}$ Familie von Vektoren aus $V$ heißt orthogonal bezüglich $γ ⇔ γ(v_i, v_j) = 0 ∀ i, j ∈ I, i \neq j$.
  Eine Familie $(v_1, \dots, v_n)$ von Vektoren aus $V$ heißt eine *Orthogonalbasis* (OB) von $(V, γ) ⇔ (v_1, \dots, v_n)$ ist eine Basis von $V$ und ist orthogonal bezüglich $γ$.
  #+end_defn
  #+begin_note latex
  - Ist $γ$ aus dem Kontext klar, wird es auch häufig weggelassen.
  -	Ist $\mathcal{B}$ eine Basis von $V$, dann gilt $\mathcal{B}$ OB von $(V, γ) ⇔ M_{\mathcal{B}}(γ)$ ist eine Diagonalmatrix.
  #+end_note
  #+begin_defn latex
  $(V, γ_v), (W, γ_w)$ quadratische Räume, $f: V \to W$ lineare Abbildung. $f$ heißt *Homomophismus quadratischer Räume* $⇔$
  \[γ_w(f(v_1), f(v_2)) = γ_v(v_1, v_2) ∀ v_1, v_2 ∈ V\]
  $f$ heißt *Isomorphismus quadratischer Räume* $⇔ f$ ist ein Isomorphismus von K-VR und ein Homomophismus quadratischer Räume.
  Notation: Wir schreiben häufig $f:(V, γ_v) \to (W, γ_w)$ für Abbildungen / Homomorphismen quadratischer Räume.
  #+end_defn
  #+begin_note latex
  Ist $f: (V, γ_v) \to (W, γ_w)$ ein Isomorphismus quadratischer Räume, dann ist $f^{-1}: (W, γ_w) \to (V, γ_v)$ ebenfalls ein Isomorphismus quadratischer Räume, und es ist $\Rang(γ_v) = \Rang(γ_w)$ (nachrechnen...)
  #+end_note
  *Ziel*: Klassifiziere quadratische Räume bis auf Isomorphie quadratischer Räume.
  #+begin_thm latex
  $(V, γ)$ quadratischer Raum. Dann besitzt $(V, γ)$ eine OB.
  #+end_thm
  #+begin_proof latex
  per Induktion nach $n = \dim V$. \\
  IA: $n= 0$: leere Familie ist OB. \\
  IS: Sei $n \geq 1$
  1. Fall: $γ(v, v) = 0 ∀ v ∈ V$
     \[⇒ ∀ v, w ∈ V: 0 = γ(v + w, v + w) = γ(v, v) + γ(w, w) + 2 γ(v, w) = 2γ(v, w)\]
	 $⇒ γ(v, w) = 0 ∀ v, w ∈ V ⇒$  Jede Basis von $V$ ist OB von $(V, γ)$
  2. $∃ v_1 ∈ V: γ(v_1, v_1) \neq 0$. Sei $Γ: V \to V^{\ast}, v ↦ γ(v, ·)$ die zu $γ$ gemäß 20.10 gehörige lineare Abbildung. Setze $H = \ker(Γ(v_1)) = \{w ∈ W \mid γ(v_1, w) = 0\}$
	 \[⇒ \dim H = \dim V - \underbrace{\dim \im(Γ(v_1))}_{\mathclap{\leq K \text{ beachte: } Γ(v_1) ∈ V^{\ast}}} ∈ \{n, n - 1\}\]
	 Es ist $v_1 \not ∈ H$ wegen $γ(v_1, v_1) \neq 0 ⇒ \dim H = n - 1 ⇒ V = \Lin((v_1)) \oplus H$. $(H, γ \mid_{H × H})$ ist ein quadratischer Raum der Dimension $n - 1$. Wegen IV	existiert eine OB
	 $(v_2, \dots, v_n)$ von $(H, γ\mid_{H × H}) ⇒ (v_1, v_2, \dots, v_n)$ ist OB von $(V, γ)$
  #+end_proof
  #+begin_conc latex
  $A ∈ M(n × n, K)$ symmetrisch. Dann existiert $T ∈ \GL(n, K)$, sodass $T^T A T$ eine Diagonalmatrix.
  #+end_conc
  #+begin_proof latex
  $A$ definiert eine symmetrische Bilinearform $Δ(A) = Δ_A^{(e_1, \dots, e_n)}$ auf $K^n$ (vergleiche 20.7, $Δ(A)(v, w) = v^T A w$).
  Nach 21.6 existiert eine OB $\mathcal{B}$ von $(K^n, Δ(A)) ⇒ M_{\mathcal{B}}(Δ(A))$ ist Diagonalmatrix, und es ist
  \[M_{\mathcal{B}}(Δ(A)) = \underbrace{(T_{(e_1, \dots, e_n)}^{\mathcal{B}})^T}_{= T^T} \underbrace{M_{(e_1, \dots, e_n)}(Δ(A))}_{A} \underbrace{T_{(e_1, \dots, e_n)}^{\mathcal{B}}}_{=: T}\]
  #+end_proof
  #+begin_conc latex
  $(V, γ)$ quadratischer Raum, $n = \dim V, r = \Rang(γ)$. Dann existieren $λ_1, \dots, λ_r ∈ K \setminus \{0\}$ und ein Isomorphismus von quadratischen Räumen
  \[Φ: (K^n, Δ(\begin{pmatrix}λ_1 &   &   &   & 0 &   \\   & \ddots &   &   &   &   \\   &   & λ_r &   &   &   \\   &   &   & 0 &   &   \\   &   &   &   & \ddots &   \\   & 0  &   &   &  & 0 \end{pmatrix})) \to  (V, γ)\]
  #+end_conc
  #+begin_proof latex
  Wegen 21.6 existiert eine OB $\mathcal{B} = (v_1, \dots, v_n)$ von $(V, γ)$. Nach Umordnung von $v_1, \dots, v_n$ sei $γ(v_i, v_i) \neq 0$ für $i = 1, \dots, s$ und $γ(v_i, v_i) = 0$ für $i = s + 1, \dots, n$
  \[⇒ M_{\mathcal{B}}(γ) = \begin{pmatrix} λ_1 & & & & & 0 \\ & \ddots & & & & \\ & & λ_s & & & \\ & & & 0 & & \\ & & & & \ddots & \\ 0 & & & & & 0\end{pmatrix} \quad λ_1, \dots, λ_s ∈ K\setminus \{0\}, r = \Rang(γ) = \Rang M_{\mathcal{B}}(γ) = s\]
  Setze $Φ:= Φ_{\mathcal{B}}: K^n \to V, e_i ↦ v_i$ (Koordinatensystem zu $\mathcal{B}$, vegleiche 15.2). $Φ$ ist Isomorphismus
  \begin{align*}
  γ(Φ_{\mathcal{B}}(v), Φ_{\mathcal{B}}(w)) &= Φ_{\mathcal{B}}^{-1}(Φ_{\mathcal{B}}(v))^T M_{\mathcal{B}}(γ) Φ_{\mathcal{B}}^{-1}(Φ_{\mathcal{B}}(w)) = v_t M_{\mathcal{B}}(γ) w \\
  &= v^T \begin{pmatrix} λ_1 & & & & & 0 \\ & \ddots & & & & \\ & & λ_r & & & \\ & & & 0 & & \\ & & & & \ddots & \\ 0 & & & & & 0\end{pmatrix} w = Δ(\begin{pmatrix}λ_1 &   & 0 \\   & \ddots &   \\ 0 &   & λ_r\end{pmatrix})(v, w)
  \end{align*}
  #+end_proof
  #+begin_note latex
  $λ_1, \dots, λ_r$ sind im allgemeinen nicht eindeutig bestimmt.
  #+end_note
  *Frage:* Kann man über speziellen Körpern mehr sagen? Wir werden $K = ℂ, ℝ$ untersuchen.
  #+begin_thm latex
  $(V, γ)$ quadratischer Raum über $ℂ, n = \dim V, r = \Rang γ$. Dass existiert eine Orthogonalbasis $\mathcal{B}$ von $(V, γ)$ mit
  \[M_{\mathcal{B}}(γ) = \begin{pmatrix}E_r & 0 \\ 0 & 0\end{pmatrix}\]
  Insbesondere existiert ein Isomorphismus quadratischer Räume $Φ(ℂ^n, Δ(\begin{pmatrix}E_r & 0 \\ 0 & 0\end{pmatrix})) \to (V, γ)$
  #+end_thm
  #+begin_proof latex
  Sei $(\tilde v_1, \dots, \tilde v_n)$ eine Orthogonalbasis von $(V, γ)$. Setze
  \[v_i := \begin{cases} \tilde v_i & γ(\tilde v_i, \tilde v_i) = 0 \\ \frac{1}{\sqrt{\tilde v_i, \tilde v_i}} \tilde v_i & γ(\tilde v_i, \tilde v_i) \neq 0 \end{cases}\]
  Hierber ist $\sqrt{γ(\tilde v_i, \tilde v_i)}$ eine komplexe Zahl	$α$ mit $α^2 = γ(\tilde v_i, \tilde v_i)$. Falls $γ(\tilde v_i, \tilde v_i) \neq 0$, dass ist
  \[γ(v_i, v_i) = γ(\frac{1}{\sqrt{γ(\tilde v_i, \tilde v_i)}}, \frac{1}{\sqrt{γ(\tilde v_i, \tilde v_i)}}) = \frac{1}{γ(\tilde v_i, \tilde v_i)} γ(\tilde v_i, \tilde v_i) = 1\]
  Außerdem: $γ(v_i, v_j) = 0 ∀ i \neq j$, da $γ(\tilde v_i, \tilde v_j) = 0 ∀ i \neq 0$.
  Setze $\mathcal{B} := (v_1, \dots, v_n)$. Nach eventueller Umnummerierung von $v_1, \dots, v_n$ ist
  \[M_{\mathcal{B}}(γ) = \begin{pmatrix}E_r & 0 \\ 0 & 0\end{pmatrix}\]
  wobei $r = \Rang M_{\mathcal{B}}(γ) = \Rang γ$.
  #+end_proof
  #+begin_conc latex
  $A ∈ M(n × n, ℂ)$ symmetrisch, $r = \Rang A$. Dass existiert ein $T ∈ \GL(n, ℂ)$, sodass
  \[T^T A T = \begin{pmatrix}E_r & 0 \\ 0 & 0\end{pmatrix}\]
  #+end_conc
  #+ATTR_LATEX: :options [21.11]
  #+begin_conc latex
  $(V, γ_V), (W, γ_W)$ quadratische Räume über $ℂ$. Dann sind äquivalent:
  1. Es gibt einen Isomorphismus quadratischer Räume $(V, γ_V) \to (W, γ_W)$
  2. $\dim V = \dim W$ und $\Rang γ_V = \Rang γ_W$
  #+end_conc
  #+begin_proof latex
  1. $⇒$ 2. vergleiche Anmerkung nach 21.5
  2. $⇒$ 1. Sei $n = \dim V = \dim W, r = \Rang γ_V = \Rang γ_W$. $⇒ (V, γ_V), (W, γ_W)$ sind als quadratische Räume isomorph zu $(ℂ^n, Δ(\begin{pmatrix}E_r &   \\   &  \end{pmatrix}))$,
	 also au $(V, γ_V) \cong (W, γ_W)$
  #+end_proof
  #+begin_defn latex
  $(V, γ)$ quadratischer Raum, $U_1, \dots, U_m ⊆ V$ UVR mit $V = U_1 \oplus \dots \oplus U_n$. Die direkte Summe heißt
  *orthogonale direkte Summe*
  \[(V = U_1 \hat oplus \dots \hat \oplus U_m) \xLeftrightarrow{\text{Def}} γ(u_i, u_j) = 0 ∀ u_i ∈ U_i, u_j ∈ U_j, i \neq j\]
  alternativ $\operp$
  #+end_defn
  #+begin_thm latex
  $(V, γ)$ quadratischer Raum über $ℝ, n = \dim V$. Dann existiert eine Orthogonalbasis $\mathcal{B}$ von $(V, γ)$, sowie $r_+, r_- ∈ \{0, \dots, \dim V\}$ mit
  \[M_{\mathcal{B}}(γ) = \begin{pmatrix}E_{r_+} &   & 0 \\   & -E_{r_-} &   \\ 0 &   & 0\end{pmatrix}\]
  Insbesondere existiert ein Isomorphismus quadratischer Räume
  \[(ℝ^n, Δ(\begin{pmatrix}E_{r_+} &   & 0 \\   & -E_{r_-} &   \\ 0 &   & 0\end{pmatrix})) \to (V, γ)\]
  Die Zahlen $r_+, r_-$ sind unabhängig von der Wahl einer solchen Basis. Wir nennen $\Signatur(γ) := (r_+, r_-)$ heißt die *Signatur* von $γ$.
  #+end_thm
  #+begin_proof latex
  1. Sei $(\tilde v_1, \dots, \tilde v_n)$ eine Orthogonalbasis von $(V, γ)$. Wir setzen
	 \[v_i := \begin{cases} \tilde v_i & γ(\tilde v_i, \tilde v_i) = 0 \\ \frac{1}{\sqrt{\abs{γ(\tilde v_i, \tilde v_i)}}} & γ(\tilde v_i, \tilde v_i) \neq 0\end{cases}\]
	 Falls $γ(\tilde v_i, \tilde v_i) \neq 0$, dass ist
	 \begin{align*}
	 γ(v_i, v_i) &= γ(\frac{1}{\sqrt{\abs{γ(\tilde v_i, \tilde v_i)}}} \tilde v_i, \frac{1}{\sqrt{\abs{γ(\tilde v_i, \tilde v_i)}}} \tilde v_i) \\
	 &=	\frac{1}{\abs{γ(\tilde v_i, \tilde v_i)}} γ(\tilde v_i, \tilde v_i) ∈ \{\pm 1\}
     \end{align*}
	 $γ(v_i, v_j) = 0$ für $i \neq j$.
	 Setze $\mathcal{B} := (v_1, \dots, v_n)$. Nach eventueller Umnummerierung von $v_1, \dots, v_n$ ist
	 \[M_{\mathcal{B}}(γ) = \begin{pmatrix} 1 & & & & & & & & \\ & \ddots & & & & & & & \\ & & 1 & & & & & & \\ & & & -1 & & & & & \\ & & & & \ddots & & & & \\ 1 & & & & & -1 & & & \\ & & & & & & 0 & & \\ & & & & & & & \ddots & \\ & & & & & & & & 0 \end{pmatrix} = \begin{pmatrix}E_{r_+} &   & 0 \\   & -E_{r_-} &   \\ 0 &   & 0\end{pmatrix}\]
	 mit geeigneten $r_+, r_- ∈ \{0, \dots, n\}$
  2. $r_+, r_-$ sind basisunabhängig: Es ist $r_+ + r_- = \Rang γ$, dies ist basisunabhängig. Es gilt zu zeigen: $r_+$ ist basisunabhängig.
	 Setze $V_+ := \Lin((v_1, \dots, v_{r_+})), V_- = \Lin((v_{r_+ + 1} ,\dots, v_{r_+ + r_-})), V_0 := \Lin((v_{r + + r_- + 1}, \dots, v_n))$
	 $⇒ V = V_+ \hat \oplus V_- \hat \oplus V_0$. Setze
     \[s := \max\{\dim W \mid W ⊆ V \text{ UVR mit } γ(w, w) > 0 ∀ w ∈ W, w \neq 0\}\]
     dies ist wohldefiniert. $V_+$ ist ein UVR von $V$ mit $γ(w, w) > 0 ∀ w ∈ V_+, w \neq 0$, denn für $w = λ_1 v_1 + \dots + λ_{r_+} v_{r_+}$ ist
	 \[γ(w, w) = λ_1^2 \underbrace{γ(v_1, v_1)}_{= 1} + \dots + λ_{r_+}^2\underbrace{v_{r_+}, v_{r_+}}_{= 1} = λ_1^2 + \dots + λ_{r_+}^2 > 0 \text{ falls } w \neq 0\]
	 $⇒ s \geq \dim V_+ = r_+$
	 Annahme: Es existiert ein UVR $W ⊆ V$ mit $γ(w, w) > 0 ∀ w ∈ W, w \neq 0$ und $\dim W > r_+$
     \[⇒ \underbrace{\dim W}_{> r_+} + \underbrace{\dim V_-}_{= r_-} + \underbrace{\dim V_0}_{n - (r_+ + r_-)} > n\]
     \begin{align*}
     ⇒ \dim(W ∩ (V_- \hat\oplus V_0)) &= \dim W + \dim(V_- \hat\oplus V_0) - \dim(W + (W_- \hat\oplus V_0)) \\
     &= \underbrace{\dim W + \dim V_- + \dim V_0}_{> n} - \underbrace{\dim(W + (V_- \hat\oplus V_0))}_{\mathclap{\leq n, \text{ da } W + (V_- \hat\oplus W_0) \text{ UVR von } V}} \\
     &= >0
     \end{align*}
     $⇒$ Es existiert $w ∈ W, w \neq 0$ mit $w ∈ W_- \hat \oplus V_0$. \\
     $⇒$ Es existiert $w_- ∈ V_-, w_0 ∈ V_0$ mit $w = w_- + w_0$ \\
     $⇒$ $γ(w, w) = γ(w_- + w_0, w_- + w_0) = \underbrace{γ(w_-, w_-)}_{< 0} + \underbrace{γ(w_0, w_0)}_{= 0} < 0$
	 Andererseits: $γ(w, w) > 0$ wegen $w ∈ W, w \neq 0 \lightning$. Somit: $r_+ = s$, insbesondere unabhängig von Basiswahl.
  #+end_proof
  #+ATTR_LATEX: :options [Sylvesterscher Trägheitssatz]
  #+begin_concdef latex
  $A ∈ M(n × n, ℝ)$ symmetrisch. Dann existieren $T ∈ \GL(n, ℝ), r_+, r_- ∈ \{0, \dots, n\}$ mit
  \[T^T A T = \begin{pmatrix}E_{r_+} &   & 0 \\   & -E_{r_-} &   \\ 0 &   & 0\end{pmatrix}\]
  Die Zahlen $r_+, r_-$ sind unabhängig von der Wahl eines solchen $T$. $\Signatur(A) := (r_+, r_-)$ heißt *Signatur* von $A$.
  #+end_concdef
  #+begin_proof latex
  folgt aus 21.13 (analog zum Beweis von 21.7).
  #+end_proof
  #+begin_note latex
  Ist $S ∈ \GL(n, ℝ)$, dann haben die Matrixen $A$ und $S^T A S$ diesselbe Signatur, denn: Ist $\tilde T ∈ \GL(, ℝ)$ mit \[\tilde T^T(S^T A S) T = \begin{pmatrix}E_{r_+} &   & 0 \\   & -E_{r_-} &   \\ 0 &   & 0\end{pmatrix}\], dann ist
  \[(S\tilde T)^T A (S\tilde T) = \begin{pmatrix}E_{r_+} &   & 0 \\   & -E_{r_-} &   \\ 0 &   & 0\end{pmatrix}\]
  #+end_note
  #+begin_conc latex
  $(V, γ_V), (W, γ_W)$ quadratische Räume über $ℝ$. Dann sind äquivalent:
  1. Es gibt einen Isomorphismus quadratischer Räume $(V, γ_V) \to (W, γ_W)$
  2. $\dim V = \dim W$ und $\Signatur(γ_V) = \Signatur(γ_W)$
  #+end_conc
  #+begin_proof latex
  1. $⇒$ 2. Für $\Signatur(γ_V) = \Signatur(γ_W)$ verwende Charakterisierung von $r_+$ aus dem Beweis von 21.3.
  2. $⇒$ 1. aus 21.13, analog zum Beweis von 21.11
  #+end_proof
  #+begin_note latex
  Man kann Folgerung 21.11/21.15 verwenden, um quadratische Formen über $ℂ$ beziehungsweise $ℝ$ bis auf Äquivalenz zu klassifizieren (vergleiche Übungen)
  #+end_note
* Euklidische Räume
  #+begin_defn latex
  $V ℝ$ -VR, $γ: V × V \to ℝ$ symmetrische Bilinearform. $γ$ heißt
  - *positiv definit* $\xLeftrightarrow{\text{Def}} γ(v, v) > 0 ∀ v ∈ V \setminus\{0\}$
  - *positiv semidefinit* $\xLeftrightarrow{\text{Def}} γ(v, v) \geq 0 ∀ v ∈ V \setminus\{0\}$
  - *negativ definit* $\xLeftrightarrow{\text{Def}} γ(v, v) < 0 ∀ v ∈ V \setminus\{0\}$
  - *negativ semidefinit* $\xLeftrightarrow{\text{Def}} γ(v, v) \leq 0 ∀ v ∈ V \setminus\{0\}$
  - *indefinit $\xLeftrightarrow{\text{Def}} γ$ ist weder positiv noch negativ semidefinit.
  Eine positiv definite symmetrische Bilinearform nennt man auch ein *Skalarprodukt*.
  #+end_defn
  #+begin_ex latex
  1. $V = ℝ^n, <·,·>:ℝ^n × ℝ^n \to ℝ, <\cvec{x_1; \vdots; x_n}, \cvec{y_1; \vdots; y_n}> := x_1 y_1 + \dots + x_n y_n$
	 ist ein Skalarprodukt auf dem $ℝ^n$. Positiv Definitheit:
	 \[<\cvec{x_1; \vdots; x_n}, \cvec{x_1; \vdots; x_n}> = x_1^2 + \dots + x_n^2 > 0, \text{ falls } \cvec{x_1; \vdots; x_n} \neq 0\]
	 $<·, ·>$ heißt das *Standardskalarprodukt* auf dem $ℝ^n$.
  2. $V= \mathcal{C}[0, 1]$
	 \[γ: \mathcal{C}[0, 1] × \mathcal{C}[0, 1] \to ℝ, (f, g) ↦ ∫_0^1 f(t) g(t) \d t\]
	 ist ein Skalarprodukt.
  #+end_ex
  #+begin_note latex
  Um die Definitheit einer symmetrischen Bilinearform nachzuweisen, genügt es nich, das Verhalten auf den Basisvektoren zu untersuchen:
  Sei $γ: ℝ^2 × ℝ^2 \to ℝ$ gegeben durch
  \[γ = Δ(\begin{pmatrix}1 & -1 \\ -2 & 1\end{pmatrix})\]
  das heißt
  \[M_{(e_1, e_2)}(γ) = \begin{pmatrix}1 & -2 \\ -2 & 1\end{pmatrix}\]
  Dann ist $γ(e_1, e_1) = 1, γ(e_2, e_2) = 1$ aber
  \[γ(\cvec{1;1}, \cvec{1; 1}) = \begin{pmatrix}1 & 1\end{pmatrix} \begin{pmatrix}1 & -2 \\ -2 & 1\end{pmatrix} \begin{pmatrix}1 \\ 1\end{pmatrix} = -2 < 0\]
  das heißt $γ$ ist indefinit.
  #+end_note
  #+begin_defn latex
  Ein *Euklidischer Raum* ist ein Paar $(V, γ)$, bestehend aus einem endlichdimensionalen $ℝ$ -VR $V$ und einem Skalarprodukt $γ$ auf $V$.
  Für den Rest dieses Abschsittes sei $(V, γ)$ ein Euklidischer Raum.
  #+end_defn
  #+begin_defn latex
  $v ∈ V$
  \[\norm{v} := \sqrt{γ(v, v)}\]
  heißt die *Norm* auf $V$. \\
  $(v_i)_{i ∈ I}$ Familie von Vektoren aus $V$ heißt *orthonormal* $\xLeftrightarrow{\text{Def}} (v_i)_{i ∈ I}$ ist orthogonal und $\norm{v_i} = 1 ∀ i ∈ I$. \\
  $\mathcal{B} = (v_1, \dots, v_n)$ heißt *Orthonormalbasis von $V ((V, γ))$ (ONB) $⇔ \mathcal{B}$ ist Basis von $V$ und $\mathcal{B}$ ist orthonormal.
  #+end_defn
  # TODO
  #+begin_remark latex
  $\mathcal{B} = (v_1, \dots, v_n)$ Orthonormalbasis von $(V, γ), v ∈ V$. Dann gilt: Ist $v = λ_1 v_1 + \dots + λ_n v_n$, dann ist $λ_i = γ(v, v_i) ∀ i = 1, \dots, n$
  #+end_remark
  #+begin_proof latex
  $γ(v, v_i) = λ_1 γ(v_1, v_i) + \dots + λ_n γ(v_n, v_i) = λ_i \underbrace{γ(v_i, v_i)}_{= 1} = λ_i$
  #+end_proof
  #+begin_remdef latex
  $U ⊆ V$ Untervektorraum.
  \[U^{\perp} := \{v ∈ V \mid γ(v, u) = 0 ∀ u ∈ U\}\]
  heißt das *orthogonale Komplement* zu $U$. $U^{\perp}$ ist ein Untervektorraum von $V$.
  #+end_remdef
  #+begin_proof latex
  leicht nachzurechnen
  #+end_proof
  #+begin_defthm latex
  $U ⊆ V$ Untervektorraum. Dann gilt:
  1. $V = U \oplus U^{\perp}$
  2. $\dim U^{\perp} = \dim V - \dim U$
  3. $(U^{\perp})^{\perp} = U$
  4. Ist $(u_1, \dots, u_m)$ eine Orthogonalbasis von $(U, γ\mid_{U × U})$, und ist $v ∈ V$ mit $v = u + v', u ∈ U, v' ∈ U^{\perp}$, dass ist
	 \[u = \sum_{j = 1}^{m} γ(v, u_j) u_j\]
	 Die lineare Abbildung
     \[π_u: V\to U, v ↦ \sum_{j = 1}^{m} γ(v, u_j) u_j\]
	 hießt die *Orthogonalprojektion* von $V$ auf $U$.
  #+end_defthm
  #+begin_proof latex
  1. $U + U^{\perp} = V$, denn: \\
	 Sei $(u_1, \dots, u_m)$ eine Orthogonalbasis von $(U, γ\mid_{n × n}), v ∈ V$. Setze
	 \begin{align*}
     v' &:= V - \sum_{j = 1}^{m} γ(v, u_j) u_j \\
	 ⇒ γ(v', u_i) &= γ(v, u_i) - \sum_{j = 1}^{m} γ(v, u_j) γ(u_j, u_i) = γ(v, u_i) - γ(v, u_i) = 0 ∀ i = 1, \dots, m \\
	 ⇒ v' &∈ U^{\perp} \\
	 ⇒ v &= \underbrace{\sum_{j = 1}^{m} γ(v, u_j) u_j}_{∈ U} + \underbrace{v'}_{\mathclap{∈ U^{\perp}}} \\
	 ⇒ V &= U + U^{\perp}
     \end{align*}
	 $U ∩ U^{\perp} = \{0\}$, denn: $u ∈ U ∩ U^{\perp} ⇒ γ(u, u) = 0 ⇒ u = 0$ (da $γ$ Skalarprodukt)
  2. aus 1., 2.
  3. Sei $u ∈ U ⇒ γ(u, w) = 0 ∀ w = U^{\perp} ⇒ u ∈ (U^{\perp})^{\perp}$, das heißt $U ⊆ U^{\perp\perp}$.
	 Wegen $\dim(U^{\perp})^{\perp} = \dim V - \dim U^{\perp} = \dim V - (\dim V - \dim U) = \dim U$ foglt $U = U^{\perp\perp}$
  #+end_proof
  #+begin_note latex
  Insbesondere gilt für alle $v ∈ V: v - π_U(v) ∈ U^{\perp}$
  #+end_note
  #+begin_ex latex
  $(V, γ) = (ℝ^2, <·,·>), U = \Lin(\cvec{1; 1}) ⇒ U^{\perp} = \Lin(\cvec{-1; 1})$, denn $\cvec{-1; 1} ∈ U^{\perp}$ wegen $<\cvec{-1; 1}, \cvec{1; 1}> = 0$, und es eist
  $\dim U^{\perp} = 2 - \dim U = 2 - 1 = 1$. Jedes Element aus $V$ lässt sich eindeutig schreiben als
  \[v = λ\cvec{1; 1} + μ\cvec{-1; 1}\]
  das heißt
  \[π_u: v = \underbrace{λ\cvec{1; 1}}_{∈ U} + μ \underbrace{\cvec{-1; 1}}_{∈ U^{\perp}} ↦ λ\cvec{1; 1} = γ(v, \cvec{1; 1})\vec{1; 1}\]
  #+end_ex
  *Frage:* Wie bestimmt man explizit eine Orthogonalbasis eines Euklidischen Raumes?
  #+ATTR_LATEX: :options [Gram-Schmidt-Verfahren]
  #+begin_algorithm latex
  *Eingabe*: $(v_1, \dots, v_n)$ Basis von $V$. \\
  *Ausgabe*: Orthogonalbasis $(w_1, \dots, w_n)$ von $(V, γ)$ \\
  *Durchführung:*
  1. Setze
     \[w_1 := \frac{v_1}{\norm{v_1}}\]
  2. Setze für $k = 2, \dots, n$
	 \[\tilde w_k := v_k - \sum_{i = 1}^{k - 1}γ(v_k, w_i) w_i, \quad w_k := \frac{\tilde w_k}{\norm{\tilde w_k}}\]
  3. $(w_1, \dots, w_n)$ ist eine Orthogonalbasis von $(V, γ)$
  #+end_algorithm
  #+begin_proof latex
  Sei $U_k := \Lin((v_1, \dots, v_k))$ für $k = 1, \dots, n$. Wir zeigen per Induktion nach $k$, dass $(w_1, \dots, w_k)$ eine Orthogonalbasis von $(U_k, γ\mid_{U_k × U_k})$ ist
  (Behauptung folgt dann aus $k = n$). \\
  Induktionsanfang: $k = 1$ klar \\
  Induktionsschritt: Sei $π_{k - 1} := π_{U_{k - 1}}: V \to V_{k - 1}$ die orthogonale Projektion.
  \[⇒ \tilde w_k = v_k - π_{k - 1}(v_k)\]
  da $(w_1, \dots, w_{k - 1})$ Orthogonalbasis von $U_{k - 1}$ nach Induktionsvorraussetzung. $⇒\tilde w_k ∈ U_{k - 1}^{\perp}$.
  Außerdem $\tilde w_k \neq 0$, da sonst $v_k = π_{k - 1}(v_k) ∈ U_{k - 1} \lightning$ zu $(v_1, \dots, v_k)$ Basis von U_k
  \[⇒ w_k = \frac{\tilde w_k}{\norm{\tilde w_k}} ∈ U_{k - 1}^{\perp}\]
  und es ist
  \[γ(w_k, w_i) = \begin{cases} 0 & i = 1, \dots, k - 1 \\ 1 & i = k\end{cases}\]
  $⇒ (w_1, \dots, w_k)$ Orthogonalbasis von $U_k$
  #+end_proof
  #+begin_ex latex
  Wir betrachten $(ℝ^3, <·, ·>), U = \Lin((v_1, v_2))$ mit $v_1 := \cvec{2; 0; 1}, v_2 := \cvec{-1; 1; 0}$. Gesucht ist eine Orthogonalbasis von $U$ bezüglich $<·, ·>$.
  Setze
  \begin{align*}
  w &:= \frac{v_1}{\norm{v_1}} = \frac{1}{\sqrt{5}}\cvec{2; 0; 1} \\
  \tilde w_2 &= v_2 - <v_2, w_1> w_1 = \cvec{-1; 1; 0} - <\cvec{-1; 1; 0}, \frac{1}{\sqrt{5}}\cvec{2; 0; 1}> \frac{1}{\sqrt{5}}\cvec{2; 0; 1} \\
  &= \cvec{-1; 1; 0} - \frac{1}{5}<\cvec{-1; 1; 0}, \cvec{2; 0; 1}>\cvec{2; 0; 1} = \cvec{-1; 1; 0} + \frac{2}{5}\cvec{2; 0; 1} = \cvec{-\frac{1}{5}; 1; \frac{2}{5}} = \frac{1}{2} \cvec{-1; 5; 2} \\
  w_2 &= \frac{\tilde w_2}{\norm{\tilde w_2}} = \frac{1}{\sqrt{30}}\cvec{-1; 5; 2}
  \end{align*}
  $⇒ (\frac{1}{\sqrt{5}}\cvec{2; 0; 1}, \frac{1}{\sqrt{30}}\cvec{-1; 5; 2})$ ist eine Orthogonalbasis von $U$.
  #+end_ex
  #+begin_defn latex
  $A ∈ M(n × n, ℝ)$ symmetrisch. $A$ heißt *positiv definit* (Notation: $A > 0$) $\xLeftrightarrow{\text{Def}}$ Die symmetrische Bilinearform
  \[Δ(A): ℝ^n × ℝ^n \to ℝ, (x, y) ↦ x^T A y\]
  ist positiv definit.
  #+end_defn
  #+begin_remark latex
  $A ∈ M(n × n, ℝ)$ symmetrisch. Dass sind äquivalent:
  1. $A > 0$
  2. $∃ T ∈ \GL(n, ℝ): A = T^T T$
  #+end_remark
  #+begin_proof latex
  1. $⇒$ 2. Sei $A > 0 ⇒ (ℝ^n, Δ(A))$ Euklidischer Raum. Sei $\mathcal{B}$ Orthogonalbasis von $(ℝ^n, Δ(A))$ $T := T_{\mathcal{B}}^{(e_1, \dots, e_n)}$
	 \[⇒ E_n = M_{\mathcal{B}}(Δ(A)) = \underbrace{(T_{(e_1, \dots, e_n)}^{\mathcal{B}})^T}_{= (T^{-1})^T} \underbrace{M_{(e_1, \dots, e_n)}(Δ(A))}_{= A} \underbrace{T_{(e_1, \dots, e_n)}^{\mathcal{B}}}_{= T^{-1}}\]
	 $⇒ A = T^T T$
  2. Sei $A = T^T T$ für ein $T ∈ \GL(n, ℝ)$. Für $x ∈ ℝ^n, x \neq 0$ ist
	 \[Δ(A)(x, x) = x^t A w = x^t T^t T x = (Tx)^T Tx = <Tx, Tx> > 0\]
  #+end_proof
  #+begin_note latex
  1., 2. sind äquivatent zu
  3. [@3] Es existiert eine obere Dreiecksmatrix $P$ mit Diagonaleinträgen, sodass $A = P^T P$ (siehe Übungen). Obiges $P$ ist sogar eindeutig bestimmt, eine solche Zerlegung heißt Cholesky-Zerlegung.
  #+end_note
  #+ATTR_LATEX: :options [Cauchy-Schwarz-Ungleichung]
  #+begin_thm latex
  $v, w ∈ V$. Dann gil:
  \[\abs{γ(v, w)} \leq \norm{v} \norm{w}\]
  Gleichheit gilt hierbar genau dann, wenn $(v, w)$ linear abhängig.
  #+end_thm
  #+begin_proof latex
  1. Beweis der Ungleichung: Falls $w = 0$, dass fertig. Im Folgenden sei $w \neq 0$. Für $λ, μ ∈ ℝ$ ist
	 \begin{align*}
	 0 &\leq γ(λv + μw, λv + μw) = λ^2γ (v, v) + μ^2 γ(w, w) + 2 λ μ γ(v, w) \\
	 \intertext{Setze $λ := γ(w, w) > 0$, dividiere durch $λ$}
	 0 &\leq γ(v, v) γ(w, w) + μ^2 + 2μ γ(v, w) \\
	 \intertext{Setze $μ := -γ(v, w)$}
	 0 &\leq γ(v, v)γ(w, w) + γ(v, w)^2 - 2γ(v, w)^2 \\
	 γ(v, w)^2 &\leq γ(v, v)γ(w, w) \\
	 \abs{γ(v, w)} &\leq \norm{v} \norm{w} \\
     \end{align*}
  2. Gleichheitsaussage: Für $w = 0$: $(v, w)$ linear abhängig und "$=$" gilt. Ab jetzt also $w \neq 0$. \\
	 "$\impliedby$" Sei $(v, w)$ linear abhängig $⇒ ∃ λ ∈ K: v = κw$
	 \[⇒ \abs{γ(v, w)}^2 = \abs{γ(λw, w)}^2 = \abs{λ^2}\abs{γ(w, w)}^2 = \abs{γ(w, w)}\abs{γ(λw, λw)} = \norm{w}^2 \norm{λw}^2\]
	 $⇒ \abs{γ(v, w)} = \norm{w}\norm{λ w} = \norm{w}\norm{v}$. \\
	 "$⇒$" Es gelte, sei also $\abs{γ(v, w)} = \norm{v}\norm{w}$. Führe die Rechnung wie in 1. rückwärts durch: Mit $λ := γ(w, w), μ = -γ(v, w)$ folgt
	 dass
     \[γ(λv + μw, λv + μw) = 0 ⇒ λv + μw = 0 ⇒ (v, w)\text{ linear abhängig}\]
  #+end_proof
