* Bilinearformen
  In diesem Abschnitt sei $V$ stets ein K-VR.
  #+begin_defn latex
  $γ: V × V \to K$ heißt eine Bilinearform auf $V$, genau dann wenn die folgenden Bedingungen erfüllt sind:
  - (B1) $γ(v_1 + v_2, w) = γ(v_1, w) + γ(v_2, w), γ(λ v, w) = λ γ(v, w)$
  - (B2) $γ(v, w_1 + w_2) = γ(v, w_1) + γ(v, w_2), γ(v, λ w) = λ γ(v, w)$
  $∀ v, w, v_1, v_2, w_1, w_2 ∈ V, λ ∈ K$.
  #+end_defn
  #+begin_ex latex
  1. $K = ℝ, V = ℝ^n, γ: ℝ^n × ℝ^n \to ℝ, γ(\cvec{x_1; \dots; x_n}, \cvec{y_1; \dots; y_n}) = x_1 y_1 + \dots + x_n y_2$
	 ist eine Bilinearform auf $ℝ^n$.
  2. $K = ℝ, V = l[0, 1], γ: l[0, 1] × l[0, 1] ↦ ℝ, γ(f, g) := ∫_0^1 f(t) g(t) \d t$
	 ist eine Bilinearform auf $l[0, 1]$.
  3. $K = ℝ, V = ℝ^2, γ: ℝ^2 × R^2 \to ℝ, γ(\cvec{x_1; x_2}, \cvec{y_1; y_2}) = x_1 y_1 + 2 x_1 y_2 - x_2 y_2$
	 ist eine Bilinearform auf $ℝ^2$.
  #+end_ex
  #+begin_defn latex
  $V$ endlichdimensional, $\mathcal{B} = (v_1, \dots, v_n)$ Basis von $V, γ$ Bilinearform auf $V$
  \[M_{\mathcal{B}}(γ) = (γ(v_i, v_j))_{\substack{1 \leq i \leq n\\ 1 \leq j \leq n}} ∈ M(n × n, K)\]
  heihßb die *Darstellungsmatrix* (*Fundamentalmatrix*) von $γ$ bezüglich $\mathcal{B}$.
  #+end_defn
  #+begin_ex latex
  1. In	20.2a ist für $\mathcal{B} = (e_1, \dots, e_n): M_{\mathcal{B}}(γ) = E_n$
  2. In 20.2p ist für $\mathcal{B} = (e_1, e_2): M_{\mathcal{B}}(γ) = \begin{pmatrix}1 & 2 \\ 0 & -1\end{pmatrix}$
  #+end_ex
  #+begin_remark latex
  $V$ endlichdimensional, $\mathcal{B} = (v_1, \dots, v_n)$ Basis von $V, γ$ Bilinearform auf $V, A = M_{\mathcal{B}}(γ)$,
  $Φ_{\mathcal{B}}: K^n \to V$ Koordinatensystem zu $\mathcal{B}, v, w ∈ V, x = \cvec{x_1; \vdots; v_n} = Φ_{\mathcal{B}}^{-1}(v)$, das heißt $v = x_1 v_1 + \dots + x_n v_n$,
  \[y = \cvec{y_1; \vdots; y_n} = Φ_{\mathcal{B}}^{-1}(w)\]
  das heißt $w = q_1 v_1 + \dots + y_n v_n$. Dann gilt:
  \[γ(v, w) = Φ_{\mathcal{B}^{-1}}^T A Φ_{\mathcal{B}}^{-1}(w) = x^t A y = \begin{pmatrix}x_1 & \dots & x_n\end{pmatrix} A \cvec{y_1; \vdots; y_n}\]
  #+end_remark
  #+begin_proof latex
  Es ist
  \begin{align*}
  y(v, w) &= γ(x_1 v_1 + \dots + x_n v_n, y_1 v_1 + \dots + y_n v_n) = \sum_{i = 1}^{n} \sum_{j = 1}^{n} x_i y_j γ(v_i, v_j) \\
  &= \sum_{i = 1}^{n} x_i \sum_{j = 1}^{n} γ(v_i, y_j) y_j = x^T A y
  \end{align*}
  #+end_proof
  #+begin_remark latex
  $V$ endlichdimensional, $\mathcal{B} = (v_1, \dots, v_n)$ Basis von $V, A ∈ M(n × n, K)$. Dann gilt: Durch
  \[Δ_A^{\mathcal{B}}: V × V \to K, (v, w) ↦ Φ_{\mathcal{B}}^{-1}(v)^T A Φ_{\mathcal{B}}^{-1}(w)\]
  ist eine Bilinearform auf $V$ gegeben.
  #+end_remark
  #+begin_proof latex
  Nachrechnen.
  #+end_proof
  #+ATTR_LATEX: :options [wichtiger Spezialfall von 20.6]
  #+begin_ex latex
  $V = K^n, \mathcal{B} = (e_1, \dots, e_n), A ∈ M(n × n, K) ⇒ Φ_{\mathcal{B}} = \id_{K^n}$. Durch
  \[Δ_A^{(e_1, \dots, e_n)}: K^n × K^n \to K, (v, w) ↦ v^t A w\]
  ist eine Bilinearform auf $K^n$ gegeben. Wir setzen kurz $Δ(A) := Δ_A := Δ_A^{(e_1, \dots, e_n)}$
  #+end_ex
  #+begin_remdef latex
  $\Bil(V):= \{γ: V × V \to K \mid γ \text{ ist Bilinearform }\}$ ist ein K-VR, ist ein UVR vom K-VR $\Abb(V × V, K)$
  #+end_remdef
  #+begin_remark latex
  $V$ endlichdimensional, $\mathcal{B} = (v_1, \dots, v_n)$ Basis von $V$. Dann gilt: Die Abbildung
  \[M_{\mathcal{B}}: \Bil(V) \to M(n × n, K)\]
  ist ein Isomorphismus von K-VR mit Umkehrabbildung
  \[Δ^{\mathcal{B}}: M(n × n, K) \to \Bil(V), A ↦ Δ_A^{\mathcal{B}}\]
  #+end_remark
  #+begin_proof latex
  1. $M_{\mathcal{B}}$ linear: nachrechnen.
  2. $Δ^{\mathcal{B}} \circ M_{\mathcal{B}} = \id_{\Bil(V)}$, denn: Sei $γ ∈ \Bil(V)$
	 \begin{align*}
	 ⇒ (Δ^{\mathcal{B}} \circ M_{\mathcal{B}})(γ)(v_i, v_j)	&= Δ_{M_{\mathcal{B}}(γ)}^{\mathcal{B}}(v_i, v_j) = Φ_{\mathcal{B}}^{-v}(v_1)^t M_{\mathcal{B}}(γ) Φ_{\mathcal{B}}^{-1}(v_j) \\
	 &= e_i^T M_{\mathcal{B}}(γ)e_j = γ(v_i, v_j)
     \end{align*}
  3. $M_{\mathcal{B}} \circ Δ^{\mathcal{B}} = \id_{M(n × n, K)}$, denn: Sei $A = (a_{ij}) ∈ M(n × n, K), B = (b_{ij}) = (M_{\mathcal{B}} \circ Δ^{\mathcal{B}})(A) = M_\mathcal{B} \circ Δ_A^{\mathcal{B}}$
	 \[b_{ij} = Δ_A^{\mathcal{B}}(v_i, v_j) = Φ_{\mathcal{B}}^{-1}(v_i)^T A Φ_{\mathcal{B}}(v_j) = e_i^T A e_j = a_{ij}\]
	 $⇒ B = A$
  #+end_proof
  #+begin_thm latex
  $V$ endlichdimensional, $\mathcal{A}, \mathcal{B}$ Basen von $V, γ$ Bilinearform auf $V$. Dann gilt:
  \[M_{\mathcal{B}}(γ) = (T_{\mathcal{A}}^{\mathcal{B}})^T M_{\mathcal{A}}(γ) T_{\mathcal{A}}^{\mathcal{B}}\]
  #+end_thm
  #+begin_proof latex
  Für $v, w ∈ V$ ist
  \begin{align*}
  Φ_{\mathcal{B}}^{-1}(v)^T M_{\mathcal{B}}(w) &= γ(v, w) = Φ_{\mathcal{A}}^{-1}(v)^T M_{\mathcal{A}}(γ) Φ_{\mathcal{A}}^{-1}(w) \\
  \intertext{16.2.2: $\tilde T_{\mathcal{A}}^{\mathcal{B}} = Φ_{\mathcal{A}}^{-1} \circ Φ_{\mathcal{B}}$}
  &= (T_{\mathcal{A}}^{\mathcal{B}} Φ_{\mathcal{B}}^{-1}(v))^T M_{\mathcal{A}}(γ) T_{\mathcal{A}}^{\mathcal{B}} Φ_{\mathcal{B}}^{-1}(w) \\
  &= (Φ_{\mathcal{B}}^{-1})^T (T_{\mathcal{A}}^{\mathcal{B}})^T M_{\mathcal{A}}(γ) T_{\mathcal{A}}^{\mathcal{B}} Φ_{\mathcal{B}}^{-1}(w) \\
  ⇒ Δ^{\mathcal{B}}(M_{\mathcal{B}}(γ))(v, w) &= Δ^{\mathcal{B}}((T_{\mathcal{A}}^{\mathcal{B}})^T M_{\mathcal{A}}(γ) T_{\mathcal{A}}^{\mathcal{B}})(v, w) \\
  ⇒ Δ^{\mathcal{B}}(M_{\mathcal{B}}(γ)) &= Δ^{\mathcal{B}}((T_{\mathcal{A}}^{\mathcal{B}})^T M_{\mathcal{A}}(γ) T_{\mathcal{A}}^{\mathcal{B}}) \\
  \intertext{$Δ^{\mathcal{B}}$ Isomorphismus}
  ⇒ M_{\mathcal{B}}(γ) = (T_{\mathcal{A}}^{\mathcal{B}})^T M_{\mathcal{A}}(γ) T_{\mathcal{A}}^{\mathcal{B}}
  \end{align*}
  #+end_proof
  #+begin_defn latex
  $V$ endlichdimensional, $γ$ Bilinearform auf $V$. Wir setzen $\Rang(γ) := \Rang M_{\mathcal{B}}(γ)$, wobei $\mathcal{B}$ eine Basis von $V$ ist.
  #+end_defn
  #+begin_note latex
  Dies ist wohldefiniert. (folgt aus 20.10, da die Matrizen $T_{\mathcal{A}}^{\mathcal{B}}$ invertierbar sind)
  #+end_note
  #+begin_remdef latex
  Es gilt:
  1. Ist $γ: V × V \to K$ eine Bilinearform, dann induziert $γ$ die linearen Abbildungen
	 \begin{align*}
	 Γ_l: V \to V^{\ast}, w ↦ γ(·, w) &\qquad γ(·, w): V \to K, v ↦ γ(v, w) \\
	 Γ_r: V \to V^{\ast}, v ↦ γ(v, ·) &\qquad γ(v, ·): V \to K, v ↦ γ(v, w) \\
     \end{align*}
  2. Jede lineare Abbildung $Γ: V \to V^{\ast}$ induziert Bilinearformen
	 \begin{align*}
	 γ_l: V × V \to K, γ_l(v, w) &:= Γ(w)(v) \\
	 γ_r: V × V \to K, γ_r(v, w) &:= Γ(v)(w) \\
     \end{align*}
  Die Zuordnungen aus 1., 2. induzieren den Isomorphismus $\Bil(V) \cong \Hom_K(V, V^{\ast})$
  #+end_remdef
  #+begin_proof latex
  Nachrechnen.
  #+end_proof
  #+begin_defn latex
  $γ$ Bilinearform auf $V$. $γ$ heißt *nicht-ausgeartet* $⇔$ $Γ_l$ und $Γ_r$ sind injektiv.
  \[⇔ γ(v, w) = 0 ∀ v ∈ V ⇒ w = 0\]
  (Injektivität von $Γ_l$), und
  \[⇔ γ(v, w) = 0 ∀ w ∈ V ⇒ v = 0\]
  (Injektivität von $Γ_r$). \\
  $γ$ heißt *perfekt* $⇔$ $Γ_l$ und $Γ_r$ sind Isomorphismen.
  #+end_defn
  #+begin_remark latex
  $V$ endlichdimensional, $γ$ Bilinearform auf $V, \mathcal{B} = (v_1, \dots, v_n)$ Basis von $V, \mathcal{B}^{\ast}$ duale Basis zu $\mathcal{B}$. Dann gilt:
  \[M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_l) = M_{\mathcal{B}}(γ) = (M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_r))^T\]
  #+end_remark
  #+begin_proof latex
  Behauptung: Es ist $Γ_l(v_i) = γ(v_1, v_i)v_1^{\ast} + \dots + γ(v_n, v_i) v_n^{\ast}$, denn $Γ_l(v_i)(v_j) = γ(v_j, v_i)$ nach Definition
  \[(γ(v_1, v_i)v_1^{\ast} + \dots + γ(v_n, v_i)v_n^{\ast})(v_j) = γ(v_j = v_i)\]
  Somit: $M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_l) = M_{\mathcal{B}}(γ)$. \\
  Analog: $Γ_r(v_i) = γ(v_i, v_1) v_1^{\ast} + \dots + γ(v_i, v_n) v_n^{\ast} ⇒ M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_r) = (M_{\mathcal{B}}(γ))^T$
  #+end_proof
  #+begin_conc latex
  $V$ endlichdimensional, $γ$ Bilinearform auf $V, \mathcal{B}$ Basis von $V$. Dann sind äquivalent:
  1. $γ$ ist nich-ausgeartet
  2. $γ$ ist perfekt
  3. $ M_{\mathcal{B}}(γ)$ invertierbar
  4. $Γ_l$ injektiv
  5. $Γ_r$ injektiv
  #+end_conc
  #+begin_proof latex
  1. $⇔$ 2. wegen $\dim V = \dim V^{\ast}$ und 12.12
  $γ$ perfekt $⇔ Γ_l, Γ_r$ Isomorphismen $⇔ M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_l), M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_r)$ invertierbar $⇔ M_{\mathcal{B}}(γ)$
  invertierbar.	$M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_l), M_{\mathcal{B}^{\ast}}^{\mathcal{B}}(Γ_r) ⇔ Γ_l$ Isomorphismus $⇔ M_{\mathcal{B}^{\ast}}^\mathcal{B}$ invertierbar.
  #+end_proof
  #+begin_defn latex
  $γ$ Bilinearform auf $V$. \\
  $γ$ heißt *symmetrisch* $⇔ γ(v, w) = γ(w, v) ∀ v, w ∈ V$ \\
  $γ$ heißt *antisymmetrisch* $⇔ γ(v, w) = -γ(w, v) ∀ v, w ∈ V$ \\
  $γ$ heißt *alterniernd* $⇔ γ(v, v) = 0 ∀ v ∈ V$.
  #+end_defn
  #+begin_note latex
  - $γ$ symmetrisch $⇒ Γ_l = Γ_r$
  - Für $\cha(K) \neq 2$ gilt: $γ$ alternierned $⇔ γ$ antisymmetrisch
  - Für $\cha(K) = 2$ gilt immer noch $γ$ alternierend $⇒ γ$ (anti)symmetrisch
	Die Umkehrung ist falsch: $γ: \mathbb{F}_2^3 × \mathbb{F}_2^3 \to \mathbb{F}, γ(x, y) = x_1 y_1 + x_2 y_2 + x_3 y_3$
	ist (anti)symmetrisch, aber nicht alternierend:
	\[γ(\cvec{\bar 1; \bar 0; \bar 0}, \cvec{\bar 1; \bar 0; \bar 0}) = \bar 1 \neq \bar 0\]
  #+end_note
  #+begin_remark latex
  $V$ endlichdimensional, $\mathcal{B}$ Basis von $V, γ$ Bilinearform auf $V$. Dann gilt:
  1. $γ$ symmetrisch $⇔ M_{\mathcal{B}}(γ)$ ist symmetrisch, das heißt $M_{\mathcal{B}}(γ)^T = M_{\mathcal{B}}(γ)$
  2. $γ$ antisymmetrisch $⇔ M_{\mathcal{B}}(γ)$ ist antisymmetrisch, das heißt $M_{\mathcal{B}}(γ)^T = -M_{\mathcal{B}}(γ)$
  #+end_remark
  #+begin_proof latex
  1. "$⇒$" klar \\
	 "$\impliedby$" Sei $M_{\mathcal{B}}(γ) = M_{\mathcal{B}}(γ)^T ⇒$ Für $v, w$ ist
	 \begin{align*}
	 γ(v, w) &= Φ_{\mathcal{B}}^{-1}(v)^T M_{\mathcal{B}}(γ)Φ_{\mathcal{B}}^{-1}(w) = Φ_{\mathcal{B}}^{-1}(v)^T M_{\mathcal{B}}(γ)^T Φ_{\mathcal{B}}^{-1}(w)^T \\
	 &= \underbrace{(Φ_{\mathcal{B}}^{-1}(w)^T M_{\mathcal{B}}(γ)Φ_{\mathcal{B}}^{-1})^T}_{∈ K} = Φ_{\mathcal{B}}^{-1}(w)^T M_{\mathcal{B}}(γ) Φ_{\mathcal{B}}^{-1}(v) = γ(w, v).
     \end{align*}
  2. analog.
  #+end_proof
* Quadratische Räume
  #+ATTR_LATEX: :options [Quadratische Form]
  #+begin_defn latex
  $V$ K-VR. Eine Abbildung $q: V \to K$ heißt eine *quadratische Form* auf $V$, genau dann wenn folgende Bedingungen erfüllt sind:
  - (Q1) $q(λ v) = λ^2 q(v) ∀ λ ∈ K, v ∈ V$
  - (Q2) Die Abbildung $ε_q: V × V \to K, (v, w) ↦ q(v + w) - q(v) - q(w)$ ist eine
	(automatisch symmetrische) Bilinearform
  #+end_defn
  #+begin_ex latex
  $K = ℝ, V = ℝ^2, q(\cvec{x_1; x_2}) = x_1^2 + x_1 x_2 + x_2^2$ ist eine quatratische Form auf $ℝ^2$
  (Q1) ist erfüllt, (Q2) ist ebenfalls erfüllt, denn
  \begin{align*}
  ε_q(\cvec{x_1; x_2}, \cvec{y_1; y_2}) &= q(\cvec{x_1 + y_1; x_2 + y_2}) - q(\cvec{x_1; x_2}) - q(\cvec{y_1; y_2}) \\
  &= (x_1 + y_1)^2 + (x_1 + y_1)(x_2 + y_2) + (x_2 + y_2)^2 - x_1^2 - x_1 x_2 - x_2^2 - x_2^2 - y_1^2 - y_1 y_2 - y_2^2 \\
  &= 2x_1 y_1 + x_1 y_2 + x_2 y_1 + 2x_2 y_2
  \end{align*}
  das heißt $ε_q$ ist symmetrische Bilinearform.
  #+end_ex
  #+begin_remark latex
  $\cha K \neq 2, V$ K-VR, $\SymBil(V) := \{γ: V × V \to K \mid γ \text{ ist symmetrische Bilinearform}\}, \Quad(V) := \{q: V \to K \mid q \text{ ist eine quadratische Form}\}$. Dann sind die Abbildungen
  \[Φ: \SymBil(V) \to \Quad(V), γ ↦ q_γ \quad q_γ:V \to K, v ↦ γ(v, v)\]
  \[Ψ: \Quad(V) \to \SymBil(V), q ↦ γ_q \frac{1}{2}ε_q\]
  zueinander inverse Bijektionen.
  #+end_remark
  #+begin_proof latex
  1. $Φ$ ist wohldefiniert, das heißt $q_γ ∈ \Quad(V) ∀ γ ∈ \SymBil(V)$. \\
	 Q1: Sei $λ ∈ K, v ∈ V ⇒ q_γ(λ v) = γ(λv, λv) = λ^2 γ(v, v) = λ^2 q_γ(v)$ \\
	 Q2:
	 \begin{align*}
     ε_{q_γ} &= q_γ(v + w) - q_γ(v) - q_γ(w) = γ(v + w, v + w) - γ(v, v) - γ(w, w) \\
     &= γ(v, w) + γ(w, v) = 2γ(v, w)
     \end{align*}
	 $⇒ ε_{q_γ}$ symmetrische Bilinearform.
  2. $Ψ$ ist wohldefiniert, denn für jedes $q ∈ \Quad(V)$ ist $γ_q = (1/2) ε_q ∈ \SymBil(V)$, da $ε_q ∈ \SymBil(V)$
  3. $Φ \circ Ψ = \id_{\Quad(V)}$: Für $q ∈ \Quad(V), v ∈ V$ ist
	 \[(Φ \circ Ψ)(q)(v) = Φ(γ_q)(v) = γ_q(v, v) = \frac{1}{2}(q(v + v) - q(v) - q(v)) = q(v)\]
  4. $Ψ \circ Φ = \id_{\SymBil(v)}$: Für $γ ∈ \SymBil(v), v, w ∈ V$ ist
	 \[(Ψ \circ Φ)(γ)(v, w) = Ψ(q_γ)(v, w) = \frac{1}{2} ε_{q_γ}(v, w) = γ(v, w)\]
  #+end_proof
  #+begin_note latex
  Philosophie dahinter: symmetrische Bilinearformen, quadratische Formen auf $K$ sind für $\cha K \neq 2$ fast dasselbe. Für
  $\cha k = 2$ kann man die Abblidung $Φ$ immer noch definieren, $Φ$ ist im allgemeinen aber weder injekiv, noch surjektiv.
  Exemplarisch: Für $K = \mathbb{F}_2, V = \mathbb{F}_2^2$ liegt die quadratische Form $q: \mathbb{F}_2^2 \to \mathbb{F}, \cvec{x_1; x_2} ↦ x_1^2 + x_1 x_2 + x_2^2$ liegt nicht im Bild vom $Φ$.
  #+end_note
  Für den Rest dieses Abschnittes sei $K$ stets ein Körper mit $\cha K \neq 2$
  #+ATTR_LATEX: :options [Quadratischer Raum]
  #+begin_defn latex
  Ein *quadratischer Raum* ist ein Paar $(V, γ)$, bestehend aus endlichdimensionalem K-VR $V$ und einer symmetrischen Bilinearform $γ$ auf $V$.
  $v, w ∈ V$ heißen *orthogonal* bezüglich $γ ⇔ γ(v, w) = 0$. $(v_i)_{i ∈ I}$ Familie von Vektoren aus $V$ heißt orthogonal bezüglich $γ ⇔ γ(v_i, v_j) = 0 ∀ i, j ∈ I, i \neq j$.
  Eine Familie $(v_1, \dots, v_n)$ von Vektoren aus $V$ heißt eine *Orthogonalbasis* (OB) von $(V, γ) ⇔ (v_1, \dots, v_n)$ ist eine Basis von $V$ und ist orthogonal bezüglich $γ$.
  #+end_defn
  #+begin_note latex
  - Ist $γ$ aus dem Kontext klar, wird es auch häufig weggelassen.
  -	Ist $\mathcal{B}$ eine Basis von $V$, dann gilt $\mathcal{B}$ OB von $(V, γ) ⇔ M_{\mathcal{B}}(γ)$ ist eine Diagonalmatrix.
  #+end_note
  #+begin_defn latex
  $(V, γ_v), (W, γ_w)$ quadratische Räume, $f: V \to W$ lineare Abbildung. $f$ heißt *Homomophismus quadratischer Räume* $⇔$
  \[γ_w(f(v_1), f(v_2)) = γ_v(v_1, v_2) ∀ v_1, v_2 ∈ V\]
  $f$ heißt *Isomorphismus quadratischer Räume* $⇔ f$ ist ein Isomorphismus von K-VR und ein Homomophismus quadratischer Räume.
  Notation: Wir schreiben häufig $f:(V, γ_v) \to (W, γ_w)$ für Abbildungen / Homomorphismen quadratischer Räume.
  #+end_defn
  #+begin_note latex
  Ist $f: (V, γ_v) \to (W, γ_w)$ ein Isomorphismus quadratischer Räume, dann ist $f^{-1}: (W, γ_w) \to (V, γ_v)$ ebenfalls ein Isomorphismus quadratischer Räume, und es ist $\Rang(γ_v) = \Rang(γ_w)$ (nachrechnen...)
  #+end_note
  *Ziel*: Klassifiziere quadratische Räume bis auf Isomorphie quadratischer Räume.
  #+begin_thm latex
  $(V, γ)$ quadratischer Raum. Dann besitzt $(V, γ)$ eine OB.
  #+end_thm
  #+begin_proof latex
  per Induktion nach $n = \dim V$. \\
  IA: $n= 0$: leere Familie ist OB. \\
  IS: Sei $n \geq 1$
  1. Fall: $γ(v, v) = 0 ∀ v ∈ V$
     \[⇒ ∀ v, w ∈ V: 0 = γ(v + w, v + w) = γ(v, v) + γ(w, w) + 2 γ(v, w) = 2γ(v, w)\]
	 $⇒ γ(v, w) = 0 ∀ v, w ∈ V ⇒$  Jede Basis von $V$ ist OB von $(V, γ)$
  2. $∃ v_1 ∈ V: γ(v_1, v_1) \neq 0$. Sei $Γ: V \to V^{\ast}, v ↦ γ(v, ·)$ die zu $γ$ gemäß 20.10 gehörige lineare Abbildung. Setze $H = \ker(Γ(v_1)) = \{w ∈ W \mid γ(v_1, w) = 0\}$
	 \[⇒ \dim H = \dim V - \underbrace{\dim \im(Γ(v_1))}_{\mathclap{\leq K \text{ beachte: } Γ(v_1) ∈ V^{\ast}}} ∈ \{n, n - 1\}\]
	 Es ist $v_1 \not ∈ H$ wegen $γ(v_1, v_1) \neq 0 ⇒ \dim H = n - 1 ⇒ V = \Lin((v_1)) \oplus H$. $(H, γ \mid_{H × H})$ ist ein quadratischer Raum der Dimension $n - 1$. Wegen IV	existiert eine OB
	 $(v_2, \dots, v_n)$ von $(H, γ\mid_{H × H}) ⇒ (v_1, v_2, \dots, v_n)$ ist OB von $(V, γ)$
  #+end_proof
  #+begin_conc latex
  $A ∈ M(n × n, K)$ symmetrisch. Dann existiert $T ∈ \GL(n, K)$, sodass $T^T A T$ eine Diagonalmatrix.
  #+end_conc
  #+begin_proof latex
  $A$ definiert eine symmetrische Bilinearform $Δ(A) = Δ_A^{(e_1, \dots, e_n)}$ auf $K^n$ (vergleiche 20.7, $Δ(A)(v, w) = v^T A w$).
  Nach 21.6 existiert eine OB $\mathcal{B}$ von $(K^n, Δ(A)) ⇒ M_{\mathcal{B}}(Δ(A))$ ist Diagonalmatrix, und es ist
  \[M_{\mathcal{B}}(Δ(A)) = \underbrace{(T_{(e_1, \dots, e_n)}^{\mathcal{B}})^T}_{= T^T} \underbrace{M_{(e_1, \dots, e_n)}(Δ(A))}_{A} \underbrace{T_{(e_1, \dots, e_n)}^{\mathcal{B}}}_{=: T}\]
  #+end_proof
  #+begin_conc latex
  $(V, γ)$ quadratischer Raum, $n = \dim V, r = \Rang(γ)$. Dann existieren $λ_1, \dots, λ_r ∈ K \setminus \{0\}$ und ein Isomorphismus von quadratischen Räumen
  \[Φ: (K^n, Δ(\begin{pmatrix}λ_1 &   &   &   & 0 &   \\   & \ddots &   &   &   &   \\   &   & λ_r &   &   &   \\   &   &   & 0 &   &   \\   &   &   &   & \ddots &   \\   & 0  &   &   &  & 0 \end{pmatrix})) \to  (V, γ)\]
  #+end_conc
  #+begin_proof latex
  Wegen 21.6 existiert eine OB $\mathcal{B} = (v_1, \dots, v_n)$ von $(V, γ)$. Nach Umordnung von $v_1, \dots, v_n$ sei $γ(v_i, v_i) \neq 0$ für $i = 1, \dots, s$ und $γ(v_i, v_i) = 0$ für $i = s + 1, \dots, n$
  \[⇒ M_{\mathcal{B}}(γ) = \begin{pmatrix} λ_1 & & & & & 0 \\ & \ddots & & & & \\ & & λ_s & & & \\ & & & 0 & & \\ & & & & \ddots & \\ 0 & & & & & 0\end{pmatrix} \quad λ_1, \dots, λ_s ∈ K\setminus \{0\}, r = \Rang(γ) = \Rang M_{\mathcal{B}}(γ) = s\]
  Setze $Φ:= Φ_{\mathcal{B}}: K^n \to V, e_i ↦ v_i$ (Koordinatensystem zu $\mathcal{B}$, vegleiche 15.2). $Φ$ ist Isomorphismus
  \begin{align*}
  γ(Φ_{\mathcal{B}}(v), Φ_{\mathcal{B}}(w)) &= Φ_{\mathcal{B}}^{-1}(Φ_{\mathcal{B}}(v))^T M_{\mathcal{B}}(γ) Φ_{\mathcal{B}}^{-1}(Φ_{\mathcal{B}}(w)) = v_t M_{\mathcal{B}}(γ) w \\
  &= v^T \begin{pmatrix} λ_1 & & & & & 0 \\ & \ddots & & & & \\ & & λ_r & & & \\ & & & 0 & & \\ & & & & \ddots & \\ 0 & & & & & 0\end{pmatrix} w = Δ(\begin{pmatrix}λ_1 &   & 0 \\   & \ddots &   \\ 0 &   & λ_r\end{pmatrix})(v, w)
  \end{align*}
  #+end_proof
  #+begin_note latex
  $λ_1, \dots, λ_r$ sind im allgemeinen nicht eindeutig bestimmt.
  #+end_note
  *Frage:* Kann man über speziellen Körpern mehr sagen? Wir werden $K = ℂ, ℝ$ untersuchen.
  #+begin_thm latex
  $(V, γ)$ quadratischer Raum über $ℂ, n = \dim V, r = \Rang γ$. Dass existiert eine Orthogonalbasis $\mathcal{B}$ von $(V, γ)$ mit
  \[M_{\mathcal{B}}(γ) = \begin{pmatrix}E_r & 0 \\ 0 & 0\end{pmatrix}\]
  Insbesondere existiert ein Isomorphismus quadratischer Räume $Φ(ℂ^n, Δ(\begin{pmatrix}E_r & 0 \\ 0 & 0\end{pmatrix})) \to (V, γ)$
  #+end_thm
  #+begin_proof latex
  Sei $(\tilde v_1, \dots, \tilde v_n)$ eine Orthogonalbasis von $(V, γ)$. Setze
  \[v_i := \begin{cases} \tilde v_i & γ(\tilde v_i, \tilde v_i) = 0 \\ \frac{1}{\sqrt{\tilde v_i, \tilde v_i}} \tilde v_i & γ(\tilde v_i, \tilde v_i) \neq 0 \end{cases}\]
  Hierber ist $\sqrt{γ(\tilde v_i, \tilde v_i)}$ eine komplexe Zahl	$α$ mit $α^2 = γ(\tilde v_i, \tilde v_i)$. Falls $γ(\tilde v_i, \tilde v_i) \neq 0$, dass ist
  \[γ(v_i, v_i) = γ(\frac{1}{\sqrt{γ(\tilde v_i, \tilde v_i)}}, \frac{1}{\sqrt{γ(\tilde v_i, \tilde v_i)}}) = \frac{1}{γ(\tilde v_i, \tilde v_i)} γ(\tilde v_i, \tilde v_i) = 1\]
  Außerdem: $γ(v_i, v_j) = 0 ∀ i \neq j$, da $γ(\tilde v_i, \tilde v_j) = 0 ∀ i \neq 0$.
  Setze $\mathcal{B} := (v_1, \dots, v_n)$. Nach eventueller Umnummerierung von $v_1, \dots, v_n$ ist
  \[M_{\mathcal{B}}(γ) = \begin{pmatrix}E_r & 0 \\ 0 & 0\end{pmatrix}\]
  wobei $r = \Rang M_{\mathcal{B}}(γ) = \Rang γ$.
  #+end_proof
  #+begin_conc latex
  $A ∈ M(n × n, ℂ)$ symmetrisch, $r = \Rang A$. Dass existiert ein $T ∈ \GL(n, ℂ)$, sodass
  \[T^T A T = \begin{pmatrix}E_r & 0 \\ 0 & 0\end{pmatrix}\]
  #+end_conc
  #+ATTR_LATEX: :options [21.11]
  #+begin_conc latex
  $(V, γ_V), (W, γ_W)$ quadratische Räume über $ℂ$. Dann sind äquivalent:
  1. Es gibt einen Isomorphismus quadratischer Räume $(V, γ_V) \to (W, γ_W)$
  2. $\dim V = \dim W$ und $\Rang γ_V = \Rang γ_W$
  #+end_conc
  #+begin_proof latex
  1. $⇒$ 2. vergleiche Anmerkung nach 21.5
  2. $⇒$ 1. Sei $n = \dim V = \dim W, r = \Rang γ_V = \Rang γ_W$. $⇒ (V, γ_V), (W, γ_W)$ sind als quadratische Räume isomorph zu $(ℂ^n, Δ(\begin{pmatrix}E_r &   \\   &  \end{pmatrix}))$,
	 also auch $(V, γ_V) \cong (W, γ_W)$
  #+end_proof
  #+begin_defn latex
  $(V, γ)$ quadratischer Raum, $U_1, \dots, U_m ⊆ V$ UVR mit $V = U_1 \oplus \dots \oplus U_n$. Die direkte Summe heißt
  *orthogonale direkte Summe*
  \[(V = U_1 \hat oplus \dots \hat \oplus U_m) \xLeftrightarrow{\text{Def}} γ(u_i, u_j) = 0 ∀ u_i ∈ U_i, u_j ∈ U_j, i \neq j\]
  alternativ $\operp$
  #+end_defn
  #+begin_thm latex
  $(V, γ)$ quadratischer Raum über $ℝ, n = \dim V$. Dann existiert eine Orthogonalbasis $\mathcal{B}$ von $(V, γ)$, sowie $r_+, r_- ∈ \{0, \dots, \dim V\}$ mit
  \[M_{\mathcal{B}}(γ) = \begin{pmatrix}E_{r_+} &   & 0 \\   & -E_{r_-} &   \\ 0 &   & 0\end{pmatrix}\]
  Insbesondere existiert ein Isomorphismus quadratischer Räume
  \[(ℝ^n, Δ(\begin{pmatrix}E_{r_+} &   & 0 \\   & -E_{r_-} &   \\ 0 &   & 0\end{pmatrix})) \to (V, γ)\]
  Die Zahlen $r_+, r_-$ sind unabhängig von der Wahl einer solchen Basis. Wir nennen $\Signatur(γ) := (r_+, r_-)$ heißt die *Signatur* von $γ$.
  #+end_thm
  #+begin_proof latex
  1. Sei $(\tilde v_1, \dots, \tilde v_n)$ eine Orthogonalbasis von $(V, γ)$. Wir setzen
	 \[v_i := \begin{cases} \tilde v_i & γ(\tilde v_i, \tilde v_i) = 0 \\ \frac{1}{\sqrt{\abs{γ(\tilde v_i, \tilde v_i)}}} & γ(\tilde v_i, \tilde v_i) \neq 0\end{cases}\]
	 Falls $γ(\tilde v_i, \tilde v_i) \neq 0$, dass ist
	 \begin{align*}
	 γ(v_i, v_i) &= γ(\frac{1}{\sqrt{\abs{γ(\tilde v_i, \tilde v_i)}}} \tilde v_i, \frac{1}{\sqrt{\abs{γ(\tilde v_i, \tilde v_i)}}} \tilde v_i) \\
	 &=	\frac{1}{\abs{γ(\tilde v_i, \tilde v_i)}} γ(\tilde v_i, \tilde v_i) ∈ \{\pm 1\}
     \end{align*}
	 $γ(v_i, v_j) = 0$ für $i \neq j$.
	 Setze $\mathcal{B} := (v_1, \dots, v_n)$. Nach eventueller Umnummerierung von $v_1, \dots, v_n$ ist
	 \[M_{\mathcal{B}}(γ) = \begin{pmatrix} 1 & & & & & & & & \\ & \ddots & & & & & & & \\ & & 1 & & & & & & \\ & & & -1 & & & & & \\ & & & & \ddots & & & & \\ 1 & & & & & -1 & & & \\ & & & & & & 0 & & \\ & & & & & & & \ddots & \\ & & & & & & & & 0 \end{pmatrix} = \begin{pmatrix}E_{r_+} &   & 0 \\   & -E_{r_-} &   \\ 0 &   & 0\end{pmatrix}\]
	 mit geeigneten $r_+, r_- ∈ \{0, \dots, n\}$
  2. $r_+, r_-$ sind basisunabhängig: Es ist $r_+ + r_- = \Rang γ$, dies ist basisunabhängig. Es gilt zu zeigen: $r_+$ ist basisunabhängig.
	 Setze $V_+ := \Lin((v_1, \dots, v_{r_+})), V_- = \Lin((v_{r_+ + 1} ,\dots, v_{r_+ + r_-})), V_0 := \Lin((v_{r + + r_- + 1}, \dots, v_n))$
	 $⇒ V = V_+ \hat \oplus V_- \hat \oplus V_0$. Setze
     \[s := \max\{\dim W \mid W ⊆ V \text{ UVR mit } γ(w, w) > 0 ∀ w ∈ W, w \neq 0\}\]
     dies ist wohldefiniert. $V_+$ ist ein UVR von $V$ mit $γ(w, w) > 0 ∀ w ∈ V_+, w \neq 0$, denn für $w = λ_1 v_1 + \dots + λ_{r_+} v_{r_+}$ ist
	 \[γ(w, w) = λ_1^2 \underbrace{γ(v_1, v_1)}_{= 1} + \dots + λ_{r_+}^2\underbrace{v_{r_+}, v_{r_+}}_{= 1} = λ_1^2 + \dots + λ_{r_+}^2 > 0 \text{ falls } w \neq 0\]
	 $⇒ s \geq \dim V_+ = r_+$
	 Annahme: Es existiert ein UVR $W ⊆ V$ mit $γ(w, w) > 0 ∀ w ∈ W, w \neq 0$ und $\dim W > r_+$
     \[⇒ \underbrace{\dim W}_{> r_+} + \underbrace{\dim V_-}_{= r_-} + \underbrace{\dim V_0}_{n - (r_+ + r_-)} > n\]
     \begin{align*}
     ⇒ \dim(W ∩ (V_- \hat\oplus V_0)) &= \dim W + \dim(V_- \hat\oplus V_0) - \dim(W + (W_- \hat\oplus V_0)) \\
     &= \underbrace{\dim W + \dim V_- + \dim V_0}_{> n} - \underbrace{\dim(W + (V_- \hat\oplus V_0))}_{\mathclap{\leq n, \text{ da } W + (V_- \hat\oplus W_0) \text{ UVR von } V}} \\
     &= >0
     \end{align*}
     $⇒$ Es existiert $w ∈ W, w \neq 0$ mit $w ∈ W_- \hat \oplus V_0$. \\
     $⇒$ Es existiert $w_- ∈ V_-, w_0 ∈ V_0$ mit $w = w_- + w_0$ \\
     $⇒$ $γ(w, w) = γ(w_- + w_0, w_- + w_0) = \underbrace{γ(w_-, w_-)}_{< 0} + \underbrace{γ(w_0, w_0)}_{= 0} < 0$
	 Andererseits: $γ(w, w) > 0$ wegen $w ∈ W, w \neq 0 \lightning$. Somit: $r_+ = s$, insbesondere unabhängig von Basiswahl.
  #+end_proof
  #+ATTR_LATEX: :options [Sylvesterscher Trägheitssatz]
  #+begin_concdef latex
  $A ∈ M(n × n, ℝ)$ symmetrisch. Dann existieren $T ∈ \GL(n, ℝ), r_+, r_- ∈ \{0, \dots, n\}$ mit
  \[T^T A T = \begin{pmatrix}E_{r_+} &   & 0 \\   & -E_{r_-} &   \\ 0 &   & 0\end{pmatrix}\]
  Die Zahlen $r_+, r_-$ sind unabhängig von der Wahl eines solchen $T$. $\Signatur(A) := (r_+, r_-)$ heißt *Signatur* von $A$.
  #+end_concdef
  #+begin_proof latex
  folgt aus 21.13 (analog zum Beweis von 21.7).
  #+end_proof
  #+begin_note latex
  Ist $S ∈ \GL(n, ℝ)$, dann haben die Matrixen $A$ und $S^T A S$ diesselbe Signatur, denn: Ist $\tilde T ∈ \GL(, ℝ)$ mit \[\tilde T^T(S^T A S) T = \begin{pmatrix}E_{r_+} &   & 0 \\   & -E_{r_-} &   \\ 0 &   & 0\end{pmatrix}\], dann ist
  \[(S\tilde T)^T A (S\tilde T) = \begin{pmatrix}E_{r_+} &   & 0 \\   & -E_{r_-} &   \\ 0 &   & 0\end{pmatrix}\]
  #+end_note
  #+begin_conc latex
  $(V, γ_V), (W, γ_W)$ quadratische Räume über $ℝ$. Dann sind äquivalent:
  1. Es gibt einen Isomorphismus quadratischer Räume $(V, γ_V) \to (W, γ_W)$
  2. $\dim V = \dim W$ und $\Signatur(γ_V) = \Signatur(γ_W)$
  #+end_conc
  #+begin_proof latex
  1. $⇒$ 2. Für $\Signatur(γ_V) = \Signatur(γ_W)$ verwende Charakterisierung von $r_+$ aus dem Beweis von 21.3.
  2. $⇒$ 1. aus 21.13, analog zum Beweis von 21.11
  #+end_proof
  #+begin_note latex
  Man kann Folgerung 21.11/21.15 verwenden, um quadratische Formen über $ℂ$ beziehungsweise $ℝ$ bis auf Äquivalenz zu klassifizieren (vergleiche Übungen)
  #+end_note
* Euklidische Räume
  #+begin_defn latex
  $V ℝ$ -VR, $γ: V × V \to ℝ$ symmetrische Bilinearform. $γ$ heißt
  - *positiv definit* $\xLeftrightarrow{\text{Def}} γ(v, v) > 0 ∀ v ∈ V \setminus\{0\}$
  - *positiv semidefinit* $\xLeftrightarrow{\text{Def}} γ(v, v) \geq 0 ∀ v ∈ V \setminus\{0\}$
  - *negativ definit* $\xLeftrightarrow{\text{Def}} γ(v, v) < 0 ∀ v ∈ V \setminus\{0\}$
  - *negativ semidefinit* $\xLeftrightarrow{\text{Def}} γ(v, v) \leq 0 ∀ v ∈ V \setminus\{0\}$
  - *indefinit* $\xLeftrightarrow{\text{Def}} γ$ ist weder positiv noch negativ semidefinit.
  Eine positiv definite symmetrische Bilinearform nennt man auch ein *Skalarprodukt*.
  #+end_defn
  #+begin_ex latex
  1. $V = ℝ^n, <·,·>:ℝ^n × ℝ^n \to ℝ, <\cvec{x_1; \vdots; x_n}, \cvec{y_1; \vdots; y_n}> := x_1 y_1 + \dots + x_n y_n$
	 ist ein Skalarprodukt auf dem $ℝ^n$. Positiv Definitheit:
	 \[<\cvec{x_1; \vdots; x_n}, \cvec{x_1; \vdots; x_n}> = x_1^2 + \dots + x_n^2 > 0, \text{ falls } \cvec{x_1; \vdots; x_n} \neq 0\]
	 $<·, ·>$ heißt das *Standardskalarprodukt* auf dem $ℝ^n$.
  2. $V= \mathcal{C}[0, 1]$
	 \[γ: \mathcal{C}[0, 1] × \mathcal{C}[0, 1] \to ℝ, (f, g) ↦ ∫_0^1 f(t) g(t) \d t\]
	 ist ein Skalarprodukt.
  #+end_ex
  #+begin_note latex
  Um die Definitheit einer symmetrischen Bilinearform nachzuweisen, genügt es nicht, das Verhalten auf den Basisvektoren zu untersuchen:
  Sei $γ: ℝ^2 × ℝ^2 \to ℝ$ gegeben durch
  \[γ = Δ(\begin{pmatrix}1 & -1 \\ -2 & 1\end{pmatrix})\]
  das heißt
  \[M_{(e_1, e_2)}(γ) = \begin{pmatrix}1 & -2 \\ -2 & 1\end{pmatrix}\]
  Dann ist $γ(e_1, e_1) = 1, γ(e_2, e_2) = 1$ aber
  \[γ(\cvec{1;1}, \cvec{1; 1}) = \begin{pmatrix}1 & 1\end{pmatrix} \begin{pmatrix}1 & -2 \\ -2 & 1\end{pmatrix} \begin{pmatrix}1 \\ 1\end{pmatrix} = -2 < 0\]
  das heißt $γ$ ist indefinit.
  #+end_note
  #+begin_defn latex
  Ein *Euklidischer Raum* ist ein Paar $(V, γ)$, bestehend aus einem endlichdimensionalen $ℝ$ -VR $V$ und einem Skalarprodukt $γ$ auf $V$.
  Für den Rest dieses Abschsittes sei $(V, γ)$ ein Euklidischer Raum.
  #+end_defn
  #+begin_defn latex
  $v ∈ V$
  \[\norm{v} := \sqrt{γ(v, v)}\]
  heißt die *Norm* auf $V$. \\
  $(v_i)_{i ∈ I}$ Familie von Vektoren aus $V$ heißt *orthonormal* $\xLeftrightarrow{\text{Def}} (v_i)_{i ∈ I}$ ist orthogonal und $\norm{v_i} = 1 ∀ i ∈ I$. \\
  $\mathcal{B} = (v_1, \dots, v_n)$ heißt *Orthonormalbasis von $V ((V, γ))$ (ONB) $⇔ \mathcal{B}$ ist Basis von $V$ und $\mathcal{B}$ ist orthonormal.
  #+end_defn
  #+begin_remark latex
  $(v_1, \dots, v_n)$ orthogonale Familie von Vektoren aus $V \setminus \{0\}$. Dann gilt:
  1. $(\frac{v_1}{\norm{v_1}}, \dots, \frac{v_n}{\norm{v_n}})$ ist eine orthonormale Familie
  2. $(v_1, \dots, v_n)$ ist linear unabhängig.
  #+end_remark
  #+begin_proof latex
  1. $\norm{v_i}^2 = γ(v_i, v_i) \neq 0$, da $γ$ positiv definit und $v_i \neq 0$.
	 \[γ(\frac{v_i}{\norm{v_i}}, \frac{v_j}{\norm{v_j}}) = \frac{1}{\norm{v_i}\norm{v_j}} γ(v_i, v_j) = \begin{cases} 0 & i \neq j \\ \frac{γ(v_i, v_i)}{\norm{v_i}^2} = 1 & i = j\end{cases}\]
  2. Sei $λ_1 v_1 + \dots + λ_n v_n = 0$
	 \begin{align*}
	 ⇒ λ_1 γ(v_1, v_i) + \dots + λ_n γ(v_n, v_i) &= 0 \\
	 ⇒ λ_i &= 0
     \end{align*}
  #+end_proof
  #+begin_remark latex
  Es gilt:
  1. $(V, γ)$ besitzt eine Orthonormalbasis
  2. $γ$ ist nicht-ausgeartet
  3. Es gibt eine Basis $\mathcal{B}$ von $V$ mit $M_{\mathcal{B}}(γ) = E_n$, wobei	$n = \dim V$
  #+end_remark
  #+begin_proof latex
  Der quadratische Raum $(V, γ)$ hat eine Orthogonalbasas $(v_1, \dots, v_n)$
  \[⇒ \mathcal{B} := (\frac{v_1}{\norm{v_1}}, \dots, \frac{v_n}{\norm{v_n}})\]
  ist eine Orthonormalbasis von $(V, γ)$. Es ist $M_{\mathcal{B}}(γ) = E_n$ ($⇒$ 3.),
  insbesodere ist $M_{\mathcal{B}}(γ)$ invertierbar $⇒ γ$ nich ausgeartet $⇒$ 2.
  #+end_proof
  #+begin_remark latex
  $\mathcal{B} = (v_1, \dots, v_n)$ Orthonormalbasis von $(V, γ), v ∈ V$. Dann gilt: Ist $v = λ_1 v_1 + \dots + λ_n v_n$, dann ist $λ_i = γ(v, v_i) ∀ i = 1, \dots, n$
  #+end_remark
  #+begin_proof latex
  $γ(v, v_i) = λ_1 γ(v_1, v_i) + \dots + λ_n γ(v_n, v_i) = λ_i \underbrace{γ(v_i, v_i)}_{= 1} = λ_i$
  #+end_proof
  #+begin_remdef latex
  $U ⊆ V$ Untervektorraum.
  \[U^{\perp} := \{v ∈ V \mid γ(v, u) = 0 ∀ u ∈ U\}\]
  heißt das *orthogonale Komplement* zu $U$. $U^{\perp}$ ist ein Untervektorraum von $V$.
  #+end_remdef
  #+begin_proof latex
  leicht nachzurechnen
  #+end_proof
  #+begin_defthm latex
  $U ⊆ V$ Untervektorraum. Dann gilt:
  1. $V = U \oplus U^{\perp}$
  2. $\dim U^{\perp} = \dim V - \dim U$
  3. $(U^{\perp})^{\perp} = U$
  4. Ist $(u_1, \dots, u_m)$ eine Orthogonalbasis von $(U, γ\mid_{U × U})$, und ist $v ∈ V$ mit $v = u + v', u ∈ U, v' ∈ U^{\perp}$, dass ist
	 \[u = \sum_{j = 1}^{m} γ(v, u_j) u_j\]
	 Die lineare Abbildung
     \[π_u: V\to U, v ↦ \sum_{j = 1}^{m} γ(v, u_j) u_j\]
	 hießt die *Orthogonalprojektion* von $V$ auf $U$.
  #+end_defthm
  #+begin_proof latex
  1. $U + U^{\perp} = V$, denn: \\
	 Sei $(u_1, \dots, u_m)$ eine Orthogonalbasis von $(U, γ\mid_{n × n}), v ∈ V$. Setze
	 \begin{align*}
     v' &:= V - \sum_{j = 1}^{m} γ(v, u_j) u_j \\
	 ⇒ γ(v', u_i) &= γ(v, u_i) - \sum_{j = 1}^{m} γ(v, u_j) γ(u_j, u_i) = γ(v, u_i) - γ(v, u_i) = 0 ∀ i = 1, \dots, m \\
	 ⇒ v' &∈ U^{\perp} \\
	 ⇒ v &= \underbrace{\sum_{j = 1}^{m} γ(v, u_j) u_j}_{∈ U} + \underbrace{v'}_{\mathclap{∈ U^{\perp}}} \\
	 ⇒ V &= U + U^{\perp}
     \end{align*}
	 $U ∩ U^{\perp} = \{0\}$, denn: $u ∈ U ∩ U^{\perp} ⇒ γ(u, u) = 0 ⇒ u = 0$ (da $γ$ Skalarprodukt)
  2. aus 1., 2.
  3. Sei $u ∈ U ⇒ γ(u, w) = 0 ∀ w = U^{\perp} ⇒ u ∈ (U^{\perp})^{\perp}$, das heißt $U ⊆ U^{\perp\perp}$.
	 Wegen $\dim(U^{\perp})^{\perp} = \dim V - \dim U^{\perp} = \dim V - (\dim V - \dim U) = \dim U$ foglt $U = U^{\perp\perp}$
  #+end_proof
  #+begin_note latex
  Insbesondere gilt für alle $v ∈ V: v - π_U(v) ∈ U^{\perp}$
  #+end_note
  #+begin_ex latex
  $(V, γ) = (ℝ^2, <·,·>), U = \Lin(\cvec{1; 1}) ⇒ U^{\perp} = \Lin(\cvec{-1; 1})$, denn $\cvec{-1; 1} ∈ U^{\perp}$ wegen $<\cvec{-1; 1}, \cvec{1; 1}> = 0$, und es ist
  $\dim U^{\perp} = 2 - \dim U = 2 - 1 = 1$. Jedes Element aus $V$ lässt sich eindeutig schreiben als
  \[v = λ\cvec{1; 1} + μ\cvec{-1; 1}\]
  das heißt
  \[π_u: v = \underbrace{λ\cvec{1; 1}}_{∈ U} + μ \underbrace{\cvec{-1; 1}}_{∈ U^{\perp}} ↦ λ\cvec{1; 1} = γ(v, \cvec{1; 1})\vec{1; 1}\]
  #+end_ex
  *Frage:* Wie bestimmt man explizit eine Orthogonalbasis eines Euklidischen Raumes?
  #+ATTR_LATEX: :options [Gram-Schmidt-Verfahren]
  #+begin_algorithm latex
  *Eingabe*: $(v_1, \dots, v_n)$ Basis von $V$. \\
  *Ausgabe*: Orthonormalbasis $(w_1, \dots, w_n)$ von $(V, γ)$ \\
  *Durchführung:*
  1. Setze
     \[w_1 := \frac{v_1}{\norm{v_1}}\]
  2. Setze für $k = 2, \dots, n$
	 \[\tilde w_k := v_k - \sum_{i = 1}^{k - 1}γ(v_k, w_i) w_i, \quad w_k := \frac{\tilde w_k}{\norm{\tilde w_k}}\]
  3. $(w_1, \dots, w_n)$ ist eine Orthonormalbasis von $(V, γ)$
  #+end_algorithm
  #+begin_proof latex
  Sei $U_k := \Lin((v_1, \dots, v_k))$ für $k = 1, \dots, n$. Wir zeigen per Induktion nach $k$, dass $(w_1, \dots, w_k)$ eine Orthogonalbasis von $(U_k, γ\mid_{U_k × U_k})$ ist
  (Behauptung folgt dann aus $k = n$). \\
  Induktionsanfang: $k = 1$ klar \\
  Induktionsschritt: Sei $π_{k - 1} := π_{U_{k - 1}}: V \to V_{k - 1}$ die orthogonale Projektion.
  \[⇒ \tilde w_k = v_k - π_{k - 1}(v_k)\]
  da $(w_1, \dots, w_{k - 1})$ Orthogonalbasis von $U_{k - 1}$ nach Induktionsvorraussetzung. $⇒\tilde w_k ∈ U_{k - 1}^{\perp}$.
  Außerdem $\tilde w_k \neq 0$, da sonst $v_k = π_{k - 1}(v_k) ∈ U_{k - 1} \lightning$ zu $(v_1, \dots, v_k)$ Basis von U_k
  \[⇒ w_k = \frac{\tilde w_k}{\norm{\tilde w_k}} ∈ U_{k - 1}^{\perp}\]
  und es ist
  \[γ(w_k, w_i) = \begin{cases} 0 & i = 1, \dots, k - 1 \\ 1 & i = k\end{cases}\]
  $⇒ (w_1, \dots, w_k)$ Orthogonalbasis von $U_k$
  #+end_proof
  #+begin_ex latex
  Wir betrachten $(ℝ^3, <·, ·>), U = \Lin((v_1, v_2))$ mit $v_1 := \cvec{2; 0; 1}, v_2 := \cvec{-1; 1; 0}$. Gesucht ist eine Orthogonalbasis von $U$ bezüglich $<·, ·>$.
  Setze
  \begin{align*}
  w &:= \frac{v_1}{\norm{v_1}} = \frac{1}{\sqrt{5}}\cvec{2; 0; 1} \\
  \tilde w_2 &= v_2 - <v_2, w_1> w_1 = \cvec{-1; 1; 0} - <\cvec{-1; 1; 0}, \frac{1}{\sqrt{5}}\cvec{2; 0; 1}> \frac{1}{\sqrt{5}}\cvec{2; 0; 1} \\
  &= \cvec{-1; 1; 0} - \frac{1}{5}<\cvec{-1; 1; 0}, \cvec{2; 0; 1}>\cvec{2; 0; 1} = \cvec{-1; 1; 0} + \frac{2}{5}\cvec{2; 0; 1} = \cvec{-\frac{1}{5}; 1; \frac{2}{5}} = \frac{1}{2} \cvec{-1; 5; 2} \\
  w_2 &= \frac{\tilde w_2}{\norm{\tilde w_2}} = \frac{1}{\sqrt{30}}\cvec{-1; 5; 2}
  \end{align*}
  $⇒ (\frac{1}{\sqrt{5}}\cvec{2; 0; 1}, \frac{1}{\sqrt{30}}\cvec{-1; 5; 2})$ ist eine Orthogonalbasis von $U$.
  #+end_ex
  #+begin_defn latex
  $A ∈ M(n × n, ℝ)$ symmetrisch. $A$ heißt *positiv definit* (Notation: $A > 0$) $\xLeftrightarrow{\text{Def}}$ Die symmetrische Bilinearform
  \[Δ(A): ℝ^n × ℝ^n \to ℝ, (x, y) ↦ x^T A y\]
  ist positiv definit.
  #+end_defn
  #+begin_remark latex
  $A ∈ M(n × n, ℝ)$ symmetrisch. Dass sind äquivalent:
  1. $A > 0$
  2. $∃ T ∈ \GL(n, ℝ): A = T^T T$
  #+end_remark
  #+begin_proof latex
  1. $⇒$ 2. Sei $A > 0 ⇒ (ℝ^n, Δ(A))$ Euklidischer Raum. Sei $\mathcal{B}$ Orthogonalbasis von $(ℝ^n, Δ(A))$ $T := T_{\mathcal{B}}^{(e_1, \dots, e_n)}$
	 \[⇒ E_n = M_{\mathcal{B}}(Δ(A)) = \underbrace{(T_{(e_1, \dots, e_n)}^{\mathcal{B}})^T}_{= (T^{-1})^T} \underbrace{M_{(e_1, \dots, e_n)}(Δ(A))}_{= A} \underbrace{T_{(e_1, \dots, e_n)}^{\mathcal{B}}}_{= T^{-1}}\]
	 $⇒ A = T^T T$
  2. Sei $A = T^T T$ für ein $T ∈ \GL(n, ℝ)$. Für $x ∈ ℝ^n, x \neq 0$ ist
	 \[Δ(A)(x, x) = x^t A w = x^t T^t T x = (Tx)^T Tx = <Tx, Tx> > 0\]
  #+end_proof
  #+begin_note latex
  1., 2. sind äquivatent zu
  3. [@3] Es existiert eine obere Dreiecksmatrix $P$ mit Diagonaleinträgen, sodass $A = P^T P$ (siehe Übungen). Obiges $P$ ist sogar eindeutig bestimmt, eine solche Zerlegung heißt Cholesky-Zerlegung.
  #+end_note
  #+ATTR_LATEX: :options [Cauchy-Schwarz-Ungleichung]
  #+begin_thm latex
  $v, w ∈ V$. Dann gil:
  \[\abs{γ(v, w)} \leq \norm{v} \norm{w}\]
  Gleichheit gilt hierbar genau dann, wenn $(v, w)$ linear abhängig.
  #+end_thm
  #+begin_proof latex
  1. Beweis der Ungleichung: Falls $w = 0$, dass fertig. Im Folgenden sei $w \neq 0$. Für $λ, μ ∈ ℝ$ ist
	 \begin{align*}
	 0 &\leq γ(λv + μw, λv + μw) = λ^2γ (v, v) + μ^2 γ(w, w) + 2 λ μ γ(v, w) \\
	 \intertext{Setze $λ := γ(w, w) > 0$, dividiere durch $λ$}
	 0 &\leq γ(v, v) γ(w, w) + μ^2 + 2μ γ(v, w) \\
	 \intertext{Setze $μ := -γ(v, w)$}
	 0 &\leq γ(v, v)γ(w, w) + γ(v, w)^2 - 2γ(v, w)^2 \\
	 γ(v, w)^2 &\leq γ(v, v)γ(w, w) \\
	 \abs{γ(v, w)} &\leq \norm{v} \norm{w} \\
     \end{align*}
  2. Gleichheitsaussage: Für $w = 0$: $(v, w)$ linear abhängig und "$=$" gilt. Ab jetzt also $w \neq 0$. \\
	 "$\impliedby$" Sei $(v, w)$ linear abhängig $⇒ ∃ λ ∈ K: v = κw$
	 \[⇒ \abs{γ(v, w)}^2 = \abs{γ(λw, w)}^2 = \abs{λ^2}\abs{γ(w, w)}^2 = \abs{γ(w, w)}\abs{γ(λw, λw)} = \norm{w}^2 \norm{λw}^2\]
	 $⇒ \abs{γ(v, w)} = \norm{w}\norm{λ w} = \norm{w}\norm{v}$. \\
	 "$⇒$" Es gelte, sei also $\abs{γ(v, w)} = \norm{v}\norm{w}$. Führe die Rechnung wie in 1. rückwärts durch: Mit $λ := γ(w, w), μ = -γ(v, w)$ folgt
	 dass
     \[γ(λv + μw, λv + μw) = 0 ⇒ λv + μw = 0 ⇒ (v, w)\text{ linear abhängig}\]
  #+end_proof
  #+ATTR_LATEX: :options [Eigenscaften der Norm]
  #+begin_remark latex
  $v, w ∈ V, λ ∈ ℝ$. Dann gilt:
  1. $\norm{v} = 0 ⇔ v = 0$
  2. $\norm{λ v} = \abs{λ} \norm{v}$
  3. $\norm{v + w} \leq \norm{v} + \norm{w}$
  #+end_remark
  #+begin_proof latex
  1. klar, da $γ$ positiv definit
  2. $\norm{λ v}^2 = γ(λ v, λ v) = λ^2 γ(v, v) = λ^2 \norm{v} ⇒ \norm{λ v} = \abs{λ} \norm{v}$
  3.
     \begin{align*}
	 \norm{v + w}^2 &= γ(v + w, v + w) = \norm{v}^2 + \norm{w}^2 + 2γ(v, w) \leq \norm{v}^2 + \norm{w}^2 + 2\abs{γ(v, w)} \\
	 &\leq \norm{v}^2 + \norm{w}^2 + 2 \norm{v} \norm{w} = (\norm{v} + \norm{w})^2 \\
	 ⇒ \norm{v + w} &\leq \norm{v} + \norm{w}
     \end{align*}
  #+end_proof
  #+begin_remark latex
  $v, w ∈ V$. Dann gilt:
  1. $\norm{v + w}^2 = \norm{v}^2 + \norm{w}^2 ⇔ γ(v, w) = 0$ \hfill Satz des Pythagoras
  2. $\norm{v + w}^2 + \norm{v - w}^2 = 2\norm{v}^2 + 2\norm{w}^2$ \hfill Parallelogrammgleichung
  #+end_remark
  #+begin_proof latex
  1. $\norm{v + w}^2 = γ(v + w, v + w) = \norm{v}^2 + \norm{w}^2 + 2γ(v, w) ⇒$ Behauptung
  2. $\norm{v + w}^2 + \norm{v - w}^2 = γ(v + w, v + w) + γ(v - w, v - w) = 2\norm{v}^2 + 2\norm{w}^2$
  #+end_proof
  #+begin_note latex
  $V ℝ$ Vektorraum. Eine Abbildung $\norm{·}: V \to ℝ_{\geq 0}$ mit den Eigenschaften 1. bis 3. aus 22.16 heißt eine Norm auf $V$, $(V, \norm{·})$ ein normierter Vektorraum.
  Man kann zeigen: Ist $(V, \norm{·})$ ein normierter Vektorraum, in dem die Parallelogrammgleichung gilt, dann	ist durch
  \[γ(v, w) := \frac{1}{2}(\norm{v + w}^2 - \norm{v}^2 - \norm{w}^2)\]
  ein Skalarprodukt auf $V$ mit $\norm{v} = \sqrt{γ(v, v)}$, das heißt in diesen Fällen ist $(V, γ)$ ein euklidischer Vektorraum, dessen Norm mit die gegebenen übereinstimmt.
  #+end_note
* Die orthogonale Gruppe
  #+begin_defn latex
  $(V, γ_V), (W, γ_W)$ Euklidische Räume, $φ: V \to W$ lineare Abbildung. $φ$ heißt *orthogonal* $\xLeftrightarrow{\text{Def}}$ $φ$ ist ein Homomorphismus quadratischer Räume, das heißt
  \[γ_W(φ(v_1), φ(v_2)) = γ_V(v_1, v_2) ∀ v_1, v_2 ∈ V\]
  #+end_defn
  #+begin_remark latex
  $(V, γ_V), (W, γ_W)$ Euklidische Räume, $φ: V \to W$ orthogonale Abbildung. Dann gilt:
  1. $\norm{φ(v)}_W = \norm{v}_V ∀ v ∈ V$
  2. $v_1 \perp v_2 ⇔ φ(v_1) \perp φ(v_2) ∀ v_1, v_2 ∈ V$
  3. $φ$ ist injektiv
  #+end_remark
  #+begin_proof latex
  1. $\norm{φ(v)}_W^2 = γ_W(φ(v), φ(v)) = γ_V(v, v) = \norm{v}_V^2$
  2. $v_1 \perp v_2 ⇔ γ_V(v_1, v_2) = 0 ⇔ γ_W(φ(v_1), φ(v_2)) = 0 ⇔ φ(v_1) \perp φ(v_2)$
  3. Sei $v ∈ V$ mit $φ(v) = 0 ⇒ \norm{φ(v)}_W = 0 ⇒ \norm{v}_V = 0 ⇒ v = 0$
  #+end_proof
  #+begin_remark latex
  $(V, γ)$ Euklidischer Raum, $n = \dim V, \mathcal{B}$ Orthogonalbasis von $(V, γ)$. Dann ist das Koordinatensystem $Φ_{\mathcal{B}}: (ℝ^n, <·,·>) \to (V, γ)$ ein
  orthogonaler Isomorphismus.
  #+end_remark
  #+begin_proof latex
  $Φ_{\mathcal{B}}$ Isomorphismus: klar. $Φ_{\mathcal{B}}$ orthogonal, denn: Sei $\mathcal{B} = (v_1, \dots, v_n)$ dann ist
  \[γ(Φ_{\mathcal{B}}(e_i), Φ_{\mathcal{B}}(e_j)) = γ(v_1, v_j) = δ_{ij} = <e_i, e_j>\]
  #+end_proof
  #+begin_remark latex
  $(V, γ)$ Euklidischer Raum, $φ ∈ \End(V)$ orthogonal. Dann gilt:
  1. $φ$ ist Isomorphismus
  2. $φ^{-1}$ ist orthogonal
  3. $λ ∈ ℝ$ Eigenwert von $γ ⇒ \abs{λ} = 1$, das heißt $λ ∈ \{\pm 1\}$
  #+end_remark
  #+begin_proof latex
  1. aus 23.2.3 folgt: $φ$ injektiv $⇒$ $φ$ Isomorphismus
  2. $v_1, v_2 ∈ V ⇒ γ(φ^{-1}(v_1), φ^{-1}(v_2)) = γ(φ(φ^{-1}(v_1)), φ(φ^{-1}(v_2))) = γ(v_1, v_2)$ $⇒$ $φ^{-1}$ orthogonal
  3. Sei $v ∈ V$ Eigenvektor zum Eigenwert $λ ⇒ \norm{v} = \norm{φ(v)} = \norm{λv} = \abs{λ} \norm{v} ⇒ \abs{λ} = 1$
  #+end_proof
  #+begin_remark latex
  $(V, γ)$ Euklidischer Raum, $n = \dim V, \mathcal{B}$ Orthogonalbasis von $V, φ ∈ \End(V), A = M_{\mathcal{B}}(φ)$. Dann sind äquivalent:
  1. $φ$ ist orthogonal
  2. $A^T A = E_n$
  #+end_remark
  #+begin_proof latex
  Wir erhalten kommutierendes Diagramm
  #+begin_export latex
  \catcode`(=12
  \catcode`)=12
  #+end_export
  \begin{figure}[H]
	 \centering
  \begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=3em] {(V, γ) & (V, γ) \\ (ℝ^n,<·,·>) & (ℝ^n, <·,·>)\\};
  \path[-stealth]
  (m-1-1) edge node [left] {$φ$} (m-2-1)
  (m-1-2) edge node [below] {$Φ_{\mathcal{B}}$} (m-1-1)
  (m-2-2) edge node [below] {$Φ_{\mathcal{B}}$} (m-2-1)
  (m-1-2) edge node [left] {$φ$} (m-2-2);
  \end{tikzpicture}
  \end{figure}
  #+begin_export latex
  \catcode`(=\active
  \catcode`)=\active
  #+end_export
  Da $Φ_{\mathcal{B}}$ orthogonaler Isomorphismus nach 23.3 folgt:
  \begin{align*}
  φ \text{ orthogonal } &⇔ \tilde A = Φ_{\mathcal{B}}^{-1} = φ\circ Φ_{\mathcal{B}} \text{ orthogonal} \\
  &⇔ ∀ x, y ∈ ℝ^n: <Ax, Ay> = <x, y> \\
  &⇔ ∀ x, y ∈ ℝ^n: (Ax)^T Ay = x^T y \\
  &⇔ ∀ x, y ∈ ℝ^n: <Ax, Ay> = x^T A^T A y = x^T y \\
  &⇔ Δ(A^T A) = Δ(E_n) \\
  &⇔ A^T A = E_n
  \end{align*}
  #+end_proof
  #+begin_remdef latex
  $A$ heißt *orthogonal* $\xLeftrightarrow{\text{Def}} A^T A = E_n$
  \[O(n) :=\{A ∈ M(n × n, ℝ) \mid A \text{ ist orthogonal }\}\]
  $O(n)$ ist bezüglich die Matrixmultiplikation eine Gruppe, die *orthogonale Gruppe* vom Rang $n$
  #+end_remdef
  #+begin_proof latex
  Wohldefiniertheit von "$·$" (das heißt Abgeschlossenheit bezüglich "$·$"): $A, B ∈ O(n) ⇒ (AB)^T AB = B^T A^T A B = B^T B = E_n ⇒ AB ∈ O(n)$. \\
  Existenz des neutralen Elements: $E_n ∈ O(n)$ \\
  Assoziativität: klar \\
  Existenz von Inversen: Sei $A ∈ A(n) ⇒ A^T A = E_n ⇒ A^{-1} = A^t ⇒ (A^{-1})^T A^{-1} = (A^T)^T A^T = A A^T = A A^{-1} = E_n$
  #+end_proof
  #+begin_note latex
  $A ∈ O(n) ⇒ \det(A) ∈ \{\pm 1\}$, denn $1 = \det(E_n) = \det(A^T A) = \det(A^T)\det(A) = \det(A)^2$
  #+end_note
  #+begin_remark latex
  $A ∈ M(n × n, ℝ)$. Dann sind äquivalent:
  1. $A ∈ O(n)$
  2. $A A^T = E_n$
  3. $A^T A = E_n$
  4. Die Transponierten der Zeilen von $A$ bilden eine Orthogonalbasis von $(ℝ^n, <·,·>)$
  5. Die Spalten von $A$ bilden eine Orthogonalbasis von $(ℝ^n, <·,·>)$
  6. Die Abbildung $\tilde A: (ℝ^n, <·,·>) \to (ℝ^n, <·,·>)$ ist orthogonal
  #+end_remark
  #+begin_proof latex
  1. $⇔$ 2. $⇔$ 3. $⇔$ klar
  2. $⇔$ 4., 3. $⇔$ 5.
  1. [@1] $⇔$ 6. aus 23.5 (setze $V = (ℝ^n, <·, ·>), \mathcal{B} = (e_1, \dots, e_n)$)
  #+end_proof
  #+begin_thm latex
  $φ: ℝ^n \to ℝ^n$ (nicht notwendig linear) abstandstreu, das heißt
  \[\norm{φ(x) - φ(y)} = \norm{x - y} ∀ x, y ∈ ℝ^n\]
  wobie $\norm{·}$ die Norm auf $(ℝ^n, <·,·>)$ bezeichne. Dann existieren eindeutig bestimmte $A ∈ O(n), b ∈ ℝ^n$, sodass
  \[φ(x) = Ax + b\]
  für alle $x ∈ ℝ^n$
  #+end_thm
  #+begin_remdef latex
  $SO(n) := \{A ∈ O(n) \mid \det A = 1\}$ ist eine Untergruppe von $O(n)$ (das heißt $SO(n) ⊆ O(n)$ und ist eine Gruppe bezüglich der eingeschränkten Verknüpfung),
  die *spezielle orthogonale Gruppe* vom Rang $n$.
  #+end_remdef
  #+begin_proof latex
  Wohldefiniertheit von "$·$" (= Abgeschlossenheit bezüglich "$·$")
  \[A, B ∈ SO(n) ⇒ AB ∈ O(n) ∧ \det(AB) = \det(A)\det(B) = 1·1 = 1\]
  neutrales Element: $E_n ∈ SO(n)$ \\
  Assoziativität: klar \\
  Existenz von Inversem: $A ∈ SO(n) ⇒ A^{-1} ∈ O(n), \det(A^{-1}) = \det(A)^{-1} = 1 ⇒ A^{-1} ∈ SO(n)$
  #+end_proof
  #+begin_ex latex
  $n = 1: O(1) = \{\pm 1\}, SO(1) = \{0\}$
  #+end_ex
  #+begin_remark latex
  $A ∈ O(2)$. Dann gilt:
  1. $A ∈ SO(2) ⇔ ∃! α ∈ [0, 2π]$ mit
	 \[A = \begin{pmatrix}\cos α & -\sin α \\ \sin α & \cos α \end{pmatrix}\]
	 In diesem Fall beschreibt $A$ eine Drehung mit Zentrum $0$ um den Winkel $α$. Außer im Fall $α ∈ \{0, π\}$ besitzt $A$ keine Eigenwerte. Falls $α = 0$:
	 \[A = \begin{pmatrix}1 & 0 \\ 0 & 1\end{pmatrix}\]
	 einziger Eigenwert: $1$. Falls $α = π$:
	 \[A = \begin{pmatrix}-1 & 0 \\ 0 & -1\end{pmatrix}\]
	 einziger Eigenwert: $-1$.
  2. $A ∈ O(2) \setminus SO(2) ⇔ ∃! α ∈ [0, 2π]$ mit
	 \[A = \begin{pmatrix}\cos α & \sin α \\ \sin α & -\cos α\end{pmatrix}\]
	 In diesem Fall beschreibt $A$ eine Spiegelung an der Geraden $\Lin(\cvec{\cos \frac{α}{2}; \sin \frac{α}{2}})$. $A$ besitzt die Eigenwerte $\pm 1$, und es existiert eine Orthogonalbasis $\mathcal{B}$ von
	 $(ℝ^2, <·,·>)$ mit
	 \[M_{\mathcal{B}}(\tilde A) = \begin{pmatrix}1 & 0 \\ 0 & -1\end{pmatrix}\]
  #+end_remark
  #+begin_proof latex
  Sei $A = \begin{pmatrix}a & c \\ b & d\end{pmatrix} ∈ O(2)$
  \begin{align*}
  ⇒ 1 &= \norm{e_1}^2 = \norm{Ae_1}^2 = a^2 + b^2 \\
  ⇒ 1 &= \norm{e_2}^2 = \norm{Ae_2}^2 = c^2 + d^2 \\
  \end{align*}
  Außerdem: $e_1 \perp e_2 ⇒ A e_1 \perp A e_2$
  \[⇒ <\cvec{a;b}, \cvec{c; d}> = 0\]
  \[⇒ \begin{pmatrix}a & b\end{pmatrix} \begin{pmatrix}c \\ d\end{pmatrix} = 0 ⇒ \cvec{c; d} ∈ \Lin((\cvec{-b; a}))\]
  das heißt es Existiert $λ ∈ ℝ$ mit
  \[\begin{pmatrix}c \\ d\end{pmatrix} = λ \begin{pmatrix}-b \\ a\end{pmatrix}\]
  \[⇒ A = \begin{pmatrix}a & -λ b \\ b & λa\end{pmatrix}, \det A = λ(a^2 + b^2) = λ ∈ \{\pm 1\}\]
  1. Fall: $λ = 1 ⇔ \det A = 1 ⇔ A ∈ SO(2)$
	 Wegen $a^2 + b^2 = 1$ ist $\cvec{a; b}$ ein Punkt auf dem Einheitskreis. $⇒ ∃! α ∈ [0, 2π\string)$ mit $a = \cos{α}, b = \sin{α}$.
	 Somit:
     \[A ∈ SO(2) ⇔ A = \begin{pmatrix}\cos α & -\sin α \\ \sin α & \cos α\end{pmatrix}\]
	 für eindeutig bestimmte $α ∈ [0, 2π\string)$. Sei $\cvec{x_1; x_2} = \cvec{\cos β; \sin β}$ ein Punkt auf dem Einheitskreis
	 \[A \cvec{\cos β; \sin β} = \begin{pmatrix}\cos α & -\sin α \\ \sin α & \cos α\end{pmatrix}\cvec{\cos β; \sin β} = \begin{pmatrix}\cos α \cos β - \sin α \sin β \\ \sin α \cos β + \cos α \sin β\end{pmatrix} = \cvec{\cos{α + β}; \sin{α + β}}\]
	 $⇒ A$ beschreibt eine Drehung mit Zentrum $0$ um den Winkel $α$. $A$ hat nur Eigenwerte, wenn $α = 0$ beziehungsweise $α = π$ (Eigenwert: $1$ beziehungsweise $-1$):
	 \[χ_A^{char} = t^2 - \Sp(A)t + \det A = t^2 - 2\cos α + 1\]
	 Eigenwerte: $λ_{1,2} = \cos α \pm \sqrt{\cos^2 α - 1}$, Eigenwert in $ℝ ⇔ \cos^2 α - 1 \geq 0 ⇔ α = 1$ oder $α = π$
  2. $λ= -1 ⇔ A ∈ O(2) \setminus SO(2)$:
	 \[⇔ A = \begin{pmatrix}a & b \\ b & -a\end{pmatrix}\]
	 Wegen $a^2 + b^2 = 1$ existiert genau ein $α ∈ [0, 2π\string)$ mit $a = \cos{α}, b = \sin{α}$. Sei $\cvec{x_1; x_2} = \cvec{\cos β; \sin β}$ ein Punkt auf dem Einheitskreis.
	 \[⇒ A \cvec{\cos β; \sin β} = \begin{pmatrix}\cos α & \sin α \\ \sin α & -\cos α\end{pmatrix} \cvec{\cos β; \sin β} = \cvec{\cos α \cos β + \sin α \sin β; \sin α \cos β - \cos α \sin β} = \cvec{\cos (α - b),  \sin{α - B}}\]
	 \[⇒ A \cvec{\cos(\frac{α}{2} + β); \sin(\frac{α}{2} + β)} = \cvec{\cos(\frac{α}{2} - β); \sin(\frac{α}{2} - β)}\]
	 $⇒ A$ beschreibt Spiegelung an der Geraden $\Lin(\cvec{\cos \frac{α}{2}; \sin \frac{α}{2}})$
	 \[χ_A^{char} = t^2 - \Sp(A) t + \det A = t^2 - 1 = (t + 1)(t - 1)\]
	 $⇒$ A diagonalisierbar und hat Eigenwert $\pm 1$. Sei $v_1$ Eigenvektor von $A$ zum Eigenwert $1$ mit $\norm{v_1} = 1, v_2$ Eigenvektor von $A$ zum Eigenwert $-1$ mit $\norm{v_2} = 1$
	 \[⇒ <v_1, v_2> = <A v_1, A v_2> = <v_1, -v_2> = -<v_1, v_2> ⇒ <v_1, v_2> = 0 ⇔ v_1 \perp v_2\]
	 Bezüglich der Orthogonalbasis $(v_1, v_2)$ des $(ℝ^2, <·, ·>)$ ist $M_{\mathcal{B}}(\tilde A) = \begin{pmatrix}1 & 0 \\ 0 & -1\end{pmatrix}$
  #+end_proof
  #+begin_conc latex
  $φ: (ℝ^2, <·, ·>) \to (ℝ^2, <·, ·>)$ orthogonale Abbildung. Dass existiert eine Orthogonalbasis $\mathcal{B}$ von $(ℝ^2, <·, ·>)$, sodass
  \[M_{\mathcal{B}}(φ) = \begin{pmatrix}\pm 1 & 0 \\ 0 & \pm 1\end{pmatrix} \text{ oder } M_{\mathcal{B}}(φ) = \begin{pmatrix}\cos α & -\sin α \\ \sin α & \cos α\end{pmatrix}, α ∈ (0, π)\]
  Die Anzahl der $\pm 1$ sowie $α$ sind unabhängig von der Wahl einer solchen Orthogonalbasis $\mathcal{B}$ (das heißt sind Invarianten von $φ$).
  #+end_conc
  #+begin_proof latex
  Existenz von $\mathcal{B}$: Sei $\mathcal{C} = (e_1, e_2), A := M_{\mathcal{C}}(φ)$, insbesondere $A ∈ O(2)$.
  1. Fall: $A ∈ SO(2) ⇒ ∃ β ∈ (0, 2π), β \neq π$ mit
	 \[A = \begin{pmatrix}\cos β & -\sin β \\ \sin β & \cos β\end{pmatrix} \text{ oder } A = \begin{pmatrix}1 & 0 \\ 0 & 1\end{pmatrix} \text{ oder } A = \begin{pmatrix}-1 & 0 \\ 0 & -1\end{pmatrix}\]
	 Falls $β ∈ (0, π)$, setze $α := β, \mathcal{B} = \mathcal{C}$. \\
	 Falls $β ∈ (π, 2π)$
	 \[⇒ M_{(e_2, e_1)}(φ) = \begin{pmatrix}\cos β & \sin β \\ -\sin β & \cos β\end{pmatrix}\]
	 Setze $α := 2π - B, \mathcal{B} := (e_2, e_1) ⇒ β = 2π - α ⇒ \cos β = \cos α, \sin β = - \sin β$
	 \[⇒ M_{\mathcal{B}}(φ) = \begin{pmatrix}\cos α & -\sin α \\ \sin α & \cos α\end{pmatrix}\]
  2. $A ∈ O(2) \setminus SO(2) ⇒ ∃$ Orthogonalbasis $\mathcal{B}$ von $(ℝ^2, <·, ·>)$ mit $M_{\mathcal{B}}(φ) = \begin{pmatrix}1 & 0 \\ 0 & -1\end{pmatrix}$.
  Eindeutigkeit: Falls $M_{\mathcal{B}}(φ) = \begin{pmatrix}\pm 1 & 0 \\ 0 & \pm -1\end{pmatrix}$, dann Anzahl der $\pm 1 = μ_{alg}$ der Eigenwirte $\pm 1$.
  Falls $M_{\mathcal{B}}(φ) = \begin{pmatrix}\cos α & -\sin α \\ \sin α & \cos α\end{pmatrix}$, dann $χ_φ^{char} = t^2 - 2\cos α t + 1 ⇒ \cos α$ ist
  unabhängig von der Wahl der Basis $\mathcal{B}$. Wegen $α ∈ (0, π)$ ist $α$ unabhängig von $\mathcal{B}$.
  #+end_proof
  #+begin_note latex
  Verallgemeinerung von 23.12 auf $(ℝ^n, <·,·>)$ ist möglich.
  #+end_note
* Der Spektralsatz
  In diesem Abschnitt sei $(V, γ)$ stets ein Euklidischer Raum.
  #+begin_remark latex
  Die Abbildung $Γ:V \to V^{\ast}, w ↦ γ(·, w)$ ist ein Isomorphismus.
  #+end_remark
  #+begin_proof latex
  $γ$ nicht ausgeartet nach 22.6 $⇒ γ$ perfekt, das heißt $Γ$ Isomorphismus.
  #+end_proof
  #+begin_note latex
  Insbesondere ist für einen Euklidischen Vektorraum $(V, γ)$ die Vektorräume $V$ und $V^{\ast}$ kanonisch isomorph.
  #+end_note
  #+begin_remark latex
  $\mathcal{B} = (v_1, \dots, v_n)$ Orthonormalbasis von $(V, γ), \mathcal{B}^{\ast} = (v_1^{\ast}, \dots, v_n^{\ast})$ duale Basis zu $\mathcal{B}$,
  $U ⊆ V$ Untervektorraum, $Γ:V\to V^{\ast}$ kanonische Abbildung aus 24.1. Dass gilt:
  1. $Γ(U^{\perp}) = U^0$
  2. $Γ(v_i) = v_i^{\ast}, i = 1, \dots, n$
  #+end_remark
  #+begin_proof latex
  1. $Γ(U^{\perp}) ⊆ U^0$, denn: Für $v ∈ U^{\perp}, u ∈ U$ ist $(Γ(v))(w) = γ(u, v) = 0 ⇒ Γ(U^{\perp}) ⊆ U^0$.
     \[\dim Γ(U^{\perp}) = \dim U^{\perp} = \dim V - \dim U = \dim U^0\]
  2. Es ist $Γ(v_i)(v_j) = γ(v_j, v_i) = δ_{ij} = v_i^{\ast}(v_j), j = 1, \dots, n$, das heißt $Γ(v_i) = v_i^{\ast}$
  #+end_proof
  #+begin_remdef latex
  $(V, γ_V), (W, γ_W)$ Euklidische Räume, $φ: V\to W$. Dass existiert genau eine lineare Abbildung $φ^{ad}: W \to V$ mit
  \[γ_W(φ(v), w) = γ_V(v, φ^{ad}(w)) ∀ v ∈ V, w ∈ W\]
  $φ^{ad}$ heißt die zu $φ$ *adjungierte Abbildung*
  #+end_remdef latex
  #+begin_proof latex
  Existenz: Wir betrachten das Diagramm
  #+begin_export latex
  \catcode`(=12
  \catcode`)=12
  #+end_export
  \begin{figure}[H]
	 \centering
  \begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=3em] {V & W \\ V^{\ast} & W^{\ast}\\};
  \path[-stealth]
  (m-2-1) edge node [left] {$φ^{ad}$} (m-1-1)
     	  edge node [below] {$Γ_W$} (m-2-2)
  (m-1-1) edge node [below] {$Γ_V$} (m-1-2)
  (m-2-1) edge node [left] {$φ^{\ast}$} (m-1-2);
  \end{tikzpicture}
  \end{figure}
  #+begin_export latex
  \catcode`(=\active
  \catcode`)=\active
  #+end_export
  und setzen $φ^{ad} := Γ_V^{-1} \circ φ^{\ast} \circ Γ_W$, $φ^{ad}$ ist linear nach Konstruktion. Es gilt für $v ∈ V, w ∈ W$:
  \begin{align*}
  γ_W(φ(v), w) &= Γ_W(w)(φ(v)) = (Γ_W(w) \circ φ)(v) = φ^{\ast}(Γ_W(w))(v) \\
  &= ((φ^{\ast} \circ Γ_W)(w))(v) = ((Γ_V \circ φ^{ad})(w))(v) = Γ_V(φ^{ad}(w))(v) \\
  &= γ(v, φ^{ad}(w))
  \end{align*}
  Eindeutigkeit: Damit obige Gleichung für alle $v ∈ V, w ∈ W$ gilt, muss das Diagramm kommutieren, das heißt $Γ_V \circ φ^{ad} = φ^{\ast} \circ Γ_W$, also $φ^{ad} = Γ_V^{-1} \circ φ^{\ast} \circ Γ_W$.
  #+end_proof
  #+begin_note latex
  Ist $φ$ orthogonal, dann ist $φ^{ad} = φ^{-1}$, denn für $v, w ∈ V$
  \[γ(φ(v), w) = γ(φ(v), φ(φ^{-1}(w))) = γ(v, φ(w))\]
  #+end_note
  #+begin_remark latex
  $(V, γ_V), (W, γ_W)$ euklidische Räume, $\mathcal{A}$ Orthonormalbasis von $(V, γ_V), \mathcal{B}$ Orthonormalbasis von $(W, γ_W)$, $φ: V \to W$ lineare Abbildung. Dass gilt
  \[M_{\mathcal{A}}^{\mathcal{B}}(φ^{ad}) = (M_{\mathcal{B}}^{\mathcal{A}}(φ))^T\]
  Insbesondere ist $(φ^{ad})^{ad} = φ$
  #+end_remark
  #+begin_proof latex
  \begin{align*}
  M_{\mathcal{A}}^{\mathcal{B}}(φ^{ad}) &= M_{\mathcal{A}}^{\mathcal{B}}(Γ_V^{-1} \circ φ^{\ast} \circ Γ_W) = \underbrace{M_{\mathcal{A}}^{\mathcal{A}^{\ast}}(Γ_V^{-1})}_{E_{\dim V}} \underbrace{M_{\mathcal{A}^{\ast}}^{\mathcal{B}^{\ast}}}_{(M_{\mathcal{B}}^{\mathcal{A}}(φ))^T} \underbrace{M_{BB^{\ast}}^{\mathcal{B}}(Γ_W)}_{= E_{\dim W}} \\
  &= (M_{\mathcal{B}}^{\mathcal{A}}(φ))^T
  \end{align*}
  #+end_proof
  #+begin_thm latex
  $(V, γ_V), (W, γ_W)$ euklidische Räume, $φ: V \to W$ lineare Abbildung. Dass gilt:
  1. $\ker (φ^{ad}) = (\im φ)^{\perp}$
  2. $\im (φ^{ad}) = (\ker φ)^{\perp}$
  #+end_thm
  #+begin_proof latex
  1. $w ∈ (\im φ)^{\perp} ⇔ γ_W(φ(v), w) = 0 ∀ v ∈ V ⇔ γ_V(v, φ^{ad}(w)) = 0 ∀ v ∈ V$, $γ$ nicht ausgeartet $⇒ φ^{ad}(w) = 0 ⇔ w ∈ \ker(φ^{ad})$
  2. $(\im(φ^{ad}))^{\perp} = \ker{(φ^{ad})^{ad}} = \ker φ ⇔ (\ker φ)^{\perp} = (\im(φ^{ad})^{\perp})^{\perp} = \im{φ^{ad}}$
  #+end_proof
  #+begin_conc latex
  $φ ∈ \End(V)$. Dann gilt:
  \[V = \ker φ \hat\oplus \im φ^{ad} \quad \text{sowie} \quad V = \ker φ^{ad} \hat\oplus \im φ\]
  #+end_conc
  #+begin_proof latex
  Es ist
  \[V = (\ker φ) \hat\oplus (\ker φ)^{\perp} = \ker φ \hat\oplus \im φ^{ad}\]
  andere Gleichung analog.
  #+end_proof
  #+ATTR_LATEX: :options [Selbstadjungiert]
  #+begin_defn latex
  $φ ∈ \End(V)$ heißt *selbstadjungiert* $⇔ φ = φ^{ad}$
  #+end_defn
  #+begin_remark latex
  $\mathcal{B}$ Orthonormalbasis von $(V, γ)$. Dann sind äquivalent:
  1. $φ$ selbstadjungiert
  2. $M_{\mathcal{B}}(φ)$ symmetrisch
  In diesem Fall $V = \ker φ \hat\oplus \im φ$
  #+end_remark
  #+begin_proof latex
  $φ$ selbstadjungiert $⇔ φ = φ^{ad} ⇔ M_{\mathcal{B}}(φ) = M_{\mathcal{B}}{φ^{ad}} = (M_{\mathcal{B}}(φ))^T$. Nach 24.6 ist dann $V = \ker φ \hat\oplus \im φ^{ad} = \ker φ \hat \oplus \im φ$
  #+end_proof
